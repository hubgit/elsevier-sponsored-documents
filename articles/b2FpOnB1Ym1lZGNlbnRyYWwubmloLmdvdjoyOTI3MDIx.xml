<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">J Parallel Distrib Comput</journal-id>
      <journal-title-group>
        <journal-title>Journal of Parallel and Distributed Computing</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">0743-7315</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2927021</article-id>
      <article-id pub-id-type="pmid">20862190</article-id>
      <article-id pub-id-type="publisher-id">YJPDC2745</article-id>
      <article-id pub-id-type="doi">10.1016/j.jpdc.2010.03.011</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Multi-heuristic dynamic task allocation using genetic algorithms in a heterogeneous distributed system</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Page</surname>
            <given-names>Andrew J.</given-names>
          </name>
          <email>ap13@sanger.ac.uk</email>
          <email>andrewjpage@gmail.com</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Keane</surname>
            <given-names>Thomas M.</given-names>
          </name>
          <email>tk2@sanger.ac.uk</email>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Naughton</surname>
            <given-names>Thomas J.</given-names>
          </name>
          <email>tomn@cs.nuim.ie</email>
          <xref rid="aff2" ref-type="aff">b</xref>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>a</label>Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Cambridge, CB10 1SA, UK</aff>
      <aff id="aff2"><label>b</label>Department of Computer Science, National University of Ireland, Maynooth, Co.Kildare, Ireland</aff>
      <aff id="aff3"><label>c</label>University of Oulu, RFMedia Laboratory, Oulu Southern Institute, Vierimaantie 5, 84100 Ylivieska, Finland</aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. <email>ap13@sanger.ac.uk</email><email>andrewjpage@gmail.com</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <month>7</month>
        <year>2010</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <month>7</month>
        <year>2010</year>
      </pub-date>
      <volume>70</volume>
      <issue>7</issue>
      <fpage>758</fpage>
      <lpage>766</lpage>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>4</month>
          <year>2009</year>
        </date>
        <date date-type="rev-recd">
          <day>6</day>
          <month>3</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>29</day>
          <month>3</month>
          <year>2010</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2010 Elsevier Inc.</copyright-statement>
        <copyright-year>2010</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>We present a multi-heuristic evolutionary task allocation algorithm to dynamically map tasks to processors in a heterogeneous distributed system. It utilizes a genetic algorithm, combined with eight common heuristics, in an effort to minimize the total execution time. It operates on batches of unmapped tasks and can preemptively remap tasks to processors. The algorithm has been implemented on a Java distributed system and evaluated with a set of six problems from the areas of bioinformatics, biomedical engineering, computer science and cryptography. Experiments using up to 150 heterogeneous processors show that the algorithm achieves better efficiency than other state-of-the-art heuristic algorithms.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Scheduling</kwd>
        <kwd>Genetic algorithms</kwd>
        <kwd>Heterogeneous</kwd>
        <kwd>Distributed computing</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <label>1</label>
      <title>Introduction</title>
      <p>Many heuristic algorithms exist for the task allocation problem, but most are limited to specific cases <xref rid="b10" ref-type="bibr">[10]</xref>. The use of evolutionary algorithms in scheduling, that apply evolutionary strategies from nature, allows for the fast exploration of the search space of possible schedules. This allows for good solutions to be found quickly and for the scheduler to be applied to more general problems. The genetic algorithm (GA) <xref rid="b6" ref-type="bibr">[6]</xref> evolutionary strategy has been shown to consistently generate more efficient solutions than other evolutionary strategies when applied to scheduling in heterogeneous distributed systems <xref rid="b2" ref-type="bibr">[2]</xref>.</p>
      <p>Many researchers have investigated the use of GAs to schedule tasks in homogeneous <xref rid="b7 b15 b16 b30" ref-type="bibr">[7,15,16,30]</xref> and heterogeneous <xref rid="b1 b2 b18 b27 b29" ref-type="bibr">[1,2,18,27,29]</xref> multi-processor systems with some success. However, the generality of these solutions are often reduced because of the assumptions made; (i) calculating schedules off-line in advance <xref rid="b1 b2 b7 b27 b29" ref-type="bibr">[1,2,7,27,29]</xref>, (ii) a priori knowledge of communication times and task processing times <xref rid="b1 b2 b7 b27 b29" ref-type="bibr">[1,2,7,27,29]</xref>, (iii) instantaneous message passing <xref rid="b30" ref-type="bibr">[30]</xref>, (iv) all processors are homogeneous <xref rid="b7 b30" ref-type="bibr">[7,30]</xref>, and are dedicated to the distributed system <xref rid="b1 b7 b10 b25 b27 b28 b29 b30 b31" ref-type="bibr">[1,7,10,25,27–31]</xref>. All of these assumptions limit the applicability of a scheduler in a real-world distributed system. It is our belief that if a scheduler is to be made applicable to real-world distributed computing environments and problems, then it should not make any prior assumptions about resource homogeneity or availability.</p>
      <p>In this paper a scheduling strategy is presented that uses a GA to schedule a set of heterogeneous tasks on to a set of heterogeneous processors in an effort to minimize the total execution time. It operates dynamically, allowing for tasks to arrive for processing continuously, and considers variable system resources, which has not been considered by other dynamic GA schedulers. To allow for efficient schedules to be produced quickly, the scheduler utilizes 8 heuristics, reducing the probability of processors becoming idle while waiting for a schedule to be generated. The scheduler has been implemented on a real-world distributed system and tested on 150 non-dedicated heterogeneous processors, with a variety of real-world problems from bioinformatics, biomedical engineering, computer science and cryptography. This paper significantly extends <xref rid="b24" ref-type="bibr">[24]</xref>, which presented a GA scheduling algorithm, enhanced by a single heuristic. Simulated experiments showed that this method could be used to create efficient schedules, however this scheduler had all system and task information available to it in advance, and the processing resources, communication resources and the task computation requirements were drawn from standard distributions. The major contributions of this paper include: on-line estimation of resources, dealing with varying resources, dynamically modeling task execution time distributions, and providing an efficient method for scheduling in real-world heterogeneous distributed systems with zero advanced knowledge.</p>
    </sec>
    <sec id="sec2">
      <label>2</label>
      <title>Genetic algorithm</title>
      <p>We have created an algorithm which can adapt to varying resource environments utilizing a multi-heuristic GA (see Algorithm 1), originally based on the homogeneous dynamic load-balancing algorithm in <xref rid="b30" ref-type="bibr">[30]</xref> and an extension of <xref rid="b24" ref-type="bibr">[24]</xref>. We wish to schedule an unknown number of tasks for processing on a distributed system with a minimal total execution time, otherwise known as makespan.</p>
      <p>
        <fig id="d30e1433">
          <graphic xlink:href="fx1"/>
        </fig>
      </p>
      <p>The set of processors of the distributed system is heterogeneous. The available network resources between processors in the distributed system can vary over time. The availability of each processor can vary over time (processors are non-dedicated). Tasks are indivisible, independent of all other tasks, arrive randomly, and can be processed by any processor in the distributed system.</p>
      <p>When tasks arrive they are placed in a queue of unscheduled tasks. Batches of tasks from this queue are scheduled on processors during each invocation of the scheduler. The queue of unscheduled tasks can contain a large number of tasks. If all of these tasks where to be scheduled at once, the scheduler could take a long time to find an efficient schedule. To reduce the execution time of the scheduler and reduce the chance of processors becoming idle, we only consider a subset of the unscheduled tasks, which we call a batch. A larger batch will usually result in a more efficient schedule <xref rid="b30" ref-type="bibr">[30]</xref>, but will incur a longer running time. To do this we dynamically set the batch size according to the estimated amount of time until the first processor becomes idle (further details can be found in <xref rid="b24" ref-type="bibr">[24]</xref>).</p>
      <p>Each idle processor in the system requests a task to process from the scheduler, which it processes and returns. The scheduler contains a queue of future tasks for each processor, and when a request for work is received the task at the head of the corresponding queue is sent for processing. A processor does not contain a queue of tasks; because network resources are limited and processing resources are not dedicated. We also wish to avoid repeatedly issuing the same task multiple times, e.g., when a machine is switched off. When the server has spare resources it continues to improve the planned schedule in a non-preemptive fashion (running tasks are not moved). This then allows for a more efficient operation of the system when a task exceeds its estimated running time. The server stores information about the processors, tasks and communication channels. This information is then used to estimate the properties of the system and the resource requirements of the tasks to be processed. This is presented in more detail in <xref rid="b22" ref-type="bibr">[22]</xref>.</p>
      <sec id="sec2.1">
        <label>2.1</label>
        <title>Encoding</title>
        <p>Each schedule is encoded as a string of characters, using the same analogy as the encoding of DNA in nature. A single solution is referred to as a chromosome, and a set of multiple possible solutions is referred to as a population. <xref rid="fig1" ref-type="fig">Fig. 1</xref> shows the encoding used within the GA. Each number represents a unique task identifier, with <inline-formula><alternatives><textual-form specific-use="jats-markup"> − 1</textual-form><mml:math id="M1" altimg="si2.gif" display="inline" overflow="scroll"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> being used to delimit different processor queues. This encoding allows for the execution order of tasks to be defined on each processor, which allows for precedence constraints (not covered in this research). If the execution order of tasks was not required to be defined, a simpler encoding can be used, where the index of each character corresponds to a task, and the character itself corresponds to a processor.</p>
      </sec>
      <sec id="sec2.2">
        <label>2.2</label>
        <title>Fitness function</title>
        <p>A fitness function attaches a value to each chromosome in the population, which indicates the quality of the schedule. It comes from the evolutionary principle of ‘survival of the fittest’, where the organisms with the best characteristics for their environment have a better chance of surviving to the next generation than weaker organisms, which are less adapted to their environment. We use a localized makespan to delineate fitness. Simply taking the makespan of a solution only considers the total execution time, however a well balanced load distribution is also a desirable property, which will also lead to a lower makespan. Thus we have developed a fitness function which utilizes both. The localized makespan looks at when each processor will become idle next, and adds on the time to process each task in the proposed schedule. The processors with the largest and smallest processing times are then identified. If these times are the same, it indicates a perfectly balanced schedule. As the difference becomes greater, so does the load imbalance, which also effects the efficiency of the resource utilization. The localized makespan of the <italic>y</italic>th batch of tasks is <inline-formula><mml:math id="M2" altimg="si4.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>n</italic><sub><italic>y</italic></sub></textual-form><mml:math id="M3" altimg="si5.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is all of the tasks, up to and including the <italic>y</italic>th batch of tasks, <italic>A</italic> is the processing time of a task, <italic>B</italic> is the communication overhead of the task, and <italic>x</italic> is a schedule from the population. The fitness value of chromosome <italic>x</italic> is <disp-formula id="fd1"><label>(1)</label><mml:math id="M4" altimg="si11.gif" display="block" overflow="scroll"><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mtext>:</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mtext>:</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise ,</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula> and <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>F</italic><sub><italic>x</italic></sub> = [0, 1]</textual-form><mml:math id="M5" altimg="si12.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. A larger value indicates a better or fitter schedule.</p>
      </sec>
      <sec id="sec2.3">
        <label>2.3</label>
        <title>Multiple heuristics</title>
        <p>We use eight simple heuristics to create an initial population within the GA scheduler. We chose to use 4 heuristics, along with 4 variations, which are very simple and commonly found in real-world systems, often with only slight variations and/or different names. Two are batch heuristics and 2 are immediate mode heuristics. We hypothesise that using more heuristics will improve the overall initial population, however this requires further research and is beyond the scope of this paper. The remainder of the population is generated using random permutations of these heuristics. The use of multiple heuristics in our initial population provides the GA with reasonable starting solutions, compared to starting with a completely randomly generated initial population. By employing elitism, the GA will always produce a solution which is equal to, or better than, the best heuristic solution in the initial population, because the best/fittest solution is always brought forward to the next generation.</p>
        <p>The eight heuristics operate on batches of tasks, and each is presented with the same set of tasks. They are also all presented with estimated task execution times, estimated communication overheads, and execution rates of the processors in MFLOP. Details of how the task execution times and communication overheads are estimated can be found in <xref rid="b22" ref-type="bibr">[22]</xref>. We will now present each of these heuristics. The complexity of each of these heuristics is <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>Θ</italic>(<italic>N</italic><sup>2</sup>)</textual-form><mml:math id="M6" altimg="si13.gif" display="inline" overflow="scroll"><mml:mi>Θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>N</italic> is the number of unmapped tasks and <italic>M</italic> is the number of processors. The complexity of the meta-heuristic proposed in this paper is also <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>Θ</italic>(<italic>N</italic><sup>2</sup>)</textual-form><mml:math id="M7" altimg="si16.gif" display="inline" overflow="scroll"><mml:mi>Θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
        <p>The max–min (MX) heuristic begins with a set of unmapped tasks. The execution time of each task on each processor is added to an ETC matrix where <inline-formula><alternatives><textual-form specific-use="jats-markup">ETC(<italic>i</italic>, <italic>j</italic>)</textual-form><mml:math id="M8" altimg="si17.gif" display="inline" overflow="scroll"><mml:mstyle mathvariant="normal"><mml:mi>ETC</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes the execution time of task <italic>i</italic> on processor <italic>j</italic>. The ETC matrix is directly equivalent to the makespan. For each task, the processor which will compute it with the minimum amount of time is selected and added to a set. The task-processor mapping with the largest completion time in this set is selected. This task is then assigned to the processors queue, and removed from the set of unmapped tasks. This process is repeated until all tasks are mapped to a processor. The MX heuristic attempts to schedule the longest running tasks as early as possible, to processors which will process the tasks as fast as possible. Tasks with shorter execution times can then be mixed with the longer running tasks resulting in an overall move evenly balanced load across the processors and a better makespan.</p>
        <p>The min–min (MM) scheduler <xref rid="b8" ref-type="bibr">[8]</xref> is similar to the MX heuristic, except that after the set of minimum completion times is found, the task with the overall minimum completion time is assigned to the corresponding processor. MM increases the probability that more tasks will get to execute on their first preference processor than with MX <xref rid="b18" ref-type="bibr">[18]</xref>.</p>
        <p>The max lightest loaded (LLX) heuristic scheduler considers the existing load on processors and the estimated MFLOP of the tasks. The set of unmapped tasks is sorted in descending order according to their estimated size. The task with the largest computational requirement (in MFLOP) is then assigned to the lightest loaded processor. This is repeated until all tasks have been mapped to processors. LLX does not consider the time a task will take to execute on a given processor. It instead aims to put large tasks on lightly loaded processors, and small tasks on heavily loaded processors. If the estimated processing time of tasks has a high error, this heuristic will still provide a reasonably distributed load compared to MX and MM.</p>
        <p>The min lightest loaded (LLM) heuristic scheduler operates in the same way as LLX, except the computational requirements of the tasks are sorted in ascending order. It attempts to schedule the smallest tasks first to increase the throughput of tasks.</p>
        <p>Each of the heuristics above, MX, MM, LLX, and LLM assume there is no network overhead for scheduling a task on a processor. Where the processing to communication (P-to-C) ratio is very high, the network overhead may be negligible, but when it is low, or when there is limited network resources, the communications overhead must be considered for scheduling a task on a processor.</p>
        <p>A variant of each of the above heuristics, MXC, MMC, LLXC and LLMC estimates the communication cost of mapping tasks to processors. Communication costs are estimated using the <italic>k</italic>-NN algorithm as described in <xref rid="b22" ref-type="bibr">[22]</xref>. The makespan is updated to include communication costs, <inline-formula><alternatives><textual-form specific-use="jats-markup">ETC(<italic>i</italic>, <italic>j</italic>) + <italic>C</italic>(<italic>i</italic>, <italic>j</italic>)</textual-form><mml:math id="M9" altimg="si21.gif" display="inline" overflow="scroll"><mml:mstyle mathvariant="normal"><mml:mi>ETC</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>C</italic>(<italic>i</italic>, <italic>j</italic>)</textual-form><mml:math id="M10" altimg="si22.gif" display="inline" overflow="scroll"><mml:mstyle mathvariant="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the estimated communication overhead associated with executing task <italic>i</italic> on processor <italic>j</italic>. As each processed task is returned, a tuple of information (total communication time in seconds, task inputs, task identifier, processor identifier) is saved to the communications observation set. An estimated communication time in seconds is generated by passing in the processor identifier <italic>j</italic>, the task identifier <italic>i</italic> and the input parameters to the task. Apart from PN (overall meta-heuristic) MXC, MMC, LLXC and LLMC are the only new heuristics proposed in this paper. All other heuristics are proposed elsewhere.</p>
        <p>Each heuristic is suited to different situations. MX performs well when there are more large tasks than small tasks, with MM performing better in the opposite situation <xref rid="b18" ref-type="bibr">[18]</xref>. LLX and LLM are ideal heuristics for the situation where the size of tasks to be processed is not known, or the estimated processing time has high error. The variations of all the heuristics, which estimate communication costs, allows for efficient schedules to be produced in systems with high communications costs, such as massively distributed systems.</p>
      </sec>
      <sec id="sec2.4">
        <label>2.4</label>
        <title>Evolutionary phase</title>
        <p>The evolutionary phase of the GA is governed by the cycle crossover method <xref rid="b20" ref-type="bibr">[20]</xref>. Two parent (<italic>A</italic> and <italic>B</italic>) strings are randomly selected from the population. Index <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>x</italic><sub>1</sub></textual-form><mml:math id="M11" altimg="si29.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is randomly chosen. <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>A</italic><sub><italic>x</italic><sub>1</sub></sub></textual-form><mml:math id="M12" altimg="si30.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>B</italic><sub><italic>x</italic><sub>1</sub></sub></textual-form><mml:math id="M13" altimg="si31.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> are marked as having been visited. The value contained in <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>B</italic><sub><italic>x</italic><sub>1</sub></sub></textual-form><mml:math id="M14" altimg="si32.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is noted. This value is then searched for in <italic>A</italic> and the index of this value is denoted as <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>x</italic><sub>2</sub></textual-form><mml:math id="M15" altimg="si34.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>A</italic><sub><italic>x</italic><sub>2</sub></sub></textual-form><mml:math id="M16" altimg="si35.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>B</italic><sub><italic>x</italic><sub>2</sub></sub></textual-form><mml:math id="M17" altimg="si36.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> are then marked as having been visited, and the value in <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>B</italic><sub><italic>x</italic><sub>2</sub></sub></textual-form><mml:math id="M18" altimg="si37.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is searched for in <italic>A</italic>. This continues until an index in <italic>A</italic> is visited twice. A cycle has now been found. All indices visited are then crossed over to produce 2 new child strings. This ensures that the child strings generated are valid, e.g. only 1 task may be scheduled to 1 processor at any time. Since both parents contain the exact same character, just in a different order, a cycle will always be found.</p>
      </sec>
      <sec id="sec2.5">
        <label>2.5</label>
        <title>Mutation</title>
        <p>Two types of mutation are employed by the GA, one randomly swaps elements of chromosomes in the population, and the other is a rebalancing heuristic. Random mutations are an essential part of a GA, perturbing the population to allow for new areas in the solution space to be searched. Every generation a percentage of elements in the population is randomly mutated. If the improvement in the makespan has not improved after 10 generations, the mutation rate is increased. Once the makespan begins to improve again the mutation rate is reduced. This reduces the probability of the GA getting stuck in a local minimum.</p>
        <p>The other mutation operation utilizes a rebalancing heuristic to reduce the makespan. It achieves this by attempting to more evenly distribute the load on processors, by swapping tasks from heavily loaded processors on to lightly loaded processors. It has an average case complexity of <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>Θ</italic>(<italic>M</italic> + <italic>N</italic>)</textual-form><mml:math id="M19" altimg="si40.gif" display="inline" overflow="scroll"><mml:mi>Θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>M</italic> is the number of processors, and <italic>N</italic> is the number of tasks. The solution generated by the heuristic will be discarded if it is worse than the starting solution, thus ensuring that the heuristic will only have a positive effect on the makespan.</p>
      </sec>
      <sec id="sec2.6">
        <label>2.6</label>
        <title>Selection</title>
        <p>The selection technique is based on the roulette wheel method <xref rid="b7 b25 b30" ref-type="bibr">[7,25,30]</xref>. The probability of a string going forward to the next generation is represented as a proportional sized slot on the roulette wheel, with a range from 0 to 1. Random numbers from 0 to 1 are then generated. The string which corresponds to the randomly selected slot is brought forward to the next generation. Since fitter strings have larger slots, they are more likely to be brought forward to the next generation. This process continues until a sufficient number of strings are selected.</p>
      </sec>
      <sec id="sec2.7">
        <label>2.7</label>
        <title>Stopping conditions</title>
        <p>When the stopping conditions are met, the evolution of the population will halt. This is to prevent the GA from running forever. Since this scheduler is intended for use in an on-line distributed system, it must produce schedules in a reasonable amount of time. Thus we use two stopping conditions: (1) there is an upper bound on the maximum number of generations, to guarantee evolution will halt and (2) if the makespan of the best solution has not changed after a set number of generations, then the GA will stop.</p>
      </sec>
    </sec>
    <sec id="sec3">
      <label>3</label>
      <title>Experiments</title>
      <p>For the experiments described in this section, we primarily used the 3 experimental setups in <xref rid="tbl1" ref-type="table">Table 1</xref>, run on a heterogeneous Java distributed system <xref rid="b11" ref-type="bibr">[11]</xref>. The first and simplest setup is a homogeneous set of processors, which we use as a base case for our experiments. This allows schedulers which favour a homogeneous set of processors to excel. The next setup is a set of processors with 2 homogeneous sets of processors. Both of theses setups used a 100 Mbps network. Finally, we used a set of processor with high heterogeneity and with a heterogeneous network which was spread over 3 different LANs and ranged from 10–100 Mbps. We had non-dedicated usage of these processors, and the actual available processing and network resources varied stochastically over time. All experiments were performed at off-peak times to minimize the effect of these variations. All the clients connected to a dedicated server running Linux (Fedora Core 4) on a 3 GHz P4 with 1 GB of RAM.</p>
      <sec id="sec3.1">
        <label>3.1</label>
        <title>Other scheduling algorithms</title>
        <p>The performance of the PN scheduler has been compared to the performance of a number of different schedulers. These schedulers are the most commonly used schedulers in distributed computing (see <xref rid="tbl2" ref-type="table">Table 2</xref>). The earliest first (EF) scheduler <xref rid="b17" ref-type="bibr">[17]</xref> is an immediate mode heuristic scheduler. It schedules tasks on the processor which will finish processing earliest. The lightest loaded (LL) scheduler is also an immediate mode scheduler, scheduling tasks on the most lightly loaded processors, without regard for the processing time of the task. MX is a batch scheduler which attempts to schedule the largest tasks first, and MM is the opposite, scheduling the smallest tasks first. We compare PN to three other evolutionary schedulers. A simulated annealing (SA) <xref rid="b14" ref-type="bibr">[14]</xref> based scheduler was created using the open source library Jannealer <xref rid="b9" ref-type="bibr">[9]</xref>. A tabu search (TA) based scheduler was created using OpenTS <xref rid="b21" ref-type="bibr">[21]</xref>. A GA scheduler (ZO) developed by Zomaya &amp; Teh <xref rid="b30" ref-type="bibr">[30]</xref> is used for comparison purposes.</p>
        <p>The scheduling algorithms are of varying complexity (see <xref rid="tbl7" ref-type="table">Table 7</xref>), from the least complex, round robin (RR), to the most complex evolutionary algorithms. These schedulers represent the most commonly used heuristics and the state-of-the-art evolutionary schedulers.</p>
      </sec>
      <sec id="sec3.2">
        <label>3.2</label>
        <title>Heterogeneous distributed system</title>
        <p>A general purpose programmable Java distributed system, which utilizes the free resources of a heterogeneous set of computers linked together by a network, has been developed <xref rid="b11" ref-type="bibr">[11]</xref>. The system has been successfully deployed on over 500 computers, which were distributed over a number of locations, and has been successfully used to process bioinformatics <xref rid="b13" ref-type="bibr">[13]</xref>, biomedical engineering <xref rid="b23" ref-type="bibr">[23]</xref>, and cryptography applications.</p>
        <p>The distributed system consists of 3 Java archive files, a client, a server and a remote interface. A problem can be created for the system simply by extending 2 classes, called Algorithm and DataManager. The Algorithm class is run on the client and specifies the actual computation to be performed. The DataManager class is run on the server and specifies how the problem is broken up into tasks and how the processed results are recombined.</p>
        <p>The distributed system provides a simple scheduling interface, which allows the administrator of the system to select a scheduling algorithm using the remote interface. To create a new scheduler, a programmer only needs to extend the SchedulerCommon API and implement a single method called generateSchedule. This method simply takes in a list of tasks and maps them to processors. The system defaults to the simplest scheduler, round robin.</p>
        <p><xref rid="tbl3" ref-type="table">Table 3</xref> has a quick overview of the properties of each problem application. The set of problem used in this section is detailed in <xref rid="b22" ref-type="bibr">[22]</xref>.</p>
      </sec>
      <sec id="sec3.3">
        <label>3.3</label>
        <title>GA experiments</title>
        <p>Parameters used within a GA, such as the number of generations, mutation rate and chromosome length, can effect the running time and quality of results generated by the GA. We will investigate the effect varying these can have on the scheduling algorithm.</p>
        <p>The execution time of the scheduler increases approximately linearly with an increase in the number of chromosomes. This can be seen in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, where we varied the chromosome length and measured the execution time of the GA. We fixed the number of generations at 500 and ignored all other stopping conditions. The execution time of a given chromosome length varies, due to the stochastic nature of overheads in a real-world distributed system, but the majority of times fall into a tight linear range. The tasks used in the experiment are described in <xref rid="tbl3" ref-type="table">Table 3</xref>. The scheduler produces schedules for large numbers of tasks and processors quickly, for example, the GA scheduler can schedule a batch of 170 tasks in under 1 s.</p>
        <p>However, the scheduler uses a variable number of generations, depending on whether the stopping conditions are met. If there in no improvement after 50 generations, the algorithm stops. The figure of 50 was chosen as a large enough figure to allow for the random mutations to evolve a solution out of a local minima without impacting significantly on the running time of the algorithm. The histogram in <xref rid="fig3" ref-type="fig">Fig. 3</xref> shows the number of generations performed before this stopping condition halts evolution. It forms a Poisson distribution, which indicates that the scheduler finds either a local minimum or the global minimum makespan within a relatively low number of generations.</p>
        <p>When the quality of the solution produced is considered, we found that the greatest average reduction in makespan occurs within the first 200 generations. <xref rid="fig4" ref-type="fig">Fig. 4</xref> shows this with a large reduction in makespan at the beginning, but the returns diminish quickly. Since the execution time of a generation is a constant factor, reducing the number of generations allows for a lower execution time of the scheduler. In a real-time system a client might be lying idle whilst waiting for a schedule to be produced, nullifying the effects of a more efficient schedule, thus a lower scheduler execution time is desirable.</p>
        <p>We then looked at the effect the population size on the makespan achieved when scheduling on a real-world distributed system with 124 processors (see <xref rid="tbl4" ref-type="table">Table 4</xref>). <xref rid="tbl5" ref-type="table">Table 5</xref> shows that when a larger population size is used, the effect on the overall makespan is negligible compared to using a small population size. This is due to the stopping condition which halts evolution if there is no improvement in makespan after 50 generations. The greater diversity in a large population allows for a minimum to be found in less generations, which offsets the longer execution time for a single generation. A smaller population requires more generations to achieve the same effect, however the execution time for each generation is less. The only difference between using a small and large population size is the spacial requirement. Thus to reduce the overall memory consumption of the algorithm we use a small population size (a micro-GA <xref rid="b3" ref-type="bibr">[3]</xref>).</p>
      </sec>
      <sec id="sec3.4">
        <label>3.4</label>
        <title>Multiple heuristics performance</title>
        <p>We wish to show that using multiple heuristics to generate schedules for the initial population of the GA provides more efficient schedules than using each individual heuristic on its own, or using a purely random initial population. In <xref rid="fig5" ref-type="fig">Fig. 5</xref> we use each heuristic individually to initialize the population of the GA. Each bar is an average 10 simulations, and we scheduled 600 tasks with normally distributed execution times on 30 heterogeneous processors.</p>
        <p>The black bar shows the average initial makespan produced by the heuristic, and the gray bar corresponds to the average final makespan produced by the GA from that initial population. The population consists of only one heuristic and random variations of the schedule produced by the heuristic. A randomly chosen initial population (RM) presented for comparison purposes. The algorithm presented in this paper (PN) utilizes all of the heuristics to generate an initial population. The initial makespan for PN is an average of the best solutions generated by the heuristics. As can be seen in <xref rid="fig5" ref-type="fig">Fig. 5</xref> using multiple heuristics provides, on average, a lower makespan.</p>
        <p>In <xref rid="fig6" ref-type="fig">Fig. 6</xref> we compared each heuristics initial solution to the final evolved solution (PN), with PN utilizing all of the heuristics. A set of 6 real-world problems (see <xref rid="b22" ref-type="bibr">[22]</xref> for details) were used for this experiment, processed by 25 non-dedicated heterogeneous processors (see <xref rid="tbl6" ref-type="table">Table 6</xref>). <xref rid="fig6" ref-type="fig">Fig. 6</xref> shows the average initial solutions (normalized makespan) found by each heuristic after scheduling 60 different batches of tasks. The final evolved solution provides more efficient solutions on average than the solutions produced initially by the individual heuristics. The errorbars also show that the schedules produced by PN vary over a smaller range than the schedules produced by the other heuristics.</p>
      </sec>
      <sec id="sec3.5">
        <label>3.5</label>
        <title>Performance evaluation</title>
        <p>Each scheduler was presented with the same set of problems and the same set of processors (see <xref rid="tbl1" ref-type="table">Table 1</xref>). The makespan is measured as the time from when the first task is requested from thedistributed system, to the time when the final task is returned to the system. <xref rid="tbl7" ref-type="table">Table 7</xref> shows that there is a huge difference in makespan (lower is better) with PN processing all tasks much faster than the next best scheduler when using a highly heterogeneous set of processors and networking resources. The variation in makespans can be accounted for by inefficient mappings of tasks to processors, such as slow processors being given computationally intensive tasks or processors with high communication overheads being given tasks with a low P-to-C ratio. The experiment was repeated with a set of resources that displayed low heterogeneity (see <xref rid="tbl1" ref-type="table">Table 1</xref>.B). With less heterogeneity the difference in makespan is only 13% between the best (PN) and the worst (SA). With high heterogeneity this difference was 132%, with PN generating the lowest makespan (<xref rid="tbl8" ref-type="table">Table 8</xref>).</p>
        <p>When the experiment is repeated on a homogeneous set of processors the differences in makespan between the schedulers becomes negligible (see <xref rid="tbl9" ref-type="table">Table 9</xref>) with most schedulers utilizing the processing resources efficiently with up to 97% efficiency. PN, ZO and TA generate schedules which are within 1% of each other in this case and can adapt well to this homogeneous resource environment, which is to be expected. The simple heuristic schedulers generate solutions which have makespans which are 20%–38% longer than the evolutionary algorithms.</p>
        <p><xref rid="fig7" ref-type="fig">Fig. 7</xref> shows the number of idle clients while the set of problems is being processed using the PN scheduler in a highly heterogeneous resource environment. The initial assignment of tasks to processors does not happen instantaneously because the client machines only contact the server at set intervals (1 min in this case). Near the end when the steep slope shows that all of the clients stop processing tasks within a short interval. If this was a shallow slope it would indicate processing resources are idle and underutilized.</p>
        <p>The overall scheduling framework used in this paper allows for a zero knowledge approach to be adopted. The target user audience for this scheduler consists of non-technical researchers, who want to create a distributed application and have it “just work” without having to worry about scheduling. As all properties of the system and the tasks to be processed are estimated on-line, there is no need for the user to provide a DAG, an ETC matrix in advance or to have previously executed the application.</p>
        <p>The simple list scheduling heuristics (LL, EF, RR) take the next available task and schedule it. They do not allow for tasks to be scheduled out of sequence. The underlying algorithms are very simple, deterministic and easy to understand. The overheads are also quite low in complexity terms <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>O</italic>(<italic>M</italic>)</textual-form><mml:math id="M20" altimg="si43.gif" display="inline" overflow="scroll"><mml:mstyle mathvariant="normal"><mml:mi>O</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic>M</italic> is the number of processors, or in the case of RR O(1). The simple batch scheduling methods (MM, MX) can take a set of tasks and schedule them at once. This allows for the scheduler to look ahead to select the best task to assign to a processor. This additional capability only makes the heuristics slightly more complicated however it does increase the complexity to <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>O</italic>(<italic>N</italic><sup>2</sup>)</textual-form><mml:math id="M21" altimg="si45.gif" display="inline" overflow="scroll"><mml:mstyle mathvariant="normal"><mml:mi>O</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
        <p>The evolutionary algorithm based schedulers (PN, ZO, TA, SA) can allocate batches of tasks to processors and utilize evolutionary techniques to find near optimal solutions. The algorithms can quickly traverse large solution spaces, which allows them to adapt to different resource and computational environments. The non-deterministic nature of the algorithms can limit the applicability of these methods, such as in time critical systems or medical systems. These techniques are also more complicated internally. They all require parameters to be set and inappropriately set parameters can have a detrimental effect on the quality of the solutions found. Whilst these methods can theoretically find near optimal solutions, this can require substantial amounts of time, for example, SA will find the optimal solution given infinite time. To ensure that these algorithms finish in a realistic amount of time, where processor idle time is minimized, stopping conditions must be imposed to cut off the algorithm and return the current best solution. These trade-offs when applied to the evolutionary heuristics result in differences in performance between the algorithms.</p>
        <p>Tabu based algorithm (TA) works reasonably well in two of the three experiments. We intend to investigate using a hybrid of this algorithm for future scheduling research as its underlying similarity to genetic algorithms may well benefit from using a number of simple heuristics. The performance of the Simulated Annealing method is affected by the parameters required for the algorithm, such as the temperature and the cool down factor. For the experiments presented here, the SA parameters were set by using the simulated annealing algorithm itself, which is a standard way of setting the parameters. This is problematic however, because it has a tendency to work for a specific set of circumstances, and when faced with the unknowns of a real-world distributed system, it cannot adapt quickly enough.</p>
        <p>Compared to other methods the GA based schedulers (PN, ZO) provide reasonably efficient solutions. PN significantly extends the ZO heuristic to work with a heterogeneous resource environment and unknown task execution time distributions. A downside to the PN algorithm is the complicated non-deterministic nature of the algorithm. A simpler algorithm is usually preferential over a complicated algorithm (Occams Razor). Also as it is non-deterministic, given the same set of inputs, it is unlikely that the same set of outputs will result. This limitation also affects the other evolutionary algorithms. Overall however, PN provides an algorithm which can provide efficient solutions in a wide variety of unknown task execution time distributions, and can adapt to heterogeneous resources. This best meets our objective to create a heterogeneous computing scheduler which can be used by non-technical users without the need for them to provide a priori knowledge of the resources or computational requirements.</p>
      </sec>
    </sec>
    <sec id="sec4">
      <label>4</label>
      <title>Conclusion</title>
      <p>A scheduler was developed for the task allocation problem in a dynamic heterogeneous distributed system. It is a multi-heuristic evolutionary algorithm, which utilizes a GA, to allocate tasks to processors in polynomial time. The use of eight heuristics to initialize the GA allowed for more efficient schedules to be created than would have been with a purely random initial population. If at any stage a processor becomes idle the scheduler returns the current best solution, which will always be at least as efficient as the best heuristic solution. The GA was implemented in Java and incorporated into a distributed system. A set of real-world problems from bioinformatics, biomedical engineering and cryptography was used to test the scheduler. Experiments were performed up to 150 heterogeneous processors, and show that the scheduler presented in this paper outperforms the most commonly used heterogeneous distributed computing scheduling heuristics. The more heterogeneous the resources of a system become, the harder it is to generate an efficient mapping of tasks to processors. We have presented an algorithm which achieves better efficiency than other schedulers as the resources become more heterogeneous.</p>
      <p>For future work, the next logical step would be to distribute the scheduling algorithm to take full advantage of the available computational resources. Investigation is also needed into dynamically adapting task execution time distribution estimation techniques to identify the characteristics of applications at runtime which may yield less erroneous computational requirements estimations. Task dependencies will also need to be considered, allowing for this work to be applied to a larger set of problems. We also intend to investigate the use of different heuristics, dynamically changing the set of heuristics at runtime based on observed performance.</p>
      <p>The distributed system software is freely available under an open source GNU GPL license from the system homepage located at <ext-link ext-link-type="uri" xlink:href="http://www.cs.nuim.ie/distributed" id="interref1">http://www.cs.nuim.ie/distributed</ext-link>.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="b1">
        <label>1</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ahmad</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Kwok</surname>
              <given-names>Y.-K.</given-names>
            </name>
            <name>
              <surname>Ahmad</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Dhodhi</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <chapter-title>Scheduling parallel programs using genetic algorithms</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Zomaya</surname>
              <given-names>A.Y.</given-names>
            </name>
            <name>
              <surname>Ercal</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Olariu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <source>Solutions to Parallel and Distributed Computing Problems</source>
          <year>2001</year>
          <publisher-name>John Wiley and Sons</publisher-name>
          <publisher-loc>New York, USA</publisher-loc>
          <fpage>231</fpage>
          <lpage>254</lpage>
          <comment>(Chapter 9)</comment>
        </element-citation>
      </ref>
      <ref id="b2">
        <label>2</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Braun</surname>
              <given-names>T.D.</given-names>
            </name>
            <name>
              <surname>Siegel</surname>
              <given-names>H.J.</given-names>
            </name>
            <name>
              <surname>Beck</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Bölöni</surname>
              <given-names>L.L.</given-names>
            </name>
            <name>
              <surname>Maheswaran</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Reuther</surname>
              <given-names>A.I.</given-names>
            </name>
            <name>
              <surname>Robertson</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Theys</surname>
              <given-names>M.D.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Hensgen</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Freund</surname>
              <given-names>R.F.</given-names>
            </name>
          </person-group>
          <article-title>A comparison of eleven static heuristics for mapping a class of independent tasks onto heterogeneous distributed computing systems</article-title>
          <source>Journal of Parallel Distributed Computing</source>
          <volume>61</volume>
          <issue>6</issue>
          <year>2001</year>
          <fpage>810</fpage>
          <lpage>837</lpage>
        </element-citation>
      </ref>
      <ref id="b3">
        <label>3</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Chipperfield</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Flemming</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <chapter-title>Parallel genetic algorithms</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Zomaya</surname>
              <given-names>A.Y.</given-names>
            </name>
          </person-group>
          <source>Parallel and Distributed Computing Handbook</source>
          <edition>1st edition</edition>
          <year>1996</year>
          <publisher-name>McGraw-Hill</publisher-name>
          <publisher-loc>New York, USA</publisher-loc>
          <fpage>1118</fpage>
          <lpage>1143</lpage>
        </element-citation>
      </ref>
      <ref id="b4">
        <label>4</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Elgamal</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>A public key cryptosystem and a signature scheme based on discrete logarithms</article-title>
          <source>IEEE Transactions on Information Theory</source>
          <volume>31</volume>
          <issue>4</issue>
          <year>1985</year>
          <fpage>469</fpage>
          <lpage>472</lpage>
        </element-citation>
      </ref>
      <ref id="b5">
        <label>5</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Glover</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Future paths for integer programming and links to artificial intelligence</article-title>
          <source>Computers and Operations Research</source>
          <volume>13</volume>
          <year>1986</year>
          <fpage>533</fpage>
          <lpage>549</lpage>
        </element-citation>
      </ref>
      <ref id="b6">
        <label>6</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Holland</surname>
              <given-names>J.H.</given-names>
            </name>
          </person-group>
          <chapter-title>Adaptation in Natural and Artificial Systems</chapter-title>
          <year>1975</year>
          <publisher-name>MIT Press</publisher-name>
          <publisher-loc>Cambridge, MA, USA</publisher-loc>
        </element-citation>
      </ref>
      <ref id="b7">
        <label>7</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hou</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ansari</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>A genetic algorithm for multiprocessor scheduling</article-title>
          <source>IEEE Transactions on Parallel and Distributed Systems</source>
          <volume>5</volume>
          <issue>2</issue>
          <year>1994</year>
          <fpage>113</fpage>
          <lpage>120</lpage>
        </element-citation>
      </ref>
      <ref id="b8">
        <label>8</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ibarra</surname>
              <given-names>O.H.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>C.E.</given-names>
            </name>
          </person-group>
          <article-title>Heuristic algorithms for scheduling independent tasks on nonidentical processors</article-title>
          <source>Journal of the ACM</source>
          <volume>24</volume>
          <issue>2</issue>
          <year>1977</year>
          <fpage>280</fpage>
          <lpage>289</lpage>
        </element-citation>
      </ref>
      <ref id="b9">
        <label>9</label>
        <mixed-citation publication-type="other">Jannealer, 2006. <ext-link ext-link-type="uri" xlink:href="http://jannealer.sourceforge.net" id="interref2">http://jannealer.sourceforge.net</ext-link>.</mixed-citation>
      </ref>
      <ref id="b10">
        <label>10</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kasahara</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Narita</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Practical multiprocessing scheduling algorithms for efficient parallel processing</article-title>
          <source>IEEE Transactions on Computers</source>
          <volume>33</volume>
          <issue>11</issue>
          <year>1984</year>
          <fpage>1023</fpage>
          <lpage>1029</lpage>
        </element-citation>
      </ref>
      <ref id="b11">
        <label>11</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Keane</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Allen</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Naughton</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>McInerney</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Waldron</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <chapter-title>Distributed Java platform with programmable MIMD capabilities</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Guelfi</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Astesiano</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Reggio</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <source>Scientific Engineering for Distributed Java Applications</source>
          <series>Springer Lecture Notes in Computer Science</series>
          <volume>vol. 2604</volume>
          <year>2003</year>
          <fpage>122</fpage>
          <lpage>131</lpage>
        </element-citation>
      </ref>
      <ref id="b12">
        <label>12</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Keane</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Naughton</surname>
              <given-names>T.J.</given-names>
            </name>
          </person-group>
          <article-title>DSEARCH: sensitive database searching using distributed computing</article-title>
          <source>Bioinformatics</source>
          <year>2004</year>
          <fpage>bti163</fpage>
        </element-citation>
      </ref>
      <ref id="b13">
        <label>13</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Keane</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Naughton</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Travers</surname>
              <given-names>S.A.A.</given-names>
            </name>
            <name>
              <surname>McInerney</surname>
              <given-names>J.O.</given-names>
            </name>
            <name>
              <surname>McCormack</surname>
              <given-names>G.P.</given-names>
            </name>
          </person-group>
          <article-title>DPRml: distributed phylogeny reconstruction by maximum likelihood</article-title>
          <source>Bioinformatics</source>
          <volume>21</volume>
          <issue>7</issue>
          <year>2005</year>
          <fpage>969</fpage>
          <lpage>974</lpage>
          <pub-id pub-id-type="pmid">15513992</pub-id>
        </element-citation>
      </ref>
      <ref id="b14">
        <label>14</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kirkpatrick</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gelatt</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Vecchi</surname>
              <given-names>M.P.</given-names>
            </name>
          </person-group>
          <article-title>Optimization by simulated annealing</article-title>
          <source>Science</source>
          <volume>220</volume>
          <issue>4598</issue>
          <year>1983</year>
          <fpage>671</fpage>
          <lpage>680</lpage>
          <pub-id pub-id-type="pmid">17813860</pub-id>
        </element-citation>
      </ref>
      <ref id="b15">
        <label>15</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kwok</surname>
              <given-names>Y.-K.</given-names>
            </name>
            <name>
              <surname>Ahmad</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic critical-path scheduling: an effective technique for allocating task graphs to multiprocessors</article-title>
          <source>IEEE Transactions on Parallel and Distributed Systems</source>
          <volume>7</volume>
          <issue>5</issue>
          <year>1996</year>
          <fpage>506</fpage>
          <lpage>521</lpage>
        </element-citation>
      </ref>
      <ref id="b16">
        <label>16</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kwok</surname>
              <given-names>Y.-K.</given-names>
            </name>
            <name>
              <surname>Ahmad</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Benchmarking and comparison of the task graph scheduling algorithms</article-title>
          <source>Journal of Parallel and Distributed Computing</source>
          <volume>59</volume>
          <issue>3</issue>
          <year>1999</year>
          <fpage>381</fpage>
          <lpage>422</lpage>
        </element-citation>
      </ref>
      <ref id="b17">
        <label>17</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>C.-Y.</given-names>
            </name>
            <name>
              <surname>Hwang</surname>
              <given-names>J.-J.</given-names>
            </name>
            <name>
              <surname>Chow</surname>
              <given-names>Y.-C.</given-names>
            </name>
            <name>
              <surname>Anger</surname>
              <given-names>F.D.</given-names>
            </name>
          </person-group>
          <article-title>Multiprocessor scheduling with interprocessor communication delays</article-title>
          <source>Operations Research Letters</source>
          <volume>7</volume>
          <issue>3</issue>
          <year>1988</year>
          <fpage>141</fpage>
          <lpage>147</lpage>
        </element-citation>
      </ref>
      <ref id="b18">
        <label>18</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maheswaran</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Siegel</surname>
              <given-names>H.J.</given-names>
            </name>
            <name>
              <surname>Hensgen</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Freund</surname>
              <given-names>R.F.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic mapping of a class of independent tasks onto heterogeneous computing systems</article-title>
          <source>Journal of Parallel and Distributed Computing</source>
          <volume>59</volume>
          <issue>2</issue>
          <year>1999</year>
          <fpage>107</fpage>
          <lpage>131</lpage>
        </element-citation>
      </ref>
      <ref id="b19">
        <label>19</label>
        <mixed-citation publication-type="other">MD5, 1992. <ext-link ext-link-type="uri" xlink:href="http://tools.ietf.org/rfc/rfc1321.txt" id="interref3">http://tools.ietf.org/rfc/rfc1321.txt</ext-link>.</mixed-citation>
      </ref>
      <ref id="b20">
        <label>20</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Oliver</surname>
              <given-names>I.M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Holland</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <chapter-title>A study of permutation crossover operators on the traveling salesman problem</chapter-title>
          <source>Proceedings of the Second International Conference on Genetic Algorithms on Genetic Algorithms and Their Application</source>
          <year>1987</year>
          <publisher-name>Lawrence Erlbaum Associates, Inc.</publisher-name>
          <fpage>224</fpage>
          <lpage>230</lpage>
        </element-citation>
      </ref>
      <ref id="b21">
        <label>21</label>
        <mixed-citation publication-type="other">OpenTS—Java Tabu Search, 2006. <ext-link ext-link-type="uri" xlink:href="http://www.coin-or.org/OpenTS" id="interref4">http://www.coin-or.org/OpenTS</ext-link>.</mixed-citation>
      </ref>
      <ref id="b22">
        <label>22</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Page</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Ahrenberg</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Naughton</surname>
              <given-names>T.J.</given-names>
            </name>
          </person-group>
          <article-title>Low memory distributed reconstruction of large digital holograms</article-title>
          <source>Optics Express</source>
          <volume>16</volume>
          <issue>3</issue>
          <year>2008</year>
          <fpage>1990</fpage>
          <lpage>1995</lpage>
          <pub-id pub-id-type="pmid">18542278</pub-id>
        </element-citation>
      </ref>
      <ref id="b23">
        <label>23</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Page</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Coyle</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Keane</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Naughton</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Markham</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ward</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <chapter-title>Distributed monte carlo simulation of light transportation in tissue</chapter-title>
          <source>Proceedings of the 20th IEEE International Parallel and Distributed Processing Symposium</source>
          <year>2006</year>
          <publisher-name>IEEE Computer Society</publisher-name>
          <publisher-loc>Rhodes, Greece</publisher-loc>
          <fpage>1</fpage>
          <lpage>4</lpage>
        </element-citation>
      </ref>
      <ref id="b24">
        <label>24</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Page</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Naughton</surname>
              <given-names>T.J.</given-names>
            </name>
          </person-group>
          <chapter-title>Dynamic task scheduling using genetic algorithms for heterogeneous distributed computing</chapter-title>
          <source>Proceedings of the 19th IEEE International Parallel and Distributed Processing Symposium</source>
          <year>2005</year>
          <publisher-name>IEEE Computer Society</publisher-name>
          <publisher-loc>Washington, DC, USA</publisher-loc>
          <fpage>189.1</fpage>
        </element-citation>
      </ref>
      <ref id="b25">
        <label>25</label>
        <mixed-citation publication-type="other">H.J. Siegel, L. Wang, V. Roychowdhury, M. Tan, Computing with heterogeneous parallel machines: advantages and challenges, in: Proceedings on Second International Symposium on Parallel Architectures, Algorithms, and Networks, Beijing, China, 1996, pp. 368–374.</mixed-citation>
      </ref>
      <ref id="b26">
        <label>26</label>
        <mixed-citation publication-type="other">SHA1, 2001. <ext-link ext-link-type="uri" xlink:href="http://tools.ietf.org/rfc/rfc3174.txt" id="interref5">http://tools.ietf.org/rfc/rfc3174.txt</ext-link>.</mixed-citation>
      </ref>
      <ref id="b27">
        <label>27</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Theys</surname>
              <given-names>M.D.</given-names>
            </name>
            <name>
              <surname>Braun</surname>
              <given-names>T.D.</given-names>
            </name>
            <name>
              <surname>Siegal</surname>
              <given-names>H.J.</given-names>
            </name>
            <name>
              <surname>Maciejewski</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Kwok</surname>
              <given-names>Y.-K.</given-names>
            </name>
          </person-group>
          <chapter-title>Mapping Tasks onto Distributed Heterogeneous Computing Systems Using a Genetic Algorithm Approach</chapter-title>
          <year>2001</year>
          <publisher-name>John Wiley and Sons</publisher-name>
          <publisher-loc>New York, USA</publisher-loc>
          <comment>pp. 135–178 (Chapter 6)</comment>
        </element-citation>
      </ref>
      <ref id="b28">
        <label>28</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zomaya</surname>
              <given-names>A.Y.</given-names>
            </name>
            <name>
              <surname>Clements</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Olariu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A framework for reinforcement-based scheduling in parallel processor systems</article-title>
          <source>IEEE Transactions on Parallel and Distributed Systems</source>
          <volume>9</volume>
          <issue>3</issue>
          <year>1998</year>
          <fpage>249</fpage>
          <lpage>260</lpage>
        </element-citation>
      </ref>
      <ref id="b29">
        <label>29</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Zomaya</surname>
              <given-names>A.Y.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>R.C.</given-names>
            </name>
            <name>
              <surname>Olariu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <chapter-title>An introduction to genetic-based scheduling in parallel processor systems</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Zomaya</surname>
              <given-names>A.Y.</given-names>
            </name>
            <name>
              <surname>Ercal</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Olariu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <source>Solutions to Parallel and Distributed Computing Problems</source>
          <year>2001</year>
          <publisher-name>John Wiley and Sons</publisher-name>
          <publisher-loc>New York, USA</publisher-loc>
          <fpage>111</fpage>
          <lpage>133</lpage>
          <comment>(Chapter 5)</comment>
        </element-citation>
      </ref>
      <ref id="b30">
        <label>30</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zomaya</surname>
              <given-names>A.Y.</given-names>
            </name>
            <name>
              <surname>Teh</surname>
              <given-names>Y.-H.</given-names>
            </name>
          </person-group>
          <article-title>Observations on using genetic algorithms for dynamic load-balancing</article-title>
          <source>IEEE Transactions on Parallel and Distributed Systems</source>
          <volume>12</volume>
          <issue>9</issue>
          <year>2001</year>
          <fpage>899</fpage>
          <lpage>911</lpage>
        </element-citation>
      </ref>
      <ref id="b31">
        <label>31</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zomaya</surname>
              <given-names>A.Y.</given-names>
            </name>
            <name>
              <surname>Ward</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Macey</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Genetic scheduling for parallel processor systems: comparative studies and performance issues</article-title>
          <source>IEEE Transactions on Parallel and Distributed Systems</source>
          <volume>10</volume>
          <issue>8</issue>
          <year>1999</year>
          <fpage>795</fpage>
          <lpage>812</lpage>
        </element-citation>
      </ref>
    </ref-list>
    <bio>
      <graphic xlink:href="pic1"/>
      <p><bold>Andrew J. Page</bold> received a B.Sc. degree in Computer Science and Software Engineering from the National University of Ireland, Maynooth in 2003. He received a Ph.D. in Computer Science in 2009 from the same university. He is now working in the Wellcome Trust Sanger Institute in Cambridge in sequencing informatics in the field of genomics. His research interests include scheduling, distributed computing and bioinformatics.</p>
    </bio>
    <bio>
      <graphic xlink:href="pic2"/>
      <p><bold>Thomas M. Keane</bold> received a B.Sc. in Computer Science and Software Engineering in 2002 and an M.Sc. in Computer Science in 2004 from the National University of Ireland, Maynooth. For his Ph.D. Thomas moved to work at the Bioinformatics laboratory at the National University of Ireland, Maynooth, working in the area of phylogenetic methods and high-throughput phylogenomics using distributed computing and completed it in 2006. He is currently a team leader at the Wellcome Trust Sanger Institute in Cambridge. His research interests include distributed computing and bioinformatics.</p>
    </bio>
    <bio>
      <graphic xlink:href="pic3"/>
      <p><bold>Thomas J. Naughton</bold> received the B.Sc. degree (double honours) in Computer Science and Experimental Physics from the National University of Ireland, Maynooth, Ireland. He has worked at Space Technology (Ireland) Ltd. and has been a visiting researcher at the Department of Radioelectronics, Czech Technical University, Prague, and the Department of Electrical and Computer Engineering, University of Connecticut, Storrs. He is a Senior Lecturer in the Department of Computer Science, National University of Ireland, Maynooth, with a permanent appointment since 2001. Since 2007 he has been a European Commission Marie Curie Fellow at Oulu Southern Institute, University of Oulu, Finland. He leads the EC FP7 three-year eight-partner collaborative project Real 3D. His research interests include optical information processing, computer theory, and distributed computing. He has served as a committee member on 12 international IEEE, ICO, and SPIE conferences. He has co-authored more than 150 publications including 35 journal articles and 20 invited conference papers. He is co-recipient of the 2008 IEEE Donald G. Fink Prize Paper Award.</p>
    </bio>
    <ack>
      <title>Acknowledgments</title>
      <p>The authors’ acknowledge the support from the Irish Research Council for Science, Engineering, and Technology, funded by the National Development Plan.</p>
    </ack>
  </back>
  <floats-group>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>Encoding of a schedule within the GA, with <inline-formula><alternatives><textual-form specific-use="jats-markup"> − 1</textual-form><mml:math id="M22" altimg="si1.gif" display="inline" overflow="scroll"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> delimiting processor queues. Each number corresponds to a unique task ID, thus allowing for a mapping of tasks to processors.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Fig. 2</label>
      <caption>
        <p>Execution time (ms) of PN scheduler with a fixed number of generations and a fixed mutation rate. The chromosome length corresponds to the number of tasks to be scheduled.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Fig. 3</label>
      <caption>
        <p>Number of generations run before stopping conditions terminate the evolution of the GA.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="fig4">
      <label>Fig. 4</label>
      <caption>
        <p>Average makespan achieved with varying numbers of generations in the GA scheduler.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="fig5">
      <label>Fig. 5</label>
      <caption>
        <p>Performance of each heuristic when used on its own to initialize the GA.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="fig6">
      <label>Fig. 6</label>
      <caption>
        <p>Performance of heuristics compared to our algorithm (PN) with real problems on a real heterogeneous distributed system, with normalized makespan.</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="fig7">
      <label>Fig. 7</label>
      <caption>
        <p>The number of idle clients in the system while the set of problems is being processed with the authors’ scheduling algorithm (PN).</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
    <table-wrap id="tbl1" position="float">
      <label>Table 1</label>
      <caption>
        <p>Client resources of different experimental setups.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left"/>
            <th align="left">Heterogeneity</th>
            <th align="left">No. proc</th>
            <th align="left">MFLOP/s</th>
            <th align="left">RAM (MB)</th>
            <th align="left">O/S</th>
            <th align="left">Processor type</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="8">A</td>
            <td align="left" rowspan="8">High</td>
            <td align="left">91</td>
            <td align="left">28–31</td>
            <td align="left">256</td>
            <td align="left">Linux</td>
            <td align="left">P3 600 MHz</td>
          </tr>
          <tr>
            <td align="left">50</td>
            <td align="left">190–229</td>
            <td align="left">512</td>
            <td align="left">Linux</td>
            <td align="left">P4 2.4 GHz</td>
          </tr>
          <tr>
            <td align="left">4</td>
            <td align="left">15</td>
            <td align="left">192</td>
            <td align="left">Linux</td>
            <td align="left">P2 266 MHz</td>
          </tr>
          <tr>
            <td align="left">1</td>
            <td align="left">154</td>
            <td align="left">1024</td>
            <td align="left">Windows</td>
            <td align="left">Centrino 1.4 GHz</td>
          </tr>
          <tr>
            <td align="left">1</td>
            <td align="left">25</td>
            <td align="left">512</td>
            <td align="left">Linux</td>
            <td align="left">P3 500 MHz</td>
          </tr>
          <tr>
            <td align="left">1</td>
            <td align="left">37</td>
            <td align="left">256</td>
            <td align="left">Linux</td>
            <td align="left">P3 1 GHz</td>
          </tr>
          <tr>
            <td align="left">1</td>
            <td align="left">72</td>
            <td align="left">256</td>
            <td align="left">Linux</td>
            <td align="left">P4 1.7 GHz</td>
          </tr>
          <tr>
            <td align="left">1</td>
            <td align="left">91</td>
            <td align="left">1024</td>
            <td align="left">FreeBSD</td>
            <td align="left">AMD 2400+XP</td>
          </tr>
          <tr>
            <td align="left" rowspan="2">B</td>
            <td align="left" rowspan="2">Low</td>
            <td align="left">45</td>
            <td align="left">28–31</td>
            <td align="left">256</td>
            <td align="left">Linux</td>
            <td align="left">P3 600 MHz</td>
          </tr>
          <tr>
            <td align="left">45</td>
            <td align="left">180–200</td>
            <td align="left">1024</td>
            <td align="left">Linux</td>
            <td align="left">P4 D820</td>
          </tr>
          <tr>
            <td align="left">C</td>
            <td align="left">Homogeneous</td>
            <td align="left">45</td>
            <td align="left">180–200</td>
            <td align="left">1024</td>
            <td align="left">Linux</td>
            <td align="left">P4 D820</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl2" position="float">
      <label>Table 2</label>
      <caption>
        <p>Taxonomy of schedulers.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Type</th>
            <th align="left">Key</th>
            <th align="left">Name</th>
            <th align="left">Reference</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="3">Immediate</td>
            <td align="left">RR</td>
            <td align="left">Round Robin</td>
            <td align="left"/>
          </tr>
          <tr>
            <td align="left">EF</td>
            <td align="left">Earliest first</td>
            <td align="left">
              <xref rid="b17" ref-type="bibr">[17]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">LL</td>
            <td align="left">Lightest loaded</td>
            <td align="left"/>
          </tr>
          <tr>
            <td align="left" rowspan="2">Batch</td>
            <td align="left">MM</td>
            <td align="left">Min–min</td>
            <td align="left">
              <xref rid="b8" ref-type="bibr">[8]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">MX</td>
            <td align="left">Max–min</td>
            <td align="left">
              <xref rid="b8" ref-type="bibr">[8]</xref>
            </td>
          </tr>
          <tr>
            <td align="left" rowspan="4">Evolutionary</td>
            <td align="left">SA</td>
            <td align="left">Simulated annealing</td>
            <td align="left">
              <xref rid="b14" ref-type="bibr">[14]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">ZO</td>
            <td align="left">Zomaya–Teh</td>
            <td align="left">
              <xref rid="b30" ref-type="bibr">[30]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">TA</td>
            <td align="left">Tabu search</td>
            <td align="left">
              <xref rid="b5" ref-type="bibr">[5]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">PN</td>
            <td align="left">This paper</td>
            <td align="left">Algorithm 1</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl3" position="float">
      <label>Table 3</label>
      <caption>
        <p>Comparison of problem properties.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Problem</th>
            <th align="left">Avg communication time (s)</th>
            <th align="left">Avg processing time (s)</th>
            <th align="left">P-to-C</th>
            <th align="left">No. tasks</th>
            <th align="left">Reference</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">SLTT</td>
            <td align="char">12.4</td>
            <td align="left">519.81</td>
            <td align="char">41.92</td>
            <td align="left">143</td>
            <td align="left">
              <xref rid="b23" ref-type="bibr">[23]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">DSEARCH</td>
            <td align="char">14.0</td>
            <td align="left">731.99</td>
            <td align="char">52.05</td>
            <td align="left">612</td>
            <td align="left">
              <xref rid="b12" ref-type="bibr">[12]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">MD5</td>
            <td align="char">14.4</td>
            <td align="left">235.52</td>
            <td align="char">16.36</td>
            <td align="left">800</td>
            <td align="left">
              <xref rid="b19" ref-type="bibr">[19]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">SHA1</td>
            <td align="char">64.5</td>
            <td align="left">543.02</td>
            <td align="char">8.42</td>
            <td align="left">900</td>
            <td align="left">
              <xref rid="b26" ref-type="bibr">[26]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">Elgamal</td>
            <td align="char">29.2</td>
            <td align="left">419.96</td>
            <td align="char">14.34</td>
            <td align="left">406</td>
            <td align="left">
              <xref rid="b4" ref-type="bibr">[4]</xref>
            </td>
          </tr>
          <tr>
            <td align="left">TSP</td>
            <td align="char">9.5</td>
            <td align="left">353.72</td>
            <td align="char">37.04</td>
            <td align="left">121</td>
            <td align="left"/>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl4" position="float">
      <label>Table 4</label>
      <caption>
        <p>Client resources used in the distributed system for the experiment shown in <xref rid="tbl5" ref-type="table">Table 5</xref>. The operating system on all clients was Linux.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">No. processors</th>
            <th align="left">MFLOP/s</th>
            <th align="left">RAM (MB)</th>
            <th align="left">Network link (Mb/s)</th>
            <th align="left">Processor type</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">47</td>
            <td align="left">180–200</td>
            <td align="left">1024</td>
            <td align="left">100</td>
            <td align="left">P4 D820</td>
          </tr>
          <tr>
            <td align="left">45</td>
            <td align="left">190–229</td>
            <td align="left">512</td>
            <td align="left">100</td>
            <td align="left">P4 2.4 GHz</td>
          </tr>
          <tr>
            <td align="left">32</td>
            <td align="left">28–31</td>
            <td align="left">256</td>
            <td align="left">10</td>
            <td align="left">P3 600 MHz</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl5" position="float">
      <label>Table 5</label>
      <caption>
        <p>Varying population size of the scheduling algorithm where the GA terminates if there is no improvement in makespan after 50 generations.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Population size</th>
            <th align="left">Makespan (s)</th>
            <th align="left">Scheduling time (s)</th>
            <th align="left">Mean scheduling time (s)</th>
            <th align="left">% Efficiency</th>
            <th align="left">% Communication costs</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">10</td>
            <td align="left">4653</td>
            <td align="left">24.0</td>
            <td align="left">0.31</td>
            <td align="left">80.4</td>
            <td align="left">0.407</td>
          </tr>
          <tr>
            <td align="left">20</td>
            <td align="left">4720</td>
            <td align="left">23.1</td>
            <td align="left">0.30</td>
            <td align="left">78.1</td>
            <td align="left">0.405</td>
          </tr>
          <tr>
            <td align="left">30</td>
            <td align="left">4701</td>
            <td align="left">26.3</td>
            <td align="left">0.30</td>
            <td align="left">86.4</td>
            <td align="left">0.444</td>
          </tr>
          <tr>
            <td align="left">40</td>
            <td align="left">4672</td>
            <td align="left">22.6</td>
            <td align="left">0.28</td>
            <td align="left">86.9</td>
            <td align="left">0.436</td>
          </tr>
          <tr>
            <td align="left">50</td>
            <td align="left">4649</td>
            <td align="left">29.7</td>
            <td align="left">0.33</td>
            <td align="left">83.3</td>
            <td align="left">0.436</td>
          </tr>
          <tr>
            <td align="left">60</td>
            <td align="left">4846</td>
            <td align="left">32.1</td>
            <td align="left">0.34</td>
            <td align="left">84.3</td>
            <td align="left">0.437</td>
          </tr>
          <tr>
            <td align="left">100</td>
            <td align="left">4686</td>
            <td align="left">25.7</td>
            <td align="left">0.31</td>
            <td align="left">80.9</td>
            <td align="left">0.463</td>
          </tr>
          <tr>
            <td align="left">1 000</td>
            <td align="left">4855</td>
            <td align="left">32.3</td>
            <td align="left">0.34</td>
            <td align="left">86.3</td>
            <td align="left">0.541</td>
          </tr>
          <tr>
            <td align="left">5 000</td>
            <td align="left">4720</td>
            <td align="left">24.8</td>
            <td align="left">0.31</td>
            <td align="left">83.1</td>
            <td align="left">0.418</td>
          </tr>
          <tr>
            <td align="left">10 000</td>
            <td align="left">4711</td>
            <td align="left">21.5</td>
            <td align="left">0.28</td>
            <td align="left">82.7</td>
            <td align="left">0.415</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl6" position="float">
      <label>Table 6</label>
      <caption>
        <p>Client resources used in the distributed system for the experiment shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">No. proc</th>
            <th align="left">MFLOP/s</th>
            <th align="left">RAM available (MB)</th>
            <th align="left">O/S</th>
            <th align="left">Network link (Mb/s)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">9</td>
            <td align="left">214</td>
            <td align="left">257–296</td>
            <td align="left">Windows</td>
            <td align="left">10</td>
          </tr>
          <tr>
            <td align="left">7</td>
            <td align="left">244</td>
            <td align="left">100</td>
            <td align="left">Windows</td>
            <td align="left">100</td>
          </tr>
          <tr>
            <td align="left">3</td>
            <td align="left">255</td>
            <td align="left">261–265</td>
            <td align="left">Windows</td>
            <td align="left">10</td>
          </tr>
          <tr>
            <td align="left">2</td>
            <td align="left">223</td>
            <td align="left">257–267</td>
            <td align="left">Windows</td>
            <td align="left">10</td>
          </tr>
          <tr>
            <td align="left">2</td>
            <td align="left">255</td>
            <td align="left">100</td>
            <td align="left">Windows</td>
            <td align="left">100</td>
          </tr>
          <tr>
            <td align="left">1</td>
            <td align="left">32</td>
            <td align="left">100</td>
            <td align="left">Windows</td>
            <td align="left">10</td>
          </tr>
          <tr>
            <td align="left">1</td>
            <td align="left">221</td>
            <td align="left">64</td>
            <td align="left">Linux</td>
            <td align="left">100</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl7" position="float">
      <label>Table 7</label>
      <caption>
        <p>Comparison of schedulers with a set of highly heterogeneous processors and a heterogeneous set of networking resources.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Scheduler</th>
            <th align="left">Makespan (s)</th>
            <th align="left">Scheduling time (s)</th>
            <th align="left">Mean scheduling time (s)</th>
            <th align="left">% Efficiency</th>
            <th align="left">% Communications</th>
            <th align="left">% Inefficiency</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">This paper (PN)</td>
            <td align="left">9 144</td>
            <td align="char">138.2</td>
            <td align="char">2.239</td>
            <td align="left">53.01</td>
            <td align="char">2.84</td>
            <td align="left">0</td>
          </tr>
          <tr>
            <td align="left">Zomaya–Teh (ZO)</td>
            <td align="left">14 278</td>
            <td align="char">276.1</td>
            <td align="char">1.904</td>
            <td align="left">33.94</td>
            <td align="char">2.24</td>
            <td align="left">56</td>
          </tr>
          <tr>
            <td align="left">Tabu (TA)</td>
            <td align="left">16 378</td>
            <td align="char">322.8</td>
            <td align="char">4.818</td>
            <td align="left">33.66</td>
            <td align="char">2.34</td>
            <td align="left">79</td>
          </tr>
          <tr>
            <td align="left">Sim. annealing (SA)</td>
            <td align="left">21 260</td>
            <td align="char">4605.4</td>
            <td align="char">47.478</td>
            <td align="left">30.07</td>
            <td align="char">5.95</td>
            <td align="left">132</td>
          </tr>
          <tr>
            <td align="left">Max–min (MX)</td>
            <td align="left">15 486</td>
            <td align="char">0.5</td>
            <td align="char">0.006</td>
            <td align="left">34.94</td>
            <td align="char">1.84</td>
            <td align="left">69</td>
          </tr>
          <tr>
            <td align="left">Min–min (MM)</td>
            <td align="left">18 321</td>
            <td align="char">0.4</td>
            <td align="char">0.005</td>
            <td align="left">32.21</td>
            <td align="char">2.30</td>
            <td align="left">100</td>
          </tr>
          <tr>
            <td align="left">Lightest loaded (LL)</td>
            <td align="left">19 645</td>
            <td align="char">0.1</td>
            <td align="char">0.001</td>
            <td align="left">25.05</td>
            <td align="char">1.82</td>
            <td align="left">114</td>
          </tr>
          <tr>
            <td align="left">Earliest first (EF)</td>
            <td align="left">14 492</td>
            <td align="char">0.4</td>
            <td align="char">0.004</td>
            <td align="left">46.88</td>
            <td align="char">10.96</td>
            <td align="left">58</td>
          </tr>
          <tr>
            <td align="left">Round Robin (RR)</td>
            <td align="left">20 314</td>
            <td align="char">0.1</td>
            <td align="char">0.001</td>
            <td align="left">31.90</td>
            <td align="char">9.10</td>
            <td align="left">122</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl8" position="float">
      <label>Table 8</label>
      <caption>
        <p>Comparison of schedulers with a set of 2 types of homogeneous processors and a heterogeneous set of networking resources.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Scheduler</th>
            <th align="left">Makespan (s)</th>
            <th align="left">Scheduling time (s)</th>
            <th align="left">Mean scheduling time (s)</th>
            <th align="left">% Efficiency</th>
            <th align="left">% Communications</th>
            <th align="left">% Inefficiency</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">This paper (PN)</td>
            <td align="left">8437</td>
            <td align="left">60.2</td>
            <td align="char">0.506</td>
            <td align="left">92.5</td>
            <td align="left">1.1</td>
            <td align="left">0</td>
          </tr>
          <tr>
            <td align="left">Zomaya–Teh (ZO)</td>
            <td align="left">8593</td>
            <td align="left">39.4</td>
            <td align="char">0.210</td>
            <td align="left">90.6</td>
            <td align="left">0.9</td>
            <td align="left">2</td>
          </tr>
          <tr>
            <td align="left">Tabu (TA)</td>
            <td align="left">8767</td>
            <td align="left">39.5</td>
            <td align="char">0.376</td>
            <td align="left">88.4</td>
            <td align="left">1.4</td>
            <td align="left">4</td>
          </tr>
          <tr>
            <td align="left">Sim. annealing (SA)</td>
            <td align="left">9564</td>
            <td align="left">3938.3</td>
            <td align="char">17.27</td>
            <td align="left">84.1</td>
            <td align="left">7.4</td>
            <td align="left">13</td>
          </tr>
          <tr>
            <td align="left">Max–min (MX)</td>
            <td align="left">9065</td>
            <td align="left">0.091</td>
            <td align="char">0.0008</td>
            <td align="left">87.3</td>
            <td align="left">1.1</td>
            <td align="left">7</td>
          </tr>
          <tr>
            <td align="left">Min–min (MM)</td>
            <td align="left">8860</td>
            <td align="left">0.166</td>
            <td align="char">0.0013</td>
            <td align="left">87.0</td>
            <td align="left">1.3</td>
            <td align="left">5</td>
          </tr>
          <tr>
            <td align="left">Lightest loaded (LL)</td>
            <td align="left">9053</td>
            <td align="left">0.021</td>
            <td align="char">0.0002</td>
            <td align="left">87.0</td>
            <td align="left">0.9</td>
            <td align="left">7</td>
          </tr>
          <tr>
            <td align="left">Earliest first (EF)</td>
            <td align="left">8602</td>
            <td align="left">0.089</td>
            <td align="char">0.0007</td>
            <td align="left">90.9</td>
            <td align="left">1.1</td>
            <td align="left">2</td>
          </tr>
          <tr>
            <td align="left">Round Robin (RR)</td>
            <td align="left">8812</td>
            <td align="left">0.096</td>
            <td align="char">0.0006</td>
            <td align="left">88.4</td>
            <td align="left">0.9</td>
            <td align="left">4</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tbl9" position="float">
      <label>Table 9</label>
      <caption>
        <p>Comparison of schedulers with a homogeneous set of processors.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Scheduler</th>
            <th align="left">Makespan (s)</th>
            <th align="left">Scheduling time(s)</th>
            <th align="left">Mean scheduling time (s)</th>
            <th align="left">% Efficiency</th>
            <th align="left">% Communications</th>
            <th align="left">% Inefficiency</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">This paper (PN)</td>
            <td align="left">10 408</td>
            <td align="left">50.4</td>
            <td align="char">0.49</td>
            <td align="left">96.9</td>
            <td align="left">1.3</td>
            <td align="left">1</td>
          </tr>
          <tr>
            <td align="left">Zomaya–Teh (ZO)</td>
            <td align="left">9 969</td>
            <td align="left">21.7</td>
            <td align="char">0.17</td>
            <td align="left">97.6</td>
            <td align="left">1.2</td>
            <td align="left">0</td>
          </tr>
          <tr>
            <td align="left">Tabu (TA)</td>
            <td align="left">10 126</td>
            <td align="left">22.3</td>
            <td align="char">0.23</td>
            <td align="left">97.5</td>
            <td align="left">1.3</td>
            <td align="left">1</td>
          </tr>
          <tr>
            <td align="left">Sim. annealing (SA)</td>
            <td align="left">10 351</td>
            <td align="left">1530.5</td>
            <td align="char">12.24</td>
            <td align="left">95.2</td>
            <td align="left">3.2</td>
            <td align="left">1</td>
          </tr>
          <tr>
            <td align="left">Max–min (MX)</td>
            <td align="left">12 034</td>
            <td align="left">0.05</td>
            <td align="char">0.01</td>
            <td align="left">81.8</td>
            <td align="left">1.1</td>
            <td align="left">20</td>
          </tr>
          <tr>
            <td align="left">Min–min (MM)</td>
            <td align="left">13 788</td>
            <td align="left">0.04</td>
            <td align="char">0.01</td>
            <td align="left">69.9</td>
            <td align="left">0.8</td>
            <td align="left">38</td>
          </tr>
          <tr>
            <td align="left">Lightest loaded (LL)</td>
            <td align="left">13 841</td>
            <td align="left">0.01</td>
            <td align="char">0.01</td>
            <td align="left">69.9</td>
            <td align="left">0.8</td>
            <td align="left">38</td>
          </tr>
          <tr>
            <td align="left">Earliest first (EF)</td>
            <td align="left">13 836</td>
            <td align="left">0.03</td>
            <td align="char">0.01</td>
            <td align="left">69.7</td>
            <td align="left">0.8</td>
            <td align="left">38</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </floats-group>
</article>