<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title-group>
        <journal-title>Neuroimage</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3094760</article-id>
      <article-id pub-id-type="pmid">21182971</article-id>
      <article-id pub-id-type="publisher-id">YNIMG7913</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2010.12.039</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Network discovery with DCM</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Friston</surname>
            <given-names>Karl J.</given-names>
          </name>
          <email>k.firston@fil.ion.ucl.ac.uk</email>
          <xref rid="af0005" ref-type="aff">a</xref>
          <xref rid="cr0005" ref-type="corresp">â</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Baojuan</given-names>
          </name>
          <xref rid="af0005" ref-type="aff">a</xref>
          <xref rid="af0015" ref-type="aff">c</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Daunizeau</surname>
            <given-names>Jean</given-names>
          </name>
          <xref rid="af0005" ref-type="aff">a</xref>
          <xref rid="af0010" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Stephan</surname>
            <given-names>Klaas E.</given-names>
          </name>
          <xref rid="af0005" ref-type="aff">a</xref>
          <xref rid="af0010" ref-type="aff">b</xref>
        </contrib>
      </contrib-group>
      <aff id="af0005"><label>a</label>The Wellcome Trust Centre for Neuroimaging, University College London, Queen Square, London WC1N 3BG, UK</aff>
      <aff id="af0010"><label>b</label>Laboratory for Social and Neural Systems Research, Dept. of Economics, University of Zurich, Switzerland</aff>
      <aff id="af0015"><label>c</label>College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, Hunan, 410073, PR China</aff>
      <author-notes>
        <corresp id="cr0005"><label>â</label>Corresponding author. The Wellcome Trust Centre for Neuroimaging, Institute of Neurology, UCL, Queen Square, London WC1N 3BG, UK. Fax: +44 207 813 1445. <email>k.firston@fil.ion.ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>01</day>
        <month>6</month>
        <year>2011</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <day>01</day>
        <month>6</month>
        <year>2011</year>
      </pub-date>
      <volume>56</volume>
      <issue>3-4</issue>
      <fpage>1202</fpage>
      <lpage>1221</lpage>
      <history>
        <date date-type="received">
          <day>30</day>
          <month>9</month>
          <year>2010</year>
        </date>
        <date date-type="rev-recd">
          <day>5</day>
          <month>12</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>13</day>
          <month>12</month>
          <year>2010</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â© 2011 Elsevier Inc.</copyright-statement>
        <copyright-year>2011</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>This paper is about inferring or discovering the functional architecture of distributed systems using Dynamic Causal Modelling (DCM). We describe a scheme that recovers the (dynamic) Bayesian dependency graph (connections in a network) using observed network activity. This network discovery uses Bayesian model selection to identify the sparsity structure (absence of edges or connections) in a graph that best explains observed time-series. The implicit adjacency matrix specifies the form of the network (e.g., cyclic or acyclic) and its graph-theoretical attributes (e.g., degree distribution). The scheme is illustrated using functional magnetic resonance imaging (fMRI) time series to discover functional brain networks. Crucially, it can be applied to experimentally evoked responses (activation studies) or endogenous activity in task-free (resting state) fMRI studies. Unlike conventional approaches to network discovery, DCM permits the analysis of directed and cyclic graphs. Furthermore, it eschews (implausible) Markovian assumptions about the serial independence of random fluctuations. The scheme furnishes a network description of distributed activity in the brain that is optimal in the sense of having the greatest conditional probability, relative to other networks. The networks are characterised in terms of their connectivity or adjacency matrices and conditional distributions over the directed (and reciprocal) effective connectivity between connected nodes or regions. We envisage that this approach will provide a useful complement to current analyses of functional connectivity for both activation and resting-state studies.</p>
      </abstract>
      <abstract abstract-type="graphical">
        <title>Research Highlights</title>
        <p>â–ºA new way of analysing effective connectivity. â–ºA way of searching over large sets of models (networks) using DCM. â–ºIdentifies the best network or graph using Bayesian model evidence. â–ºCan be applied to resting state and activation studies.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Bayesian</kwd>
        <kwd>Neuronal</kwd>
        <kwd>Generalised Filtering</kwd>
        <kwd>Dynamic Causal Modelling</kwd>
        <kwd>fMRI</kwd>
        <kwd>Random differential equations</kwd>
        <kwd>Stochastic</kwd>
        <kwd>Resting-state</kwd>
        <kwd>Connectivity</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="s0005">
      <title>Introduction</title>
      <p>Historically, Dynamic Causal Modelling (DCM) has been portrayed as a hypothesis-led approach to understanding distributed neuronal architectures underlying observed brain responses (<xref rid="bb0180" ref-type="bibr">Friston et al., 2003</xref>). Generally, competing hypotheses are framed in terms of different networks or graphs, and Bayesian model selection is used to quantify the evidence for one network (hypothesis) over another (<xref rid="bb0345" ref-type="bibr">Penny et al., 2004</xref>). However, in recent years, the number of models over which people search (the model-space) has grown enormously; to the extent that DCM is now used to discover the best model over very large model-spaces (e.g., <xref rid="bb0440 bb0355" ref-type="bibr">Stephan et al., 2010; Penny et al., 2010</xref>). Here, we take this discovery theme one step further and throw away prior knowledge about the experimental causes of observed responses to make DCM entirely data-led. This enables network discovery using observed responses during both activation studies and (task-free) studies of autonomous or endogenous activity during the â€œresting stateâ€. In what follows, we describe this scheme in terms of the underlying generative model, the construction of model-spaces and how these spaces are searched for the optimum model. In addition to covering the theoretical background to DCM discovery, we illustrate its application to an fMRI (attention to motion) dataset used previously in technical papers. In subsequent work, we will illustrate its application to resting-state fMRI data (Li et al., in preparation).</p>
      <p>This paper comprises four sections. In the first, we describe the form of the DCM used in subsequent sections. This is exactly the same as the conventional DCM for fMRI but includes endogenous fluctuations, which are represented by random differential equations (<xref rid="bb0290" ref-type="bibr">Li et al., 2010</xref>). These equations can be regarded as a [bi]linear approximation to any nonlinear model of neuronal dynamics. In this paper, we take a closer look at what this linear approximation means, when considering endogenous fluctuations that arise from self-organised dynamics (e.g., <xref rid="bb0445 bb0245" ref-type="bibr">Suckling et al., 2008; Honey et al, 2009</xref>). Having established the basic form of our model, we then turn to model inversion and consider briefly the distinction between deterministic and stochastic schemes. This distinction is important because stochastic or random fluctuations are inevitable, when modelling self-organised dynamics at a macroscopic scale. The second section deals with the construction of model-spaces and how each model within these spaces is evaluated or â€˜scoredâ€™. The main focus of this section will be on efficient scoring and plausible constraints or priors on models that restrict the search to sensible subspaces. We consider the difficult problem of scoring very large numbers of models. To finesse this problem we use a proxy for the model evidence based upon the conditional density over the parameters of a fully connected network (the Savageâ€“Dickey density ratio; <xref rid="bb0150" ref-type="bibr">Dickey, 1971</xref>). This section borrows several ideas from graph theory and connects them with Bayesian constructs in DCM. The resulting search scheme is used in Monte Carlo simulations to evaluate and validate the accuracy of model selection; i.e., network discovery. The third section applies these procedures to an empirical fMRI time-series, acquired under an attention to motion paradigm. This section illustrates the sorts of results that can be obtained and revisits some key questions about the functional architecture of hierarchies in the brain and the relative expression of top-down and bottom-up influences. We conclude with a brief discussion of the relevance of this scheme for determining neuronal architectures from measured brain responses in general, and its implications for the characterisation of fMRI time-series in particular.</p>
    </sec>
    <sec id="s0010">
      <title>The generative model and its inversion</title>
      <p>This section describes the causal or generative model for the specific fMRI application considered in this paper. The model is basically the same as the conventional DCM for fMRI; however, we will motivate the assumptions implicit in the usual approximation. This more detailed examination of DCM for fMRI discloses the central importance (and nature) of fluctuations in neurophysiologic states, which have been ignored in classical (deterministic) variants of Dynamic Causal Modelling.</p>
      <p>We have introduced several schemes recently that accommodate fluctuations on hidden neuronal and other physiological states (<xref rid="bb0350 bb0105 bb0195 bb0290" ref-type="bibr">Penny et al., 2005; Daunizeau et al, 2009; Friston et al., 2010; Li et al., 2010</xref>). This means that one can estimate hidden states generating observed data, while properly accommodating endogenous or random fluctuations. These become particularly important when modelling endogenous dynamics, which are itinerant (wandering) and sometimes ergodic (e.g., resting-state fMRI time-series). We will exploit these schemes to search over models that couple fluctuating dynamics in different parts of the brain. We first consider the generative model <italic>per se</italic> and then turn to its inversion or optimisation. Here, we take the opportunity to consider two alternative approaches to dealing with fluctuations in neuronal activity; the first is based upon Generalised Filtering for stochastic DCM described in <xref rid="bb0195" ref-type="bibr">Friston et al. (2010)</xref> and applied to fMRI in <xref rid="bb0290" ref-type="bibr">Li et al. (2010)</xref>. The nature of these generalised schemes speaks to the fact that there is no real difference between hidden states and parameters in DCM; therefore, it should be possible to cast unknown fluctuations in neuronal states as unknown parameters. In fact, this approach was used in the pioneering work of <xref rid="bb0385" ref-type="bibr">Riera et al. (2004)</xref>. We will address the implicit exchangeability of states and parameters by comparing stochastic DCM (<xref rid="bb0105 bb0290" ref-type="bibr">Daunizeau et al., 2009; Li et al., 2010</xref>) with deterministic DCMs that model unknown fluctuations in neuronal states with a mixture of temporal basis functions.</p>
      <sec id="s0015">
        <title>The generative model</title>
        <p>DCM for fMRI rests on a generative model that has two components. The first is a neuronal model describing interactions (dependencies) in a distributed network of neuronal populations. The second component maps neuronal activity to observed hemodynamic responses. This component has been described in detail many times previously and rests on a hemodynamic model (subsuming the Balloon model; <xref rid="bb0065 bb0180 bb0430" ref-type="bibr">Buxton et al., 1998; Friston et al, 2003; Stephan et al., 2007</xref>) and basically corresponds to a generalised (nonlinear) convolution. In this paper, we will focus exclusively on the neuronal model, because the hemodynamic part is exactly the same as described previously (<xref rid="bb0430" ref-type="bibr">Stephan et al., 2007</xref>). Although we will focus on neuronal systems, the following arguments apply to any complex distributed system with coupled nonlinear dynamics. This means that the procedures described later could (in principle) be applied in different domains.</p>
        <p>This material that follows is a bit abstract and could be skipped by the pragmatic reader. It is presented to make four key points: (i) the dynamics of coupled systems can be summarised with a small number of macroscopic variables that describe their behaviour; (ii) the time constants of these macroscopic dynamics are necessarily greater than those of the underlying macroscopic dynamics; (iii) reducing the dynamics to macroscopic variables necessarily induces fast fluctuations in these variables (cf., system noise) <italic>even if the system is deterministic</italic> and (iv) these fluctuations are analytic (continuously differentiable). The last point is crucial because it renders the model non-Markovian and calls for (inversion) schemes that eschew Markovian assumptions (e.g., Generalised Filtering: <xref rid="bb0195 bb0485" ref-type="bibr">Friston et al, 2010; Li et al., in press</xref>).</p>
        <p>Consider the system generating neurophysiologic time-series. This comprises a set of <italic>n</italic> regions, vertices or nodes, where each node corresponds to a vast number of neurons in a cortical area, source or spatial mode (pattern). We will first assume that the dynamics of neuronal states in one node <italic>Î¾</italic>Â =Â [<italic>Î¾</italic><sub>1</sub>, <italic>Î¾</italic><sub>2</sub>, â€¦]<sup><italic>T</italic></sup> evolve according to some unknown and immensely complicated equations of motion:<disp-formula id="fo0005"><label>(1)</label><mml:math id="M1" altimg="si3.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>Î¾</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mi>Î¾</mml:mi></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â‰œ</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>â€¦</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>These two equations represent the same dynamics but the first is expressed in terms of the original neuronal states (e.g., transmembrane voltages and conductances) of every neuron in the node, while the second equations are expressed in terms of the amplitude of patterns or modes of the original variables, <italic>U</italic>(<italic>Î¾</italic>)Â =Â <italic>U</italic><sup>âˆ’</sup><italic>Î¾</italic>. This alternative description can be regarded as a change of variables <italic>Î¾</italic>Â =Â <italic>UÎ¶</italic>Â â‡’Â <italic>Î¶</italic>Â =Â <italic>U</italic><sup>âˆ’</sup><italic>Î¾</italic>. We assume that this mapping <italic>U</italic><sup>âˆ’</sup>Â :Â <italic>Î¾</italic>Â â†’Â <italic>Î¶</italic> is chosen so that it conforms locally to the generalised eigenvectors <italic>U</italic>(<italic>Î¾</italic>) of the Jacobian <inline-formula><alternatives><textual-form specific-use="jats-markup">â„‘ = âˆ‚â€‰<italic>f</italic>â€‰/â€‰âˆ‚â€‰<italic>Î¾</italic></textual-form><mml:math id="M2" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="fraktur">I</mml:mi><mml:mo>=</mml:mo><mml:mo>âˆ‚</mml:mo><mml:mi>f</mml:mi><mml:mo>/</mml:mo><mml:mo>âˆ‚</mml:mo><mml:mi>Î¾</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, with eigenvalues <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>U</italic><sup> âˆ’ </sup>â„‘<italic>U</italic> = <italic>Î»</italic></textual-form><mml:math id="M3" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mo>âˆ’</mml:mo></mml:msup><mml:mi mathvariant="fraktur">I</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mi>Î»</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The Jacobian describes the stability of flow in state-space; i.e., how quickly flow changes with position. We now appeal (heuristically) to the centre manifold theorem and synergetic treatments of high-dimensional, self-organising systems (<xref rid="bb0210 bb0075 bb0225" ref-type="bibr">Ginzburg and Landau, 1950; Carr, 1981; Haken, 1983</xref>); see <xref rid="bb0120 bb0325 bb0115" ref-type="bibr">De Monte et al (2003), Melnik and Roberts, 2004 and Davis, 2006</xref>, for interesting examples and applications. Namely, we make the assumption that the eigenvalues <inline-formula><mml:math id="M4" altimg="si6.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mi>k</mml:mi><mml:mo>âˆ’</mml:mo></mml:msubsup><mml:mi mathvariant="fraktur">I</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> associated with each mode <italic>Î¶</italic><sub><italic>k</italic></sub>Â =Â <italic>U</italic><sub><italic>k</italic></sub><sup>âˆ’</sup><italic>Î¾</italic> are distributed sparsely; <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>Î»</italic><sub>1</sub> &gt; <italic>Î»</italic><sub>2</sub> &gt; <italic>Î»</italic><sub>3</sub>â€¦ âˆˆ â„œ</textual-form><mml:math id="M5" altimg="si7.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>â€¦</mml:mo><mml:mo>âˆˆ</mml:mo><mml:mi mathvariant="fraktur">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. That is, one or a small number of them are near zero, whereas the rest are large and negative. This assumption has additional plausibility for neuronal systems, given their tendency to show self-organised criticality and slowing (<xref rid="bb0425 bb0405 bb0445 bb0265" ref-type="bibr">Stam and de Bruin, 2004; Shin and Kim, 2006; Suckling et al., 2008; Kitzbichler et al., 2009</xref>). Critical slowing means that some modes decay slowly and show protracted correlations over time.</p>
        <p>Put simply, all this means is that the dynamics of any system comprising many elements can be decomposed into a mixture of (orthogonal) patterns over variables describing its state. By necessity, some of these patterns dissipate more quickly than others. Generally, some patterns decay so slowly that they predominate over others that disappear as soon as they are created. Mathematically, this means that <italic>P</italic> (principal) eigenvalues <italic>Î»</italic><sub><italic>p</italic></sub>Â â†’Â 0 : <italic>p</italic>Â â‰¤Â <italic>P</italic> are nearly zero and the associated eigenvectors or modes <italic>U</italic><sub><italic>p</italic></sub>(<italic>Î¾</italic>) are slow and unstable. In this case, <italic>Î¶</italic><sub><italic>p</italic></sub>Â =Â <italic>U</italic><sub><italic>p</italic></sub><sup>âˆ’</sup><italic>Î¾</italic>Â :Â <italic>p</italic>Â â‰¤Â <italic>P</italic> are known as order parameters. Order parameters are mixtures of states encoding the amplitude of the slow (unstable) modes that determine macroscopic behaviour. Other fast (stable) modes <italic>Î¶</italic><sub><italic>q</italic></sub>Â =Â <italic>U</italic><sub><italic>q</italic></sub><sup>âˆ’</sup><italic>Î¾</italic>Â :Â <italic>q</italic>Â &gt;Â <italic>P</italic> have large negative eigenvalues, which means that they decay or dissipate quickly to an invariant attracting set or manifold, <italic>h</italic>(<italic>Î¶</italic><sub><italic>p</italic></sub>), such that <inline-formula><mml:math id="M6" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In other words, the invariant (centre) manifold <italic>h</italic>(<italic>Î¶</italic><sub><italic>p</italic></sub>) attracts trajectories and contains the solutions to Eq.Â <xref rid="fo0005" ref-type="disp-formula">(1)</xref>. When there is only one order parameter or principal mode, this manifold is a line or curve in state-space and <italic>Î¶</italic><sub>1</sub> could represent the distance along that curve (see <xref rid="f0005" ref-type="fig">Fig.Â 1</xref>). The unstable fast modes decay quickly because the eigenvalue is effectively their rate of decay. One can see this easily by taking a first-order Taylor expansion of Eq.Â <xref rid="fo0005" ref-type="disp-formula">(1)</xref> about the centre manifold:<disp-formula id="fo0010"><label>(2)</label><mml:math id="M7" altimg="si9.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mi>q</mml:mi></mml:msub><mml:mo>â‰ˆ</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mi>q</mml:mi><mml:mo>âˆ’</mml:mo></mml:msubsup><mml:mi mathvariant="fraktur">I</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfenced><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>The slow modes (which flow on the centre manifold) are then said to enslave the fast modes (which the centre manifold attracts): This is the â€œslaving principleâ€ (<xref rid="bb0210 bb0075 bb0225" ref-type="bibr">Ginzburg and Landau, 1950; Carr, 1981; Haken, 1983</xref>). The crucial thing here is that the distribution of eigenvalues (rates of dissipation) induces a separation of temporal scales, so that we can approximate Eq.Â <xref rid="fo0005" ref-type="disp-formula">(1)</xref> with:<disp-formula id="fo0015"><label>(3)</label><mml:math id="M8" altimg="si10.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>Ï‰</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Ï‰</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mi>q</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:msub><mml:mi>Î¶</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mo>â€¦</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>Here, we have gathered the influences of the fast modes, on the motion of the slow modes, into fast fluctuations <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>Ï‰</italic><sub><italic>p</italic></sub> âˆˆ â„œ</textual-form><mml:math id="M9" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Ï‰</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mi mathvariant="fraktur">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> using a Taylor expansion about the centre manifold. We can do this because the states are generally near the centre manifold. Basically, we have thrown away the fast or stable modes and replaced them with fluctuations on the centre manifold. It should be noted that the transverse fluctuations <italic>Î¶</italic><sub><italic>q</italic></sub>(<italic>t</italic>) about the centre manifold are not necessarily small. Large fluctuations can occur fairly generically, if periodic orbits embedded in the centre manifold remain transversely unstable. In this case, the transverse dynamics can be of large amplitude, giving rise to what is known as bubbling (<xref rid="bb0010 bb0015" ref-type="bibr">Ashwin et al., 1994, 1996</xref>).</p>
        <p>An important example of the resulting contractions of state-space dynamics are those subtended by symmetries (or near symmetries) among the dynamics, such as the coupling of nearly identical systems (e.g., cortical macrocolumns). In such a setting, the centre manifold approximates a hyper-diagonal subspace or a smoothly mapped (synchronisation) manifold close by (e.g., <xref rid="bb0250" ref-type="bibr">Hu et al., 2010</xref>): The presence of strong transverse flow towards this manifold and a weakly stable or unstable flow on the manifold is exactly the sort of behaviour described by Eq.Â <xref rid="fo0015" ref-type="disp-formula">(3)</xref> and has clear relevance for cortical dynamics. Indeed, manifolds that arise from near symmetry in coupled dynamical systems have been studied extensively as models of synchronised neuronal activity (e.g. <xref rid="bb0025 bb0030" ref-type="bibr">Breakspear, 2004; Breakspear and Stam, 2005</xref>).</p>
        <p>Usually, the centre manifold theorem is used to characterise the dynamics on the centre manifold in terms of its bifurcations and structural stability, through normal forms for the associated equations of motion. Here, we are simply using the existence of a centre (or synchronisation) manifold to place formal priors on the form of a generative model. The resulting form (Eq.Â <xref rid="fo0015" ref-type="disp-formula">(3)</xref>) comprises slow deterministic dynamics and fast fluctuations that can be treated as analytic (differentiable) random terms. These fluctuations are analytic because they are a mixture of fast deterministic dynamics. Furthermore, in complex self-organising systems, they will exhibit smoothness. This is because some of the fast modes will show critical slowing (i.e., their eigenvalues will move towards zero). This is important because it means Eq.Â <xref rid="fo0015" ref-type="disp-formula">(3)</xref> is a random differential equation, not a stochastic differential equation (in which the random terms are Markovian and have no smoothness). Dynamic causal models based on random differential equations call for a slighter more sophisticated treatment than conventional state-space models based on stochastic differential equations (see below).</p>
        <p>In summary, we have exploited the separation of temporal scales seen in self-organising dynamics (and the ensuing adiabatic expansion implicit in Eq.Â <xref rid="fo0015" ref-type="disp-formula">3</xref>) to summarise the behaviour of a neuronal ensemble in terms of random differential equations. The key thing about this formulation is that the dynamics of order parameters are much slower than the fluctuations they enslave. We have not considered the exact nature of the order parameters <italic>Î¶</italic><sub><italic>p</italic></sub> but have in mind a single circular (phase) variable (see <xref rid="f0005" ref-type="fig">Fig.Â 1</xref>), such that the rate of change <inline-formula><mml:math id="M10" altimg="si12.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> reflects the instantaneous frequency of an oscillating mode (cf., <xref rid="bb0035 bb0270 bb0480" ref-type="bibr">Brown et al., 2004; Kopell and Ermentrout, 1986; Penny et al., 2009</xref>). If we define <inline-formula><mml:math id="M11" altimg="si13.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as the frequency of the <italic>i</italic>-th node and <inline-formula><mml:math id="M12" altimg="si14.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Ï‰</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>Ï‰</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as fluctuations in that frequency, Eq.Â <xref rid="fo0015" ref-type="disp-formula">(3)</xref> tells us that (dropping the subscript for clarity)<disp-formula id="fo0020"><label>(4)</label><mml:math id="M13" altimg="si15.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Â¨</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:mo>â‰ˆ</mml:mo><mml:mfrac><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mrow><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msup><mml:mi>Î¶</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>Ï‰</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>Î»</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Ï‰</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>Ï‰</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>This equation says that, in the absence of exogenous influences, each node will show smooth fluctuations in the frequency at which its principal mode oscillates. Note that the frequency will not decay because the underlying (quasiperiodic) dynamics are on the centre manifold, where <italic>Î»</italic><sup>(<italic>i</italic>)</sup>Â =Â âˆ‚<italic>f</italic><sup>(<italic>i</italic>)</sup>/âˆ‚<italic>Î¶</italic><sup>(<italic>i</italic>)</sup>Â =Â 0Â :Â âˆ€<italic>i</italic> (there is no imaginary component because there is only one order parameter). One could use either Eq.Â <xref rid="fo0015" ref-type="disp-formula">(3)</xref> or Eq.Â <xref rid="fo0020" ref-type="disp-formula">(4)</xref> as the formal basis of a generative model for neuronal dynamics. Both share the same key attribute; namely, smooth fluctuations on slow macroscopic dynamics. We will use Eq.Â <xref rid="fo0015" ref-type="disp-formula">(3)</xref> as the basis for stochastic DCMs for electromagnetic brain signals in future papers. When modelling fMRI data (in this paper) we will use Eq.Â <xref rid="fo0020" ref-type="disp-formula">(4)</xref>, because this summary of neuronal activity is the most prescient for fMRI responses. This is because it is generally assumed that fMRI signals scale with the predominant frequency of neuronal activity (<xref rid="bb0260 bb0280 bb0395" ref-type="bibr">Kilner et al., 2005; Laufs and Duncan, 2007; Rosa et al., 2010</xref>). We now turn to how different nodes are coupled and see how a separation of fast and slow dynamics in a distributed network of nodes provides a model for network dynamics. We will see that only the slow dynamics are communicated among nodes, which means we can model distributed activity with a small number of macroscopic variables (e.g. one per node) with fast fluctuations that are specific to each node.</p>
      </sec>
      <sec id="s0020">
        <title>Generative models of network activity</title>
        <p>To simplify the model of responses distributed over <italic>n</italic> nodes, we adopt a mean-field assumption (see <xref rid="bb0130" ref-type="bibr">Deco et al., 2008</xref>). This simply means that the dynamics of one node are determined by the mean or average activity in another. Intuitively, this is like assuming that each neuron in one node â€˜seesâ€™ a sufficiently large number of neurons in another to render the effective influence the same as the average over all neurons in the source node. The dynamics of these averages are enslaved by the slow modes of other regions so that the motion of the order parameter of each mode is a function of the order parameters from all nodes and local fluctuations:<disp-formula id="fo0025"><label>(5)</label><mml:math id="M14" altimg="si16.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:mfenced open="(" close=")"><mml:mi>Î¶</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mi>Ï‰</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Â¨</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mrow><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msup><mml:mi>Î¶</mml:mi><mml:mfenced open="(" close=")"><mml:mi>j</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>j</mml:mi></mml:mfenced></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>Ï‰</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â‡’</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mfenced open="(" close=")"><mml:mi>Î¶</mml:mi></mml:mfenced><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>Ï‰</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Î¶</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="[" close="]"><mml:mrow><mml:msup><mml:mi>Î¶</mml:mi><mml:mfenced open="(" close=")"><mml:mn>1</mml:mn></mml:mfenced></mml:msup><mml:mo>,</mml:mo><mml:mo>â‹¯</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>Î¶</mml:mi><mml:mfenced open="(" close=")"><mml:mi>n</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="[" close="]"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mn>1</mml:mn></mml:mfenced></mml:msup><mml:mo>,</mml:mo><mml:mo>â‹¯</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>n</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Ï‰</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="[" close="]"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>Ï‰</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mn>1</mml:mn></mml:mfenced></mml:msup><mml:mo>,</mml:mo><mml:mo>â‹¯</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>Ï‰</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mfenced open="(" close=")"><mml:mi>n</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mrow><mml:mrow><mml:mo>âˆ‚</mml:mo><mml:msup><mml:mi>Î¶</mml:mi><mml:mfenced open="(" close=")"><mml:mi>j</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfrac><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>Here <italic>A</italic>Â âŠ‚Â <italic>Î¸</italic> are unknown quantities or parameters encoding the effective connectivity or coupling among nodes. Crucially, this is the random differential equation used in stochastic DCM for fMRI (ignoring bilinear terms and exogenous inputs). In summary, we end up with a very simple model of neuronal dynamics that has been used for many years. In previous work, we motivated the deterministic variant of this model by a Taylor series approximation to unknown non-autonomous dynamics (<xref rid="bb0180 bb0435" ref-type="bibr">Friston et al., 2003; Stephan et al., 2008</xref>). Here, we have shown how this form emerges naturally from a basic but fundamental principle (the slaving principle), which applies to coupled dynamical systems that self-organise (<xref rid="bb0210 bb0225" ref-type="bibr">Ginzburg and Landau, 1950; Haken, 1983</xref>). It should be acknowledged that this model is less physiologically grounded than equivalent DCMs for electromagnetic data (where the hidden states are the voltages and currents of neural masses). The hidden neuronal states here are some (unspecified) phase-variable that reports the frequency at which neuronal states orbit an (unspecified) manifold: However, unlike our previous treatments, we have principled reasons to suppose this phase-variable (and its manifold) exist. In previous motivations, we represented the macroscopic behaviour of each node with one (<xref rid="bb0180" ref-type="bibr">Friston et al, 2003</xref>) or two (<xref rid="bb0305" ref-type="bibr">Marreiros et al, 2008</xref>) macroscopic neuronal states; with no motivation for why this was appropriate or sufficient. The current treatment provides that motivation and shows that using a small number of macroscopic states creates fast (analytic) fluctuations, which are ignored in deterministic models. Crucially, these fluctuations are mandated by the slaving principle, even in the absence of stochastic or random effects. This completes our specification of the generative model for distributed neuronal responses under adiabatic and mean field assumptions. We now turn to the inversion of this model, given empirical data.</p>
      </sec>
      <sec id="s0025">
        <title>Model inversion</title>
        <p>In DCM, models are usually inverted by optimising a free-energy bound <inline-formula><alternatives><textual-form specific-use="jats-markup">â„±(<italic>y</italic>, <italic>q</italic>) â‰¤ ln<italic>p</italic>(<italic>y</italic>|<italic>m</italic>)</textual-form><mml:math id="M15" altimg="si17.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>â‰¤</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> on the model log-evidence (or log marginal likelihood of the data <italic>y</italic> conditioned upon a model <italic>m</italic>), assuming the posterior is approximately Gaussian (the Laplace assumption). Optimising this bound, with respect to a proposal density, <italic>q</italic>(<italic>Ï‘</italic>), provides two things: First, it provides a free-energy or bound approximation <inline-formula><alternatives><textual-form specific-use="jats-markup">â„± â‰ˆ ln<italic>p</italic>(<italic>y</italic>|<italic>m</italic>)</textual-form><mml:math id="M16" altimg="si18.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>â‰ˆ</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> to the log-evidence. This will be used in the next section for model comparison or scoring. Second, it makes the proposal density an approximate conditional density <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>q</italic>(<italic>Ï‘</italic>) = ğ’©(<italic>Î¼</italic>, ğ’)</textual-form><mml:math id="M17" altimg="si19.gif" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>Ï‘</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>Î¼</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> on the unknown states and parameters <italic>Ï‘</italic>Â =Â {<italic>x</italic>, <italic>Î¸</italic>} of the model, given the data. This conditional density obtains from the construction of the free-energy, which is simply the log-evidence minus the divergence between the proposed and true conditional density. This means that maximising the free-energy minimises the difference between the two, such that the free-energy becomes an approximate log-evidence and the proposal density becomes an approximate conditional density (for technical details see <xref rid="bb0180 bb0185 bb0195" ref-type="bibr">Friston et al., 2003, 2007, 2010</xref>).</p>
        <p>The key thing that we need to consider here is the nature of the conditional density; in other words, what are the unknown states and parameters, <italic>Ï‘</italic>Â =Â {<italic>x</italic>, <italic>Î¸</italic>}? Above, we appealed to the separation of temporal scales (and the slaving principle) to separate the dynamics into a deterministic and a fluctuating part. These fluctuations mean that hidden neuronal states have to be represented probabilistically and call for DCMs that allow for system or state noise (cf. the fluctuations above). Recently, we introduced a Generalised Filtering scheme (<xref rid="bb0195" ref-type="bibr">Friston et al., 2010</xref>) that represents hidden states in generalised coordinates of motion and absorbs unknown (time-invariant) parameters into the filter. This scheme is efficient and allows one to infer on hidden states and parameters using models based on random differential equations like Eq.Â <xref rid="fo0025" ref-type="disp-formula">(5)</xref>. It accommodates random differential equations by representing the generalised motion of hidden states, which means that their fluctuations are analytic. We will use this scheme in subsequent sections. However, at this point, we note that there is an alternative to Generalised Filtering that uses a deterministic formulation, without random fluctuations. This scheme uses exogenous inputs in deterministic DCMs to model the fluctuations on neuronal states. Essentially, this converts the problem of inferring hidden states into a problem of inferring the parameters (coefficients) of temporal basis functions modelling unknown hidden states (cf. <xref rid="bb0385" ref-type="bibr">Riera et al., 2004</xref>). This rests on reformulating Eq.Â <xref rid="fo0020" ref-type="disp-formula">(4)</xref> to give<disp-formula id="fo0030"><label>(6)</label><mml:math id="M18" altimg="si20.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>Ï‰</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Ï‰</mml:mi><mml:msub><mml:mfenced open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub></mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mfenced open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced><mml:mi>j</mml:mi></mml:msub><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>Here, <italic>u</italic>(<italic>t</italic>)<sub><italic>j</italic></sub>Â :Â <italic>j</italic>Â =Â 1, â€¦, <italic>J</italic> is the <italic>j</italic>-th temporal basis function. In what follows, we use a discrete cosine basis set, where the number of components is one quarter of the length of the time series. This basis set was chosen because of its well known efficiency in modelling (compressing) typical signals. Note that this deterministic model ensures the fluctuations are smooth and analytic. Under this model, there is no uncertainty about the states, given the parameters <italic>Î¸</italic>Â âŠƒÂ {<italic>A</italic>, <italic>C</italic>}, and therefore our conditional density is just over the parameters. In short, the deterministic scheme optimises <italic>q</italic>(<italic>Ï‘</italic>)Â :Â =Â <italic>q</italic>(<italic>Î¸</italic>) by absorbing unknown fluctuations into the parameters, while Generalised Filtering absorbs unknown parameters into the fluctuating states.</p>
        <p>Eq.Â <xref rid="fo0030" ref-type="disp-formula">(6)</xref> has been introduced to show the formal connection between stochastic and deterministic DCM and promote a particular perspective on exogenous inputs: namely, that they are prior expectations (usually based on known experimental design) about hidden neuronal fluctuations. This perspective is exploited in the last section and provides a graceful link (conceptually and practically) between activation and resting-sate studies; i.e., activation studies can be treated as task-free and <italic>vice-versa</italic>. The only difference is the prior belief we have about the motion of hidden neuronal states. The following comparative evaluations of deterministic and stochastic formulations are not meant to be exhaustive or definitive but are presented to highlight when their formal connection breaks down.</p>
      </sec>
      <sec id="s0030">
        <title>Stochastic vs. deterministic models</title>
        <p>To compare and contrast the stochastic and deterministic schemes, we generated synthetic fMRI data using Eq.Â <xref rid="fo0025" ref-type="disp-formula">(5)</xref> and the hemodynamic equations of motion in the forward model of DCM for fMRI (<xref rid="bb0180" ref-type="bibr">Friston et al., 2003</xref>; <xref rid="bb0430" ref-type="bibr">Stephan et al., 2007</xref>). The results of these simulations are shown in <xref rid="f0010" ref-type="fig">Fig.Â 2</xref> and exhibit the characteristic amplitude and ultra slow fluctuations seen in resting state time-series. This figure shows the response of three nodes, over 256 (3.22Â s) time-bins, to smooth neuronal fluctuations that were generated independently for each region. These fluctuations were generated by smoothing a sequence of independent Gaussian variables so that they had a Gaussian autocorrelation function of two time-bins width and a log-precision of four (precision is inverse variance). Small, random fluctuations in the hemodynamic states (like normalised flow, volume and deoxyhemoglobin content) had a Gaussian autocorrelation width of half a time bin and a log-precision of sixteen. These values were chosen to produce a maximum signal change of about 1% in the fMRI signals. The coupling parameters used for this simulation used a small chain of three areas, with reciprocal connections (see black arrows in the insert in <xref rid="f0010" ref-type="fig">Fig.Â 2</xref>):<disp-formula id="fo0035"><label>(7)</label><mml:math id="M19" altimg="si21.gif" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.5</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>+</mml:mo><mml:mn>.4</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mo>+</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.5</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.4</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>The use of positive and negative coupling parameters produces the anti-correlated responses seen between the first two nodes and the third (<xref rid="f0010" ref-type="fig">Fig.Â 2</xref>, upper left panel). The remaining model parameters controlling the hemodynamic convolution (response function) were set to their usual priors (c.f. <xref rid="bb0180" ref-type="bibr">Friston et al., 2003</xref>) and scaled by a small Gaussian deviate with a log-precision of six (i.e., perturbed in a region-specific way by a random scaling with a standard deviation exp(âˆ’Â 6/2)Â =Â .0498 of about 5%). These synthetic data were then used for model inversion using (i) a conventional deterministic scheme (<xref rid="bb0180" ref-type="bibr">Friston et al., 2003</xref>) that modelled exogenous input with a discrete cosine set with 64 components and (ii) Generalised Filtering (<xref rid="bb0195" ref-type="bibr">Friston et al., 2010</xref>), respectively. In both schemes, we assumed a prior log-precision of six for observation noise and (for Generalised Filtering) a log-precision of six for hidden neural states and sixteen for the remaining hemodynamic states. This essentially treats neuronal fluctuations as the predominant source of hemodynamics and assumes hemodynamic fluctuations are largely neuronal in origin. The priors on the coupling parameters were mildly informative with a mean of zero and a precision of one half (a variance of two). The ensuing (marginal) conditional densities of the parameters <italic>q</italic>(<italic>A</italic>) for both schemes are shown in <xref rid="f0015 f0020" ref-type="fig">Figs.Â 3 and 4</xref>.</p>
        <p><xref rid="f0015" ref-type="fig">Fig.Â 3</xref> shows the conditional density of the coupling parameters for the deterministic scheme in terms of their 90% conditional confidence intervals (red bars) and true values (black bars). It can been seen that the deterministic scheme, modelling fluctuations with fixed temporal basis functions, underestimates the coupling strengths and consequently overestimates the amplitude of the neuronal fluctuations (represented by exogenous inputs) causing them (i.e., it overestimates the parameters <italic>C</italic>Â âŠ‚Â <italic>Î¸</italic>). This is not an invalid result, in that the true values lie within the conditional confidence intervals; however, this model provides inefficient estimates in relation to Generalised Filtering: <xref rid="f0020" ref-type="fig">Fig.Â 4</xref> shows the equivalent results for Generalised Filtering, which are much more accurate and precise (note the smaller confidence intervals in the upper left panel). In this model, there are no exogenous inputs because dynamics are explained by hidden fluctuations in neuronal and hemodynamic states. Here, the inferred neuronal fluctuations are much closer to the true values used to generate data (lower right panel). Crucially, in contrast to deterministic schemes, stochastic DCM also infers the hidden physiological states that mediate neurovascular coupling (e.g., flow, volume and deoxyhemoglobin content), which are shown in the upper right panel.</p>
        <p>Because the deterministic scheme used a discrete cosine set with 64 parameters to model neuronal fluctuations, it implicitly imposes smoothness constraints on the fluctuations. The equivalent smoothness constraints in Generalised Filtering come from priors on the precision of generalised fluctuations (i.e., fluctuations and their high-order temporal derivatives). Interestingly, the computational cost of using the Generalised Filtering scheme was less than the deterministic scheme (about 80 and 100Â s per iteration during model inversion, respectively). This was a bit surprising, because Generalised Filtering allows for random fluctuations, not just on the neuronal states, but all (four) hemodynamic states in each node or region (see <xref rid="f0020" ref-type="fig">Fig.Â 4</xref>). In other words, Generalised Filtering was not just estimating the neuronal states but all unknown physiological states. In short, (in this instance) there is little to be gained, either in terms of accuracy, completeness or computational efficiency, from the deterministic formulation. Therefore, we used Generalised Filtering for the rest of this work.</p>
      </sec>
      <sec id="s0035">
        <title>Summary</title>
        <p>In summary, this section has rehearsed the linear approximation to network dynamics used in DCM for fMRI, but from a new perspective. Here, we have taken care to develop this approximation from basic principles (e.g., centre manifold theorem and the slaving principle) and to highlight the role of endogenous fluctuations. These fluctuations model the dynamics attributable to fast (stable) modes that become enslaved by the slow (unstable) modes, which determine macroscopic behaviour. We then used a mean-field assumption to provide a generative model for distributed responses, cast as a random differential equation. One important insight from this motivation is that the time-constants (implicit in the model parameters) of macroscopic network dynamics are much longer than the microscopic time constants (e.g., effective membrane time constants). For example, fluctuations in the characteristic frequency of each mode (Eq.Â <xref rid="fo0020" ref-type="disp-formula">(4)</xref>) may be much slower (e.g., 100â€“10,000Â ms) than the oscillatory dynamics (e.g., 10 to 1000Â ms) of the (slow) modes themselves, which again are far slower than the dynamics of the fast modes (e.g., .1 to 10Â ms). This is important because it suggests that priors on the parameters should allow for slow dynamics. In the next section, we focus on the priors on the effective coupling matrix, <italic>A</italic>Â âŠ‚Â <italic>Î¸</italic>, which determines the network dynamics and its architecture.</p>
      </sec>
    </sec>
    <sec id="s0040">
      <title>Searching model-spaces</title>
      <p>Having established the form of the generative model and the optimisation of its parameters, we now turn to optimising the model <italic>per se</italic>. In terms of model optimisation or scoring, we are searching for the model that has the highest evidence or marginal likelihood. Usually, in DCM, one uses the free-energy bound as an approximation to the log-evidence. The problem we now contend with is how to score large numbers of models. For the purposes of network discovery, we can associate each model <italic>m</italic> with a particular sparsity structure on the effective connectivity matrix, <italic>A</italic>Â âŠ‚Â <italic>Î¸</italic>. In what follows, we will use several terms from graph theory: A graph comprises a set of nodes and edges (connections), where the edges are deployed according to an <italic>adjacency</italic> matrix <inline-formula><alternatives><textual-form specific-use="jats-markup">ğ’œ(<italic>m</italic>)</textual-form><mml:math id="M20" altimg="si22.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. This contains zero or non-zero elements that designate the absence or presence of a connection respectively. In general, graphs can be directed or undirected, cyclic or acyclic. We will deal with directed cyclic graphs. This means that we allow for directed connections and for cycles or loops within the graph; this includes reciprocal connections between two nodes. It is worthwhile noting structural causal modelling based on Bayesian networks (belief networks or directed acyclic graphical models; <xref rid="bb0420 bb0340" ref-type="bibr">Spirtes et al, 2000; Pearl, 2009</xref>) generally deal with directed acyclic graphs; although there are treatments of linear cyclic graphs as models of feedback (<xref rid="bb0380" ref-type="bibr">Richardson and Spirtes, 1999</xref>). Furthermore, analyses of functional connectivity (and of diffusion tensor imaging data) only consider undirected graphs because the direction of the influence between two nodes is not accessible. This is because functional connectivity is the statistical dependence between two time-series, which has no inherent directionality; although one could argue that directed transfer entropy tests for functional connectivity over time (e.g., <xref rid="bb0295" ref-type="bibr">Lizier et al., 2010</xref>). We can relax these (undirected and acyclic) constraints, because we have an explicit (directed and cyclic) generative model of how data are produced. In what follows, we will consider restrictions on the size of the model-space that is searched, using priors based on each model's adjacency matrix. We then turn to approximate scoring, based upon the conditional densities on the coupling parameters of fully connected graphs from the previous section. Finally, we demonstrate the sensitivity and specificity of the scoring scheme, using simulated data.</p>
      <sec id="s0045">
        <title>Graphs, priors and dependencies</title>
        <p>In this section, we cast network discovery in terms of inference on Bayesian dependency graphs. A Bayesian dependency graph encodes conditional dependencies with edges among variables associated with each node of the graph. The absence of an edge (anti-edge) represents causal independence; i.e., changing the variable in a source node does not change the variable in the target node. This means that discovering the network entails discovering the anti-edges that determine the sparsity structure.</p>
        <p>It is important to realise that a dynamic causal model is (formally) a Bayesian dependency graph, in which the form of the dependencies among hidden states is described with deterministic or random differential equations. This means that a DCM can be structurally cyclic (e.g., nodes can be reciprocally connected); however, the underlying Bayesian dependency graph is acyclic. This is because the variables in a parent node change the motion of variables in their children, which can only affect their parent in the future. This precludes instantaneous (cyclic) dependencies. Formally speaking, for every DCM there is an equivalent Dynamic Bayesian Network (DBN), whose nodes represent variables at successive time points. The conditional dependencies among these nodes are specified by the solutions of the differential equations over the discrete time intervals of the DBN. Crucially, the equivalent DBN is acyclic because future variables cannot affect past variables. In short, although a DCM can be structurally cyclic, the implicit dynamic Bayesian network is acyclic.</p>
        <p>In the present context, establishing an anti-edge means inferring a DCM (graph) without a particular connection is more likely than the equivalent graph that includes the connection. The implicit difference in log-evidence for these two models is the log-Bayes factor and (by the Neymanâ€“Pearson Lemma) is the most efficient statistic for testing the relative likelihood of both models. This means, in principle, we have a straightforward way to identify conditional dependencies and, by scoring all possible models, discover the underlying dependency graph. Note that this can, in theory, finesse so called missing region problem (c.f., <xref rid="bb0390 bb0110" ref-type="bibr">Roebroeck et al., 2009; Daunizeau et al., in press</xref>) that can arise when a connection is inferred that is actually mediated by common input. This is because an exhaustive model search will preclude a false inference of conditional dependency between two unconnected nodes, provided the source of common input is part of the full model (and that one can invert it). Furthermore, DCM discovery discloses the underlying network in a way that equivalent analyses of functional connectivity cannot aspire to. This is because functional connectivity is simply the statistical dependence between two nodes that could be conditionally independent when conditioned on a third node. Having said this there are finessed functional connectivity analyses that use partial correlations (e.g., <xref rid="bb0310 bb0315 bb0415" ref-type="bibr">Marrelec et al., 2006, 2009; Smith et al., 2010</xref>). Indeed, the principal aim of structural causal modelling (<xref rid="bb0320 bb0420" ref-type="bibr">Meek, 1995; Spirtes et al., 2000</xref>; <xref rid="bb0340" ref-type="bibr">Pearl, 2009</xref>) is to identify these conditional independencies.</p>
        <p>An anti-edge requires that the effective connectivity between two nodes in a DCM is zero. This is enforced by a prior on the unknown coupling parameters, which defines a model. The variances of these priors can be encoded in an adjacency matrix: A prior variance of zero (i.e., no uncertainty) forces the posterior estimate to take its prior mean. Under zero mean priors (as for all coupling parameters in DCM for fMRI), a zero entry in the adjacency matrix thus prohibits an effective connection, establishing an anti-edge between the respective regions. Conversely, a finite value in the adjacency matrix means that the connection has finite variance and that its posterior estimate can take non-zero values. In short, the adjacency matrix from graph theory furnishes formal priors on coupling parameters <inline-formula><alternatives><textual-form specific-use="jats-markup">ğ’œ<sub><italic>ij</italic></sub>(<italic>m</italic>) = 0 â‡” <italic>p</italic>(<italic>A</italic><sub><italic>ij</italic></sub>|<italic>m</italic>) = <italic>Î´</italic>(0)</textual-form><mml:math id="M21" altimg="si23.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>â‡”</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Î´</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. This means that there are as many models as there are adjacency matrices. Although, in principle, it should be easy to optimise the model (adjacency matrix) with an exhaustive search, this is seldom possible in practice. This is because the combinatorics of deploying <italic>k</italic> edges among <italic>n</italic> nodes becomes unmanageable when the size of the graph is large.</p>
        <p>To finesse this problem we can assume all connections in the brain are directed and reciprocal. This (bidirectional coupling) assumption rests on longstanding anatomical observations (<xref rid="bb0475" ref-type="bibr">Zeki and Shipp, 1988</xref>) that it is rare for two cortical areas to be connected in the absence of a reciprocal connection (there are rare but important exceptions in sub-cortical circuits). More recently, this notion was confirmed in comprehensive analyses of large connectivity databases demonstrating a very strong tendency of cortico-cortical connections to be reciprocal (<xref rid="bb0275" ref-type="bibr">KÃ¶tter and Stephan, 2003</xref>). From a functional point of view, modern theories of brain function that appeal to the Bayesian brain, call on reciprocal message passing between units encoding predictions and prediction errors (<xref rid="bb0330 bb0160" ref-type="bibr">Mumford, 1992; Friston, 2008</xref>). Others theories that rest on reciprocal connections include belief propagation algorithms and Bayesian update schemes that have been proposed as metaphors for neuronal processing (<xref rid="bb0140" ref-type="bibr">Deneve, 2008</xref>). Despite this strong motivation for introducing symmetry constraints on the adjacency matrix, it should be noted that the assumption of reciprocal coupling is not necessary for network discovery; it is used here to demonstrate how prior beliefs can constrain model spaces. Furthermore, this constraint does not mean that the effective connection strengths are identical for both directions of a reciprocal connection: The posterior estimates of coupling can be very different.</p>
        <p>Even with this constraint, the number of models <inline-formula><alternatives><textual-form specific-use="jats-markup">|ğ’œ(<italic>m</italic><sub><italic>i</italic></sub>)| = 2<sup><italic>n</italic>(<italic>n</italic> âˆ’ 1)â€‰/â€‰2</sup></textual-form><mml:math id="M22" altimg="si24.gif" overflow="scroll"><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> can still be too great to explore exhaustively (see <xref rid="f0025" ref-type="fig">Fig.Â 5</xref>). For example, with three regions there are 8 models, for four regions there are 64, for eight regions there are 268,435,456; and so on. This means that there is a combinatoric explosion as one increases the number of nodes in the network. In what follows, we describe a procedure that deals with this problem by scoring models based on the inversion of just one (full) model.</p>
      </sec>
      <sec id="s0050">
        <title>Approximating the model evidence</title>
        <p>We want to find a simple way of scoring large numbers (thousands or millions) of models. We can do this by exploiting the fact that each model can be formed from a fully connected model by switching off various coupling parameters. If we can find a way to approximate the log-evidence of any reduced model (graph), nested within the full model, from the conditional density over the parameters of the full model, then we only need to invert a single model (i.e., the full model) to score an arbitrary number of reduced models extremely efficiently.</p>
        <p>More formally, we seek the log-evidence ln <italic>p</italic>(<italic>y</italic>|<italic>m</italic><sub><italic>i</italic></sub>) of model <italic>i</italic>. Here, <italic>m</italic><sub><italic>i</italic></sub> denotes a reduced model with a subset of (reduced) parameters <italic>Î¸</italic><sub><italic>i</italic></sub>Â âŠ‚Â <italic>Î¸</italic><sub><italic>F</italic></sub> that are zero; these define the anti-edges we are trying to discover. By definition, the likelihood of any data under <italic>m</italic><sub><italic>i</italic></sub> is the same as their likelihood under the full model, given the reduced parameters are zero. This means (via Bayes rule)<disp-formula id="fo0040"><label>(8)</label><mml:math id="M23" altimg="si25.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¸</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¸</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â‡’</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¸</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>âˆ’</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¸</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>The last term is just the log-evidence of the full model, which we will treat as zero, because log-evidences are only unique up to an additive constant. Eq.Â <xref rid="fo0040" ref-type="disp-formula">(8)</xref> says that the relative log-evidence of a reduced model, given some data, is equal to the log-posterior minus the log-prior that its reduced parameters are zero, under a full model. This is intuitively sensible; in that a conditional density over reduced parameters that is far from a prior of zero suggests the reduced parameters are needed to explain the data. Eq.Â <xref rid="fo0040" ref-type="disp-formula">(8)</xref> contains the Savageâ€“Dickey density ratio (<xref rid="bb0150" ref-type="bibr">Dickey, 1971</xref>; see also <xref rid="bb0170" ref-type="bibr">Friston and Penny, 2011</xref>) that is used for nested model comparison, and indeed all classical inference using the extra sum of squares principle (such as F-tests or analysis of variance, ANOVA).</p>
        <p>We can approximate the marginal posterior in Eq.Â <xref rid="fo0040" ref-type="disp-formula">(8)</xref> using the approximate conditional density <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>q</italic>(<italic>Ï‘</italic>|<italic>m</italic>) = ğ’©(<italic>Î¼</italic>, ğ’)</textual-form><mml:math id="M24" altimg="si26.gif" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>Ï‘</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>Î¼</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> from the inversion schemes considered in the previous section.<disp-formula id="fo0045"><label>(9)</label><alternatives><textual-form specific-use="jats-markup">ln<italic>p</italic>(<italic>y</italic>|<italic>m</italic><sub><italic>i</italic></sub>) â‰ˆ ln<italic>q</italic>(<italic>Î¸</italic><sub><italic>i</italic></sub> = 0|<italic>m</italic><sub><italic>F</italic></sub>) âˆ’ ln<italic>p</italic>(<italic>Î¸</italic><sub><italic>i</italic></sub> = 0|<italic>m</italic><sub><italic>F</italic></sub>).</textual-form><mml:math id="M25" altimg="si27.gif" overflow="scroll"><mml:mrow><mml:mo>ln</mml:mo><mml:mi mathvariant="italic">p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi mathvariant="italic">m</mml:mi><mml:mi mathvariant="italic">i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>â‰ˆ</mml:mo><mml:mo>ln</mml:mo><mml:mi mathvariant="italic">q</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¸</mml:mi><mml:mi mathvariant="italic">i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi mathvariant="italic">m</mml:mi><mml:mi mathvariant="italic">F</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>âˆ’</mml:mo><mml:mo>ln</mml:mo><mml:mi mathvariant="italic">p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>Î¸</mml:mi><mml:mi mathvariant="italic">i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi mathvariant="italic">m</mml:mi><mml:mi mathvariant="italic">F</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mtext>.</mml:mtext></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>Here <italic>q</italic>(<italic>Î¸</italic><sub><italic>i</italic></sub>Â =Â 0|<italic>m</italic><sub><italic>F</italic></sub>) is the marginal conditional density over the reduced parameters under the full model. Crucially, after inverting a single (full) model, we can score any new model using Eq.Â <xref rid="fo0045" ref-type="disp-formula">(9)</xref>. The reason this works is that the new (reduced) model is defined in terms of priors on quantities (parameters) that have been fully characterised during inversion of the full model. Furthermore, Eq.Â <xref rid="fo0045" ref-type="disp-formula">(9)</xref> provides an internal test of the quality of the free-energy bound on log-evidence. This is because the relative log-evidences anticipated by Eq.Â <xref rid="fo0045" ref-type="disp-formula">(9)</xref> should be the same as those following explicit inversion of each reduced model. In short, we have a way to scan all the models we are interested in and identify the model with the greatest evidence.</p>
        <p>To illustrate this <italic>post hoc</italic> model selection we repeated the simulations above using four nodes (with independent neuronal fluctuations in each region) and coupling parameters<disp-formula id="fo0050"><label>(10)</label><mml:math id="M26" altimg="si28.gif" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.5</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>+</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mo>+</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.5</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.5</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>+</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>+</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>.5</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
        <p>This corresponds to the coupling architecture (adjacency matrix) illustrated by the insert in <xref rid="f0025" ref-type="fig">Fig.Â 5</xref> (where the solid black arrows denote edges and the grey arrows denote anti-edges). We then evaluated the log-evidence using Eq.Â <xref rid="fo0045" ref-type="disp-formula">(9)</xref>, for all (64) models with a prior on the coupling parameters, defined in terms of allowable adjacency matrices<disp-formula id="fo0055"><label>(11)</label><mml:math id="M27" altimg="si29.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>Î </mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Î </mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>Ã—</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â€¦</mml:mo><mml:mo>,</mml:mo><mml:mn>64</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="script">A</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>âˆˆ</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>The conditional densities of the coupling parameters, following inversion of the full model, are shown in <xref rid="f0030" ref-type="fig">Fig.Â 6</xref> (upper left panel). As before, these are relatively accurate, with a slightly overconfident underestimate of the (negative) self-connections that had a prior precision of 128. The resulting log-evidences over the 64 models are depicted on the upper right, showing that seven of the reduced models had a greater log-evidence than the full model. Of these, the model with the true architecture had, under flat priors over models, the greatest posterior probability. The lower right panel shows the same results but in terms of model posteriors <italic>p</italic>(<italic>m</italic><sub><italic>i</italic></sub>|<italic>y</italic>)Â âˆÂ <italic>p</italic>(<italic>y</italic>|<italic>m</italic><sub><italic>i</italic></sub>) (Eq.Â <xref rid="fo0045" ref-type="disp-formula">(9)</xref>). To illustrate the dependency of the log-evidence on the size of each graph (model), we have plotted the log-evidence for each model as a function of its number of (reciprocal) connections (lower left panel). One can see that generally, models with more connections have greater evidence because they provide a more accurate explanation for the data. However, the best model within each graph size shows the opposite behaviour; when the number of connections exceeds the true number, the log-evidence diminishes and always falls below that of the true model (here zero, by definition, and denoted by the red dot). This reflects the fact that model evidence automatically penalises redundant parameters or complexity.</p>
        <p>To assess the accuracy of the free-energy bound on log-evidence, we explicitly inverted each model and recorded its free-energy. The results in <xref rid="f0035" ref-type="fig">Fig.Â 7</xref> testify to the quality of the free-energy bound and demonstrate a reasonable correspondence between the proxy in Eq.Â <xref rid="fo0045" ref-type="disp-formula">(9)</xref> and the log-evidence as approximated with the free-energy of each reduced model. To achieve this correspondence we had to apply a model prior that penalised each connection by a fixed amount (by subtracting a log-prior cost of 45.8 per connection). Strictly speaking this should not be necessary; however, Generalised Filtering optimises a posterior over parameters that is time-dependent (i.e., optimises the time or path integral of free-energy). This complicates the relationship between posteriors on parameters (which change with time) and priors (which do not). The free-energy used here is therefore based on the Bayesian parameter average over time (see <xref rid="bb0195" ref-type="bibr">Friston et al., 2010</xref>, AppendixÂ 2 for details). Despite this complication, it is reassuring to note that, in models with the correct size, both the <italic>post hoc</italic> and explicit log-evidence proxies identify the same and correct model (see the lower panel of <xref rid="f0035" ref-type="fig">Fig.Â 7</xref>).</p>
      </sec>
      <sec id="s0055">
        <title>Specificity and sensitivity</title>
        <p>Finally, we repeated the simulations described above 200 times and recorded the distance between the true model and the model with the largest evidence. Data were generated by sampling coupling parameters from a uniform distribution <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>A</italic><sub><italic>ij</italic></sub>~ğ’°(Â¼, Â½)</textual-form><mml:math id="M28" altimg="si30.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula> and switching the sign of reciprocal connections randomly. Connections were then eliminated using an adjacency matrix selected at random from the middle row of <xref rid="f0025" ref-type="fig">Fig.Â 5</xref> (lower panel). Self-connections were sampled from <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>A</italic><sub><italic>ii</italic></sub>~ğ’©( âˆ’ Â½, Â¼)</textual-form><mml:math id="M29" altimg="si31.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ii</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula>. Candidate simulations were discarded if the simulated data exceeded 2% BOLD signal change. The results of these Monte Carlo simulations are shown in <xref rid="f0040" ref-type="fig">Fig.Â 8</xref>: To assess the sensitivity and specificity of the discovery scheme, we used the prior covariance matrix of the model selected to record the number of false positives (when an anti-edge was falsely inferred to be present) and false negatives (when an edge was falsely inferred to be absent). However, this assessment was unnecessary because the model selection procedure attained 100% accuracy. In other words, the correct adjacency structure (model) was selected in all cases and, implicitly, the scheme had a 100% specificity and selectivity for identifying the presence or absence of a connection. This compares favourably with simulations using similar graphs and levels of noise that tested a comprehensive battery of functional connectivity analyses (<xref rid="bb0415" ref-type="bibr">Smith et al., 2010</xref>). Their results show that â€œin general correlation-based approaches can be quite successful, methods based on higher-order statistics are less sensitive, and lag-based approaches perform very poorly. More specifically: There are several methods that can give high sensitivity to network connection detection on good quality FMRI data, in particular, partial correlation, regularised inverse covariance estimation and several Bayes net methods; however, accurate estimation of connection directionality is more difficult to achieveâ€. <xref rid="bb0415" ref-type="bibr">Smith et al. (2010)</xref> generated realistic fMRI data using the same type of DCM used here; however, they focussed on the application of functional connectivity methods and (structurally) acyclic graphs. The current results show that it is possible to achieve 100% sensitivity and specificity, with cyclic graphs, provided one uses an appropriate generative model to make inferences about effective connectivity.</p>
        <p>The conditional means of individual connections are plotted against their true values (over all simulations) in <xref rid="f0040" ref-type="fig">Fig.Â 8</xref>, for the full model (upper left panel) and selected model (upper right panel). The key thing to note here is the shrinkage of the conditional estimates to the true value of zero, under the optimal model (see the central black dot in the upper right panel). This reflects the fact that this form of model selection implements automatic relevance determination (<xref rid="bb0300" ref-type="bibr">MacKay, 1995</xref>), by virtue of optimising the model evidence with respect to model hyperparameters; in this instance, the shrinkage priors prescribed by an adjacency matrix. Interestingly, there was a mild shrinkage to the true values in the remaining (relevant) connections. This is seen more clearly when plotting the change in conditional estimate against the error (lower panel). One would hope to see that these changes were positive when the error was negative (i.e., the estimate was too high) and <italic>vice versa</italic>. This is exactly what was found (on average).</p>
        <p>These results are presented to show that, in principle, it is fairly easy to identify the correct functional architecture of directed cyclic graphs, provided one uses an appropriate generative model and has sufficiently precise data. The data and noise in these simulations had a standard deviation of about .35 and exp(âˆ’Â 4/2)Â â‰ˆ.14, respectively, giving a signal to noise ratio of about 2.6. This is large for a single voxel but not untypical of eigenvariates or averages used to summarise regional activity. For comparison, <xref rid="bb0415" ref-type="bibr">Smith et al. (2010)</xref> used a noise level of .1% to 1% and fMRI signals with maximum amplitudes of about 4%, whereas we used a noise level of .14% and signals with maximum amplitudes of about 2%. For both the simulations and empirical analyses below we used 256 bins of 3.22Â s, corresponding to 13.7Â min of scanning time.</p>
      </sec>
      <sec id="s0060">
        <title>Summary</title>
        <p>In this section, we have cast network discovery in terms of optimising dynamic Bayesian dependency graphs (represented as DCMs) and considered how this translates into Bayesian model selection. We finessed the problem of searching large model-spaces on two fronts. First, motivated by empirical evidence on anatomical connectivity patterns, we restricted the model-space to bidirectional connections. Although helpful, this constraint is not, strictly speaking, necessary. More importantly, we introduced a proxy scoring scheme based upon the Savageâ€“Dickey density ratio. This works well for the time-series and levels of noise considered. Equipped with this scoring scheme, we can search over enormous model-spaces, while only inverting a single (full) DCM. For a typical fMRI study, model inversion takes about five to ten minutes on a modern computer and <italic>post hoc</italic> model selection takes a few seconds.</p>
        <p>It should be remembered that these simulation results show only that it is possible to recover the connectivity structure from realistic responses; however, this clearly rests on having the right generative model. In this section, we used the same model to generate and explain data. In the next section we turn to empirical data, where there is no such guarantee.</p>
      </sec>
    </sec>
    <sec id="s0065">
      <title>An empirical illustration</title>
      <p>In this section, we apply the procedures described in the previous section to an empirical dataset that has been used previously to describe developments in causal modelling and related analyses. We have deliberately chosen an activation study to show that DCM discovery can be applied to conventional studies as well as (design-free) resting-state studies. The interesting distinction between the two applications reduces to prior constraints on the fluctuations. In other words, as discussed in <xref rid="bb0290" ref-type="bibr">Li et al. (2010)</xref>, under stochastic DCM, designed or experimental manipulations furnish prior expectations about fluctuations in neuronal states. We can elect to include these priors or ignore them. In the analysis below, we throw these priors away and let the data tell us if our experimental manipulations had any discernable effect upon neuronal activity. We hoped to show that the inferred neuronal states did indeed reflect the experimental manipulations and, at the same time, discover the hierarchical (or non-hierarchical) architecture subtending observed responses. We are not suggesting that this is a good way to analyse activation studies; it just allows us to show the inversion scheme returns valid estimates of hidden states: However, applying stochastic DCM to activation data is potentially interesting, because it allows one to quantify how much neural activity can be attributed to evoked responses (i.e., the experimental design or exogenous inputs) relative to endogenous and recurrent activity. In what follows, we will briefly describe the data used for our analysis and then report the results of network discovery.</p>
      <sec id="s0070">
        <title>Empirical data</title>
        <p>These data were acquired during an attention to visual motion paradigm and have been used previously to illustrate psychophysiological interactions, structural equation modelling, multivariate autoregressive models, Kalman filtering, variational filtering, DEM and Generalised Filtering (<xref rid="bb0175 bb0040 bb0045" ref-type="bibr">Friston et al., 1997; BÃ¼chel and Friston, 1997, 1998</xref>; <xref rid="bb0180 bb0190 bb0195" ref-type="bibr">Friston et al., 2003, 2008, 2010</xref>; <xref rid="bb0230 bb0435 bb0290" ref-type="bibr">Harrison et al., 2003; Stephan et al., 2008; Li et al., 2010</xref>). Data were acquired from a normal subject at two Tesla using a Magnetom VISION (Siemens, Erlangen) whole body MRI system, during a visual attention study. Contiguous multi-slice images were obtained with a gradient echo-planar sequence (TEÂ =Â 40Â ms; TRÂ =Â 3.22 s; matrix sizeÂ =Â 64Â Ã—Â 64Â Ã—Â 32, voxel size 3Â Ã—Â 3Â Ã—Â 3Â mm). Four consecutive 100 scan sessions were acquired, comprising a sequence of ten scan blocks of five conditions. The first was a dummy condition to allow for magnetic saturation effects. In the second, <italic>Fixation</italic>, subjects viewed a fixation point at the centre of a screen. In an <italic>Attention</italic> condition, subjects viewed 250 dots moving radially from the centre at 4.7 degrees per second and were asked to detect changes in radial velocity. In <italic>No attention</italic>, the subjects were asked simply to view the moving dots. In a <italic>Static</italic> condition, subjects viewed stationary dots. The order of the conditions alternated between <italic>Fixation</italic> and visual stimulation (<italic>Static</italic>, <italic>No Attention</italic>, or <italic>Attention</italic>). In all conditions subjects fixated the centre of the screen. No overt response was required in any condition and there were no actual changes in the speed of the dots. The data were analysed using a conventional SPM analysis (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). The regions or nodes chosen for network analysis were selected in a rather <italic>ad hoc</italic> fashion and are used here simply to demonstrate procedural details; however, we were careful to avoid the danger highlighted by the analyses of <xref rid="bb0415" ref-type="bibr">Smith et al. (2010)</xref> who note: â€œâ€¦the use of functionally inaccurate ROIs (when defining the network nodes and extracting their associated time series) is extremely damaging to network estimationâ€. We therefore ensured that the regional summaries were defined functionally by selecting regions showing evoked responses. Six representative regions were defined as clusters of contiguous voxels surviving an (omnibus) <italic>F</italic>-test for all effects of interest at pÂ &lt;Â .001 (uncorrected) in the conventional SPM analysis. These regions were chosen to cover a distributed network (of largely association cortex) in the right hemisphere, from visual cortex to frontal eye fields (see <xref rid="t0005" ref-type="table">TableÂ 1</xref> for details). The activity of each region (node) was summarised with its principal eigenvariate to ensure an optimum weighting of contributions for each voxel with the ROI (see <xref rid="f0045" ref-type="fig">Fig.Â 9</xref>). In this example, one can see evoked responses in visual areas (every 60Â s) with a progressive loss of stimulus-bound activity and a hint of attentional modulation and other fluctuations in higher regions.</p>
      </sec>
      <sec id="s0075">
        <title>Model inversion and selection</title>
        <p>As for the simulated data of the previous section, we inverted a DCM with full connectivity using the first 256 volumes of the time-series. Because we did not know the level of observation noise in these data, we reduced the prior expectation of its log-precision to four; otherwise, the analyses of simulated and empirical data were identical. A summary of the conditional expectations of hidden states generating regional activity are shown in <xref rid="f0050" ref-type="fig">Fig.Â 10</xref> (upper right). The solid lines are time-dependent means and the grey regions are 90% confidence intervals (i.e., confidence tubes). These states comprise, for each region, neuronal activity, vasodilatory signal, normalised flow, volume and deoxyhemoglobin content, where the last three are log-states. These hidden states provide the predicted responses in the upper left panel for each region and the associated prediction errors (red dotted lines). The same data are plotted in the lower panels for the first four minutes of data acquisition, with hidden neuronal states on the left and hemodynamic states on the right (where log-states are plotted as states). These results are presented to show that inferred neuronal activity in the visual region (highlighted in blue) follows visual stimulation (grey filled areas â€” high for attention and low for no attention). This confirms that model inversion has effectively deconvolved neuronal activity from hemodynamic signals; and that this deconvolution is veridical, in relation to known experimental manipulations. Recall that the model was not informed of these manipulations but can still recover evoked responses. The associated hemodynamic states of all regions are shown on the lower right (blue highlights blood flow in the visual region). It can be seen that changes in blood flow are in the order of 10%, which is in the physiologically plausible range.</p>
        <p><xref rid="f0055" ref-type="fig">Fig.Â 11</xref> summarises the results of <italic>post hoc</italic> model selection. The inversion of the full model took about 16Â min (about 16 iterations of about one minute each), while the <italic>post hoc</italic> search took about 16Â s. The upper left panel shows the log-evidence profile over the 2<sup>15</sup>Â =Â 32,768 models considered (reflecting all possible combinations of bidirectional edges among the six nodes analysed). There is a reasonably clear optimum model. This is evident if we plot the implicit log-posterior as a model posterior (assuming flat priors over models), as shown on the upper right. In this case, we can be over 80% certain that a particular network architecture generated the observed fMRI data. The parameter estimates of the connections under the full model (left) and the selected model (right) are shown in the lower panels. One can see that three (bidirectional) connections have been switched off, as their parameter estimates are reduced to their prior value of zero. It is these anti-edges that define the architecture we seek. This is a surprisingly dense network, in which all but three of the fifteen reciprocal connections appear to be necessary to explain observed responses. This dense connectivity may reflect the fact we are using macroscopic regional summaries of activity (that may be engendered by sparse connections on a mesoscopic scale); it may also reflect the fact that we deliberately chose regions that play an integrative (associational) role in cortical processing (c.f., hubs in graph theory; <xref rid="bb0050" ref-type="bibr">Bullmore and Sporns, 2009</xref>). There is an interesting structure to the anti-edges that speaks to the well known segregation of dorsal and ventral pathways in the visual system (<xref rid="bb0455" ref-type="bibr">Ungerleider and Haxby, 1994</xref>): The missing connections are between (i) the superior temporal sulcus and the early visual system, and (ii) the (ventral) superior temporal sulcus/angular gyrus and (dorsal) posterior parietal cortex. On the other hand, there are strong effective connections from the visual system to the prefrontal cortex. This does not mean that there are direct (monosynaptic) connections between these regions; it means they show conditional dependencies that are mediated in a neuronally plausible (polysynaptic) fashion, which cannot be explained by regional activities in the other nodes we considered.</p>
        <p><xref rid="f0060" ref-type="fig">Fig.Â 12</xref> shows the underlying graph in anatomical and functional (spectral embedding) space. Note that these plots refer to undirected graphs, although our scheme provides separate estimates for both directions of reciprocal connections (we will look at directed connections strengths below). The upper panel shows the same regions depicted in <xref rid="f0045" ref-type="fig">Fig.Â 9</xref>, but now connected using the conditional means of the coupling parameters, under the reduced (optimal) model. The colour of the arrows reports the source of the strongest bidirectional connection, while its width represents its absolute (positive or negative) strength. This provides a description of the architecture in anatomical space. A more functionally intuitive depiction of this graph is provided in the lower panel. Here, we have used spectral embedding to place the nodes in a functional space, where the distance between them reflects the strength of bidirectional coupling. Spectral embedding uses the eigenvectors <italic>V</italic>Â =Â <italic>eig</italic>(<italic>L</italic>) (principal components), of the weighted graph Laplacian, to define locations that best capture the proximity or conditional dependence between nodes. The Laplacian is<disp-formula id="fo0060"><label>(12)</label><mml:math id="M30" altimg="si32.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mo>âˆ‘</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi mathvariant="italic">kj</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>W</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>â‰ </mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>W</italic> is a weighted adjacency matrix based on the conditional expectations of <italic>A</italic>Â âŠ‚Â <italic>Î¸</italic>. <xref rid="f0060" ref-type="fig">Fig.Â 12</xref> uses the first three eigenvectors to define this functional space. This is similar to multi-dimensional scaling but uses the graph Laplacian based upon a weighted adjacency matrix to define similarities. The weighted adjacency matrix was, in this case, simply the maximum (absolute) conditional estimate of bidirectional coupling parameters; <italic>W</italic><sub><italic>ij</italic></sub>Â =Â max(|<italic>A</italic><sub><italic>ij</italic></sub>|, |<italic>A</italic><sub><italic>ji</italic></sub>|).</p>
        <p>Spectral embedding suggests that the frontal eye fields (<italic>fef</italic>) play a central and supraordinate role in this network, in the sense that they are remote from the visual region but predominate in terms of the strength of their efferent connections. Interestingly, the prefrontal cortex (<italic>pfc</italic>) and visual region (<italic>vis</italic>) are the furthest apart in anatomical space but the closest pair of nodes in functional space. This reflects the strength of the coupling between these nodes and more generally the tight functional integration between visual and prefrontal areas during visual attention tasks (e.g., <xref rid="bb0145 bb0205" ref-type="bibr">Desimone and Duncan, 1995; Gazzaley et al., 2007</xref>). Note that this characterisation of the network is insensitive to the sign of connections. Before concluding, we now provide an exemplar analysis that can only be pursued using cyclic directed graphs with asymmetric reciprocal connections; namely an analysis of hierarchical structure.</p>
      </sec>
      <sec id="s0080">
        <title>Asymmetric connections and hierarchies</title>
        <p>Network analyses using functional connectivity or diffusion weighted MRI data cannot ask whether a connection is larger in one direction relative to another, because they are restricted to the analysis of undirected (simple) graphs. However, here we have the unique opportunity to exploit asymmetries in reciprocal connections and revisit questions about hierarchical organisation (e.g., <xref rid="bb0070 bb0240 bb0285 bb0375" ref-type="bibr">Capalbo et al., 2008; Hilgetag et al., 2000; Lee and Mumford, 2003; Reid et al., 2009</xref>). There are many interesting analyses that one could consider, given a weighted (and signed) adjacency matrix. Here, we will illustrate a simple analysis of functional asymmetries: Hierarchies are defined by the distinction between forward (bottom-up) and backward (top-down) connections. There are several strands of empirical and theoretical evidence to suggest that, in comparison to bottom-up influences, the net effects of top-down connections on their targets are inhibitory (e.g., by recruitment of local lateral connections; cf, <xref rid="bb0005 bb0095" ref-type="bibr">Angelucci and Bullier, 2003; Crick and Koch, 1998</xref>). Theoretically, this is consistent with predictive coding, where top-down predictions suppress prediction errors in lower levels of a hierarchy (e.g., <xref rid="bb0450 bb0160 bb0085" ref-type="bibr">Summerfield et al., 2006; Friston, 2008; Chen et al., 2009</xref>). One might therefore ask which hierarchical ordering of the nodes maximises the average strength of forward connections relative to their backward homologue? This can be addressed by finding the order that maximises an asymmetry index, derived from the estimated effective (directed) connection strengths:<disp-formula id="fo0065"><label>(13)</label><mml:math id="M31" altimg="si33.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo>âˆ‘</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>Ëœ</mml:mo></mml:mover><mml:mi mathvariant="italic">ij</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>Ëœ</mml:mo></mml:mover><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ji</mml:mi></mml:msub><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>The resulting order was <italic>vis</italic>, <italic>sts</italic>, <italic>pfc</italic>, <italic>ppc</italic>, <italic>ag</italic>, and <italic>fef</italic>, which is not dissimilar to the vertical deployment of the nodes in functional embedding space (<xref rid="f0060" ref-type="fig">Fig.Â 12</xref>; lower panel). The middle panel shows the asymmetry indices for each connection, based on the conditional estimates of the selected model. This is a pleasing result because it places the visual cortex at the bottom of the hierarchy and the frontal eye fields at the top, which we would expect from the functional anatomy of these regions. Note that there was nothing in the data selection or modelling that could bias the conditional estimates of directed coupling to produce this result. As such, it can be taken as an incidental face validation of the discovery scheme. Before closing, we now turn to a more explicit validation, using empirical data in which conditional dependencies among nodes are destroyed.</p>
      </sec>
      <sec id="s0085">
        <title>A null analysis</title>
        <p>As a final step towards demonstrating the face validity of the network discovery scheme, we examined whether the discovery scheme detects the absence of conditional dependencies. Conditional dependencies can be destroyed by phase-shuffling the empirical data from the example above to remove any dependencies among nodes, while preserving the within-node dependencies over time (i.e., their spectral properties). Phase-shuffling involves Fourier transforming each regional time-series, randomising the phases (independently in each region) and taking the inverse Fourier transform. Phase-shuffled data only contain evidence for a graph with no edges. <xref rid="f0065" ref-type="fig">Fig.Â 13</xref> reports the <italic>post hoc</italic> model selection results following inversion of phase-shuffled data using exactly the same format as <xref rid="f0045" ref-type="fig">Fig.Â 9</xref>. This selection should result in an edgeless graph, which is nearly the case but not quite: It can be seen that the log-evidence profile is much shallower in comparison to the analysis of unshuffled data (by an order of magnitude). This results in small model posteriors (upper right) that are distributed over several models. The models with fewer connections are towards the right of these profiles. The model with the greatest evidence retained four out of fifteen connections (see the conditional estimates under the reduced model on the lower right). This is a slightly disappointing result, because we would have hoped to have seen no edges survive model selection. However, there was little evidence for the graph with four connections relative to graphs with fewer connections (with log-Bayes factors of less than three; <xref rid="bb0255" ref-type="bibr">Kass and Raftery, 1995</xref>). In short, even with real data, the <italic>post hoc</italic> model selection proposed for network discovery appears to identify anti-edges, provided one pays attention to the relative evidence for alternative models. Clearly, to assess sensitivity in a classical (frequentist) sense, one would have to assess the distribution of the log-evidence of the most likely model, under the null hypothesis. However, this begs the question: What is the null model for the absence of a conditional dependence or anti-edge?</p>
      </sec>
      <sec id="s0090">
        <title>Summary</title>
        <p>In summary, we have seen how DCM can be applied in a purely data-led way to fMRI studies. In this instance, we used an activation study where we had some prior expectations about the form of the evoked responses. Despite the fact that these expectations were not part of the model, the inferred neural states conformed to what we hoped to elicit experimentally. Furthermore, without biasing inference on models, we disclosed a hierarchical organisation of visual and prefrontal processing areas that has reasonable construct validity in terms of known functional anatomy. A striking result from this data-led application was that the strength of backward connections can be greater than the strength of forward connections: Note all top-down connection from the frontal eye fields were stronger in absolute terms than the equivalent bottom-up connections (<xref rid="f0060" ref-type="fig">Fig.Â 12</xref>). This is entirely sensible, given the greater abundance of backward connections anatomically, both within the cortical hierarchy and from cortex to subcortical structures (e.g., <xref rid="bb0410" ref-type="bibr">Sillito and Jones, 2002</xref>). Furthermore, the importance of backward connections or top-down influences fits comfortably with predictive coding accounts of brain function, which emphasise the importance of predictions that are generated in a top-down fashion (<xref rid="bb0370 bb0155" ref-type="bibr">Rao and Ballard, 1999; Friston, 2005</xref>).</p>
      </sec>
    </sec>
    <sec id="s0095">
      <title>Discussion</title>
      <p>The quest for discovering causal network structure has a long history, and automatic procedures for determining optimal model structure, given empirical measurements, have played an increasingly important role. For example, various algorithmic search procedures have been proposed for inferring causal structure from association (or covariance) data, often under the framework of Bayesian networks (e.g., <xref rid="bb0215 bb0420 bb0340" ref-type="bibr">Glymour et al., 1987; Spirtes et al, 2000; Pearl, 2009</xref>).</p>
      <p>In the domain of neuroimaging, there has been a growing interest in searching model-spaces, both in the context of DCM (and other models of effective connectivity) and analyses of functional connectivity. For example, <xref rid="bb0055" ref-type="bibr">Bullmore et al. (2000)</xref> introduced an automatic search procedure for structural equation models of fMRI data, and <xref rid="bb0460" ref-type="bibr">ValdÃ©s-Sosa et al. (2005)</xref> has done important work on optimisation of multivariate autoregressive models, in terms of sparsity. Other important work in this area has looked at the efficiency of various correlation schemes and Granger causality, when identifying the sparsity and connectivity structure of real and simulated data (e.g., <xref rid="bb0090 bb0200 bb0415" ref-type="bibr">Cole et al., 2010; Gates et al., 2010; Smith et al., 2010</xref>). Finally, discovery of causal network structure from neuroimaging data has also been pursued in the context of Bayesian networks. <xref rid="bb0365" ref-type="bibr">Ramsey et al. (2010)</xref> introduced an â€œindependent multisample greedy equivalence searchâ€ algorithm (IMaGES) for fMRI data. This method uses the Bayesian information criterion (BIC; <xref rid="bb0400" ref-type="bibr">Schwarz, 1978</xref>) for automatic scoring of Markov equivalence classes of directed acyclic graphs (DAGs). The restriction to DAGs means, however, that IMaGES only returns acyclic (feed-forward) graphs of effective connectivity.</p>
      <p>It is difficult to comment upon the comparative performance of DCM, which deals with dynamic models, in relation to approaches that do not (see <xref rid="bb0465" ref-type="bibr">ValdÃ©s-Sosa et al., 2010</xref> for a full discussion). Other schemes that use dynamic graphs include Granger causality (<xref rid="bb0220" ref-type="bibr">Granger, 1969</xref>) and Dynamic Bayesian Networks (DBN: e.g., <xref rid="bb0060 bb0360" ref-type="bibr">Burge et al., 2009; Rajapakse and Zhou, 2007</xref>). However, there is a growing appreciation that Granger causality may not be appropriate for fMRI time-series (e.g., <xref rid="bb0335" ref-type="bibr">Nalatore et al, 2007</xref>) and performs poorly in comparison to structural (non-dynamic) approaches based upon partial correlations (<xref rid="bb0415" ref-type="bibr">Smith et al., 2010</xref>). Granger causality and DBN rest on the theory of Martingales (i.e. Markovian assumptions), which may be inappropriate for real dynamical systems, whose fast fluctuations are analytic and may themselves show critical slowing (i.e., non-Markovian or long-memory behaviour) (see <xref rid="bb0390 bb0165" ref-type="bibr">Roebroeck et al. (2009) and Friston (2009)</xref> for discussion). In fact, one motivation for inventing DCM was to address the shortcomings of autoregressive and underlying Markovian models. Having said this, the computational expediency of functional connectivity and Granger causal schemes mean that they can handle (in principle) vast numbers of nodes and may therefore play a helpful role in identifying candidate networks for the analyses of (directed) effective connectivity described in this paper.</p>
      <sec id="s0100">
        <title>Future work</title>
        <p>Clearly, much work lies ahead in determining the sorts of networks that can be discovered efficiently with the scheme considered here. There are several obvious issues that need exploring: First, we need to establish the level of observation noise that permits veridical discovery: Increasing levels of noise reduces the posterior confidence in non-zero connections and predisposes them to removal during <italic>post hoc</italic> optimisation. The level of noise used in the simulations is not unrealistic but guaranteed a strong connection could be estimated with a high degree of precision. For example, in results of <xref rid="f0030" ref-type="fig">Fig.Â 6</xref>, the difference in log-evidence between the best model and its nearest competitor was about six. This translates into a log-odds ratio of about exp(6)Â â‰ˆÂ 400Â :Â 1 or a Z-score of about 2.8. This reflects the efficiency of the model selection and explains why we were able to identify the correct model in all the simulations. We are currently assessing the sensitivity and specificity of <italic>post hoc</italic> model selection as a function of observation noise: The results in this paper can be regarded as proof of principle that it is possible to recover the true network, provided that one has ideal (but not untypical) data. Another key aspect that may determine the identifiability of certain connections is their relative strength and sign. By construction, all the reciprocal connections in our simulations had the same (positive or negative) sign. This is because we found that strong reciprocal connections with opposite signs were estimated inefficiently, with shrinkage to their prior mean of zero. This means that they are unlikely to survive <italic>post hoc</italic> optimisation. One can see heuristically why this occurs (in terms of conditional dependences); however, this and related issues need to be explored properly. Finally, both inversion of the full model and its <italic>post hoc</italic> optimisation are sensitive to the shrinkage priors over the parameters. We used fairly arbitrary (non informative) priors; however, these priors can themselves be optimised using the same formalism behind <italic>post hoc</italic> model optimisation (see <xref rid="bb0170" ref-type="bibr">Friston and Penny, 2011</xref>).</p>
        <p>We have illustrated networks with a relatively small number of nodes (two to six). In principle, the scheme can handle much larger networks; however, the time taken to invert the (full) model may become prohibitively long (because the number of free parameters increases quadratically with the number of nodes). Having said this, DCM is used routinely to invert models with thousands of free parameters (e.g. DCM for induced electromagnetic sources; <xref rid="bb0080" ref-type="bibr">Chen et al., 2008</xref>). One approach to large numbers of nodes (e.g., voxels) is to summarise distributed activity in terms of modes or patterns and then estimate the coupling among those patterns (cf, <xref rid="bb0080 bb0235" ref-type="bibr">Chen et al., 2008; Havlicek et al., 2010</xref>). In terms of the increase in the size of model space with the number of nodes; as noted by one of our reviewers, one could employ a greedy search using the <italic>post hoc</italic> log-evidence. In our current implementation of automatic <italic>post hoc</italic> searches, we eliminate redundant parameters, starting with the eight parameters that have the smallest effect on log-evidence when removed. This process is repeated until no more parameters are removed or less than eight parameters remain. Of course, one would restrict an exhaustive search of models to preclude those that violate prior beliefs.</p>
        <p>In this paper, we have modelled all the nonlinearities that cause (chaotic) itinerancy in real biological time-series as fluctuations in random differential equations. Because our DCM is a linear approximation, these nonlinearities are absorbed into the fluctuating terms that are inferred during model inversion. Fluctuations are important because they can predominate in certain contexts. For example, the patterns of synchronisation and coherent activity observed in resting-state time-series (both empirically and in simulations) can themselves wax and wane at a slower timescale. Indeed, it is commonly thought that the ultra slow fluctuations seen in fMRI may reflect a modulation of fast synchronised activity at the neuronal level that may be a principal determinant of observed BOLD signal (<xref rid="bb0260 bb0135 bb0125" ref-type="bibr">Kilner et al., 2005; Deco et al., 2009; de Pasquale et al., 2010</xref>). From the point of view of generative models, this suggests that the coupling parameters are themselves state and implicitly time-dependent. One can model this state-dependency, and ensuing itinerancy, by simply adding nonlinear (quadratic) terms to the coupling matrix as described in <xref rid="bb0435" ref-type="bibr">Stephan et al. (2008)</xref>. This provides a DCM based on nonlinear random differential equations that can, in principle, be inverted using Generalised Filtering. One of the reasons that we chose the attentional dataset was that we know that there are strong contextual (experimental) effects on the coupling that are usually ascribed to attentional modulation of intrinsic or extrinsic connections in the visual nodes of the network. This modulation has been variously modelled in terms of exogenous (experimental manipulations of attentional set) or endogenous (state-dependent) terms (<xref rid="bb0290" ref-type="bibr">Li et al., 2010</xref>). In future work, we hope to compare models with and without nonlinear (state-dependent) coupling using the inversion and selection schemes described above. In short, network discovery can also be applied to bilinear and nonlinear DCMs to discover functional architectures with nonlinear (state-dependent) effects.</p>
        <p>In a similar vein, model averaging and selection procedures currently applied to the free-energy approximations following inversion of reduced models can be applied to the <italic>post hoc</italic> log-evidence used for model discovery (see <xref rid="bb0440" ref-type="bibr">Stephan et al., 2010</xref> for an overview of these procedures). For example, in group studies (when treating the model as a fixed effect over subjects) one would simply add <italic>post hoc</italic> log-evidences to discover the best model over subjects and proceed in the usual way. Similarly, <italic>post hoc</italic> log-evidences can be used for random effects model selection for group studies. Again we will illustrate this in future application papers.</p>
      </sec>
      <sec id="s0105">
        <title>Conclusion</title>
        <p>In conclusion, we hope to have introduced a scheme that people may find useful when answering questions in a discovery or data-led fashion, while retaining powerful constraints on the way that those data were generated. We have also described a solution to searches on large model-spaces which finesse problems due to combinatorics on connections and computational overhead. We envisage that this approach could be useful in analysing resting-state studies (<xref rid="bb0100 bb0020 bb0470" ref-type="bibr">Damoiseaux and Greicius, 2009; Biswal et al., 2010; Van Dijk et al., 2010</xref>) or indeed any data reporting unknown or endogenous dynamics (e.g. sleep EEG). Although we have illustrated the approach using region specific summaries of fMRI data from an activation study, there is no reason why exactly the same approach could not be applied to the activity of distributed modes, such as those from principal or independent component analysis (cf, <xref rid="bb0235" ref-type="bibr">Havlicek et al., 2010</xref>). Finally, having access to the adjacency matrices summarising functional brain architectures (in terms of effective connectivity) opens the door to graph theoretic analyses that leverage important advances in network theory (e.g., <xref rid="bb0050" ref-type="bibr">Bullmore and Sporns, 2009</xref>).</p>
        <p>The schemes described in this paper are implemented in Matlab code and are available freely as part of the open-source software package SPM8 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). Furthermore, the attentional data set used in this paper can be downloaded from the above website, for people who want to reproduce the analyses described in this paper.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="bb0005">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Angelucci</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bullier</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Reaching beyond the classical receptive field of V1 neurons: horizontal or feedback axons?</article-title>
          <source>J. Physiol. Paris</source>
          <volume>2003</volume>
          <issue>97</issue>
          <year>2003</year>
          <fpage>141</fpage>
          <lpage>154</lpage>
          <pub-id pub-id-type="pmid">14766139</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0010">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ashwin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Buescu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Stewart</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Bubbling of attractors and synchronization of chaotic attractors</article-title>
          <source>Phys. Lett. A</source>
          <volume>193</volume>
          <year>1994</year>
          <fpage>126</fpage>
          <lpage>139</lpage>
        </element-citation>
      </ref>
      <ref id="bb0015">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ashwin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Buescu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Stewart</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>From attractor to chaotic saddle: a tale of transverse stability</article-title>
          <source>Nonlinearity</source>
          <volume>9</volume>
          <year>1996</year>
          <fpage>703</fpage>
          <lpage>737</lpage>
        </element-citation>
      </ref>
      <ref id="bb0020">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Biswal</surname>
              <given-names>B.B.</given-names>
            </name>
            <name>
              <surname>Mennes</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zuo</surname>
              <given-names>X.N.</given-names>
            </name>
            <name>
              <surname>Gohel</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kelly</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Beckmann</surname>
              <given-names>C.F.</given-names>
            </name>
            <name>
              <surname>Adelstein</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Buckner</surname>
              <given-names>R.L.</given-names>
            </name>
            <name>
              <surname>Colcombe</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dogonowski</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Ernst</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Fair</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Hampson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hoptman</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Hyde</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Kiviniemi</surname>
              <given-names>V.J.</given-names>
            </name>
            <name>
              <surname>KÃ¶tter</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>C.P.</given-names>
            </name>
            <name>
              <surname>Lowe</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Mackay</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Madden</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Madsen</surname>
              <given-names>K.H.</given-names>
            </name>
            <name>
              <surname>Margulies</surname>
              <given-names>D.S.</given-names>
            </name>
            <name>
              <surname>Mayberg</surname>
              <given-names>H.S.</given-names>
            </name>
            <name>
              <surname>McMahon</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Monk</surname>
              <given-names>C.S.</given-names>
            </name>
            <name>
              <surname>Mostofsky</surname>
              <given-names>S.H.</given-names>
            </name>
            <name>
              <surname>Nagel</surname>
              <given-names>B.J.</given-names>
            </name>
            <name>
              <surname>Pekar</surname>
              <given-names>J.J.</given-names>
            </name>
            <name>
              <surname>Peltier</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Petersen</surname>
              <given-names>S.E.</given-names>
            </name>
            <name>
              <surname>Riedl</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Rombouts</surname>
              <given-names>S.A.</given-names>
            </name>
            <name>
              <surname>Rypma</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Schlaggar</surname>
              <given-names>B.L.</given-names>
            </name>
            <name>
              <surname>Schmidt</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Seidler</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Siegle</surname>
              <given-names>G.J.</given-names>
            </name>
            <name>
              <surname>Sorg</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Teng</surname>
              <given-names>G.J.</given-names>
            </name>
            <name>
              <surname>Veijola</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Villringer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Walter</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Weng</surname>
              <given-names>X.C.</given-names>
            </name>
            <name>
              <surname>Whitfield-Gabrieli</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Williamson</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Windischberger</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zang</surname>
              <given-names>Y.F.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H.Y.</given-names>
            </name>
            <name>
              <surname>Castellanos</surname>
              <given-names>F.X.</given-names>
            </name>
            <name>
              <surname>Milham</surname>
              <given-names>M.P.</given-names>
            </name>
          </person-group>
          <article-title>Toward discovery science of human brain function</article-title>
          <source>Proc. Natl Acad. Sci. USA</source>
          <volume>107</volume>
          <year>2010</year>
          <fpage>4734</fpage>
          <lpage>4739</lpage>
          <pub-id pub-id-type="pmid">20176931</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0025">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Breakspear</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>â€œDynamicâ€ connectivity in neural systems: theoretical and empirical considerations</article-title>
          <source>Neuroinformatics</source>
          <volume>2</volume>
          <issue>2</issue>
          <year>2004</year>
          <fpage>205</fpage>
          <lpage>226</lpage>
          <pub-id pub-id-type="pmid">15319517</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0030">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Breakspear</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Stam</surname>
              <given-names>C.J.</given-names>
            </name>
          </person-group>
          <article-title>Dynamics of a neural system with a multiscale architecture</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <volume>360</volume>
          <issue>1457</issue>
          <year>2005</year>
          <fpage>1051</fpage>
          <lpage>1074</lpage>
          <comment>May 29</comment>
          <pub-id pub-id-type="pmid">16087448</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0035">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brown</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Moehlis</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>On the phase reduction and response dynamics of neural oscillator populations</article-title>
          <source>Neural Comput.</source>
          <volume>16</volume>
          <year>2004</year>
          <fpage>673</fpage>
          <lpage>715</lpage>
          <pub-id pub-id-type="pmid">15025826</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0040">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>BÃ¼chel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Modulation of connectivity in visual pathways by attention: cortical interactions evaluated with structural equation modelling and fMRI</article-title>
          <source>Cereb. Cortex</source>
          <volume>7</volume>
          <year>1997</year>
          <fpage>768</fpage>
          <lpage>778</lpage>
          <pub-id pub-id-type="pmid">9408041</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0045">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>BÃ¼chel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic changes in effective connectivity characterized by variable parameter regression and Kalman filtering</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>6</volume>
          <year>1998</year>
          <fpage>403</fpage>
          <lpage>408</lpage>
          <pub-id pub-id-type="pmid">9788081</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0050">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bullmore</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sporns</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <volume>10</volume>
          <issue>3</issue>
          <year>2009</year>
          <fpage>186</fpage>
          <lpage>198</lpage>
          <comment>Mar</comment>
          <pub-id pub-id-type="pmid">19190637</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0055">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bullmore</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Horwitz</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Honey</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>How good is good enough in path analysis of fMRI data?</article-title>
          <source>Neuroimage</source>
          <volume>11</volume>
          <year>2000</year>
          <fpage>289</fpage>
          <lpage>301</lpage>
          <pub-id pub-id-type="pmid">10725185</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0060">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Burge</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lane</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Link</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Clark</surname>
              <given-names>V.P.</given-names>
            </name>
          </person-group>
          <article-title>Discrete dynamic Bayesian network analysis of fMRI data</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>30</volume>
          <issue>1</issue>
          <year>2009</year>
          <fpage>122</fpage>
          <lpage>137</lpage>
          <pub-id pub-id-type="pmid">17990301</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0065">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Buxton</surname>
              <given-names>R.B.</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>E.C.</given-names>
            </name>
            <name>
              <surname>Frank</surname>
              <given-names>L.R.</given-names>
            </name>
          </person-group>
          <article-title>Dynamics of blood flow and oxygenation changes during brain activation: the Balloon model</article-title>
          <source>Magn. Res. Med.</source>
          <volume>39</volume>
          <year>1998</year>
          <fpage>855</fpage>
          <lpage>864</lpage>
        </element-citation>
      </ref>
      <ref id="bb0070">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Capalbo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Postma</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Goebel</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Combining structural connectivity and response latencies to model the structure of the visual system</article-title>
          <source>PLoS Comput. Biol.</source>
          <volume>4</volume>
          <year>2008</year>
          <fpage>e1000159</fpage>
          <pub-id pub-id-type="pmid">18769707</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0075">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Carr</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <chapter-title>Applications of Centre Manifold Theory</chapter-title>
          <year>1981</year>
          <publisher-name>Springer-Verlag</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0080">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>C.C.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic causal modelling of induced responses</article-title>
          <source>Neuroimage</source>
          <volume>41</volume>
          <issue>4</issue>
          <year>2008</year>
          <fpage>1293</fpage>
          <lpage>1312</lpage>
          <comment>Jul 15</comment>
          <pub-id pub-id-type="pmid">18485744</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0085">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>C.C.</given-names>
            </name>
            <name>
              <surname>Henson</surname>
              <given-names>R.N.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Kilner</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Forward and backward connections in the brain: a DCM study of functional asymmetries</article-title>
          <source>Neuroimage</source>
          <volume>45</volume>
          <issue>2</issue>
          <year>2009</year>
          <fpage>453</fpage>
          <lpage>462</lpage>
          <comment>Apr 1</comment>
          <pub-id pub-id-type="pmid">19162203</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0090">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cole</surname>
              <given-names>M.W.</given-names>
            </name>
            <name>
              <surname>Pathak</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Schneider</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Identifying the brain's most globally connected regions</article-title>
          <source>Neuroimage</source>
          <volume>49</volume>
          <issue>4</issue>
          <year>2010</year>
          <fpage>3132</fpage>
          <lpage>3148</lpage>
          <comment>Feb 15</comment>
          <pub-id pub-id-type="pmid">19909818</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0095">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Crick</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Koch</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Constraints on cortical and thalamic projections: the no-strong-loops hypothesis</article-title>
          <source>Nature</source>
          <volume>391</volume>
          <issue>6664</issue>
          <year>1998</year>
          <fpage>245</fpage>
          <lpage>250</lpage>
          <comment>Jan 15</comment>
          <pub-id pub-id-type="pmid">9440687</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0100">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Damoiseaux</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Greicius</surname>
              <given-names>M.D.</given-names>
            </name>
          </person-group>
          <article-title>Greater than the sum of its parts: a review of studies combining structural connectivity and resting-state functional connectivity</article-title>
          <source>Brain Struct. Funct.</source>
          <volume>213</volume>
          <issue>6</issue>
          <year>2009</year>
          <fpage>525</fpage>
          <lpage>533</lpage>
          <comment>Oct</comment>
          <pub-id pub-id-type="pmid">19565262</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0105">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
          </person-group>
          <article-title>Variational Bayesian identification and prediction of stochastic nonlinear dynamic causal models</article-title>
          <source>Physica D</source>
          <volume>238</volume>
          <issue>21</issue>
          <year>2009</year>
          <fpage>2089</fpage>
          <lpage>2118</lpage>
          <comment>Nov 1</comment>
          <pub-id pub-id-type="pmid">19862351</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0110">
        <mixed-citation publication-type="other">Daunizeau J, David, O., Stephan K. E (in press). Dynamic Causal Modelling: a critical review of the biophysical and statistical foundation. <italic>Neuroimage</italic>.</mixed-citation>
      </ref>
      <ref id="bb0115">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Davis</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Low-dimensional manifolds in reactionâ€“diffusion equations.Â 1. Fundamental aspects</article-title>
          <source>J. Phys. Chem. A</source>
          <volume>110</volume>
          <issue>16</issue>
          <year>2006</year>
          <fpage>5235</fpage>
          <lpage>5256</lpage>
          <comment>Apr 27</comment>
          <pub-id pub-id-type="pmid">16623450</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0120">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>De Monte</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>d'Ovidio</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Mosekilde</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Coherent regimes of globally coupled dynamical systems</article-title>
          <source>Phys. Rev. Lett.</source>
          <volume>90</volume>
          <issue>5</issue>
          <year>2003</year>
          <fpage>054102</fpage>
          <comment>Feb 7</comment>
          <pub-id pub-id-type="pmid">12633359</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0125">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>de Pasquale</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Della Penna</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Snyder</surname>
              <given-names>A.Z.</given-names>
            </name>
            <name>
              <surname>Lewis</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Mantini</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Marzetti</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Belardinelli</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ciancetta</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Pizzella</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Romani</surname>
              <given-names>G.L.</given-names>
            </name>
            <name>
              <surname>Corbetta</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Temporal dynamics of spontaneous MEG activity in brain networks</article-title>
          <source>Proc. Natl Acad. Sci. USA</source>
          <volume>107</volume>
          <issue>13</issue>
          <year>2010</year>
          <fpage>6040</fpage>
          <lpage>6045</lpage>
          <comment>Mar 30</comment>
          <pub-id pub-id-type="pmid">20304792</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0130">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deco</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Jirsa</surname>
              <given-names>V.K.</given-names>
            </name>
            <name>
              <surname>Robinson</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>Breakspear</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>The dynamic brain: from spiking neurons to neural masses and cortical fields</article-title>
          <source>PLoS Comput. Biol.</source>
          <volume>4</volume>
          <issue>8</issue>
          <year>2008</year>
          <fpage>e1000092</fpage>
          <comment>Aug 29</comment>
          <pub-id pub-id-type="pmid">18769680</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0135">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deco</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Jirsa</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>McIntosh</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Sporns</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>KÃ¶tter</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Key role of coupling, delay, and noise in resting brain fluctuations</article-title>
          <source>Proc. Natl Acad. Sci. USA</source>
          <volume>106</volume>
          <issue>25</issue>
          <year>2009</year>
          <fpage>10302</fpage>
          <lpage>10307</lpage>
          <comment>Jun 23</comment>
          <pub-id pub-id-type="pmid">19497858</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0140">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deneve</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian spiking neurons I: inference</article-title>
          <source>Neural Comput.</source>
          <volume>20</volume>
          <issue>1</issue>
          <year>2008</year>
          <fpage>91</fpage>
          <lpage>117</lpage>
          <comment>Jan</comment>
          <pub-id pub-id-type="pmid">18045002</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0145">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Desimone</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Duncan</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Neural mechanisms of selective visual attention</article-title>
          <source>Annu. Rev. Neurosci.</source>
          <volume>18</volume>
          <year>1995</year>
          <fpage>193</fpage>
          <lpage>222</lpage>
          <pub-id pub-id-type="pmid">7605061</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0150">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dickey</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>The weighted likelihood ratio, linear hypotheses on normal location parameters</article-title>
          <source>Ann. Stat.</source>
          <volume>42</volume>
          <year>1971</year>
          <fpage>204</fpage>
          <lpage>223</lpage>
        </element-citation>
      </ref>
      <ref id="bb0155">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>A theory of cortical responses</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <volume>360</volume>
          <year>2005</year>
          <fpage>815</fpage>
          <lpage>836</lpage>
          <pub-id pub-id-type="pmid">15937014</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0160">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical models in the brain</article-title>
          <source>PLoS Comput. Biol.</source>
          <volume>4</volume>
          <issue>11</issue>
          <year>2008</year>
          <fpage>e1000211</fpage>
          <comment>Nov</comment>
          <pub-id pub-id-type="pmid">18989391</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0165">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic causal modeling and Granger causality Comments on: the identification of interacting networks in the brain using fMRI: model selection, causality and deconvolution</article-title>
          <source>Neuroimage</source>
          <year>2009</year>
          <comment>Sep 19. [Epub ahead of print]</comment>
        </element-citation>
      </ref>
      <ref id="bb0170">
        <mixed-citation publication-type="other">Friston and Penny (2011). <italic>Post hoc</italic> model selection â€” under review.</mixed-citation>
      </ref>
      <ref id="bb0175">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>BÃ¼chel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fink</surname>
              <given-names>G.R.</given-names>
            </name>
            <name>
              <surname>Morris</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Rolls</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Dolan</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Psychophysiological and modulatory interactions in neuroimaging</article-title>
          <source>Neuroimage</source>
          <volume>6</volume>
          <year>1997</year>
          <fpage>218</fpage>
          <lpage>229</lpage>
          <pub-id pub-id-type="pmid">9344826</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0180">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Harrison</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic causal modelling</article-title>
          <source>Neuroimage</source>
          <volume>19</volume>
          <year>2003</year>
          <fpage>1273</fpage>
          <lpage>1302</lpage>
          <pub-id pub-id-type="pmid">12948688</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0185">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Mattout</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Trujillo-Barreto</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Variational free energy and the Laplace approximation</article-title>
          <source>Neuroimage</source>
          <volume>34</volume>
          <issue>1</issue>
          <year>2007</year>
          <fpage>220</fpage>
          <lpage>234</lpage>
          <comment>Jan 1</comment>
          <pub-id pub-id-type="pmid">17055746</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0190">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Trujillo-Barreto</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>DEM: a variational treatment of dynamic systems</article-title>
          <source>Neuroimage</source>
          <volume>41</volume>
          <issue>3</issue>
          <year>2008</year>
          <fpage>849</fpage>
          <lpage>885</lpage>
          <comment>Jul 1</comment>
          <pub-id pub-id-type="pmid">18434205</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0195">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Generalised Filtering</article-title>
          <source>Math. Probl. Eng.</source>
          <year>2010</year>
          <comment>Article ID 621670.</comment>
        </element-citation>
      </ref>
      <ref id="bb0200">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gates</surname>
              <given-names>K.M.</given-names>
            </name>
            <name>
              <surname>Molenaar</surname>
              <given-names>P.C.</given-names>
            </name>
            <name>
              <surname>Hillary</surname>
              <given-names>F.G.</given-names>
            </name>
            <name>
              <surname>Ram</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Rovine</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Automatic search for fMRI connectivity mapping: an alternative to Granger causality testing using formal equivalences among SEM path modeling, VAR, and unified SEM</article-title>
          <source>Neuroimage</source>
          <volume>50</volume>
          <issue>3</issue>
          <year>2010</year>
          <fpage>1118</fpage>
          <lpage>1125</lpage>
          <comment>Apr 15</comment>
          <pub-id pub-id-type="pmid">20060050</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0205">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gazzaley</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rissman</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cooney</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Rutman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Seibert</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Clapp</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>D'Esposito</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Functional interactions between prefrontal and visual association cortex contribute to top-down modulation of visual processing</article-title>
          <source>Cereb. Cortex</source>
          <volume>17</volume>
          <issue>Suppl 1</issue>
          <year>2007</year>
          <fpage>i125</fpage>
          <lpage>i135</lpage>
          <comment>Sep</comment>
          <pub-id pub-id-type="pmid">17725995</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0210">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ginzburg</surname>
              <given-names>V.L.</given-names>
            </name>
            <name>
              <surname>Landau</surname>
              <given-names>L.D.</given-names>
            </name>
          </person-group>
          <article-title>On the theory of superconductivity</article-title>
          <source>Zh. Eksp. Teor. Fiz.</source>
          <volume>20</volume>
          <year>1950</year>
          <fpage>1064</fpage>
        </element-citation>
      </ref>
      <ref id="bb0215">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Glymour</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Scheines</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Spirtes</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Kelly</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <chapter-title>Discovering Causal Structure: Artificial Intelligence, Philosophy of Science, and Statistical Modeling</chapter-title>
          <year>1987</year>
          <publisher-name>Academic Press</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0220">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Granger</surname>
              <given-names>C.W.J.</given-names>
            </name>
          </person-group>
          <article-title>Investigating causal relations by econometric models and cross-spectral methods</article-title>
          <source>Econometrica</source>
          <volume>37</volume>
          <year>1969</year>
          <fpage>414</fpage>
        </element-citation>
      </ref>
      <ref id="bb0225">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Haken</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <chapter-title>Synergistics: an introduction</chapter-title>
          <source>Non-Equilibrium Phase Transition and Self-Organisation in Physics, Chemistry and Biology</source>
          <edition>Third Edition</edition>
          <year>1983</year>
          <publisher-name>Springer Verlag</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0230">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Harrison</surname>
              <given-names>L.M.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Multivariate autoregressive modeling of fMRI time series</article-title>
          <source>Neuroimage</source>
          <volume>19</volume>
          <year>2003</year>
          <fpage>1477</fpage>
          <lpage>1491</lpage>
          <pub-id pub-id-type="pmid">12948704</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0235">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Havlicek</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Brazdil</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Calhoun</surname>
              <given-names>V.D.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic Granger causality based on Kalman filter for evaluation of functional network connectivity in fMRI data</article-title>
          <source>Neuroimage</source>
          <volume>53</volume>
          <issue>1</issue>
          <year>2010</year>
          <fpage>65</fpage>
          <lpage>77</lpage>
          <comment>Oct 15</comment>
          <pub-id pub-id-type="pmid">20561919</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0240">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hilgetag</surname>
              <given-names>C.C.</given-names>
            </name>
            <name>
              <surname>O'Neill</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Young</surname>
              <given-names>M.P.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical organization of macaque and cat cortical sensory systems explored with a novel network processor</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <volume>355</volume>
          <issue>1393</issue>
          <year>2000</year>
          <fpage>71</fpage>
          <lpage>89</lpage>
          <comment>Jan 29</comment>
          <pub-id pub-id-type="pmid">10703045</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0245">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Honey</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Sporns</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Cammoun</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gigandet</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Thiran</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Meuli</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hagmann</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Predicting human resting-state functional connectivity from structural connectivity</article-title>
          <source>Proc. Natl Acad. Sci. USA</source>
          <volume>106</volume>
          <issue>6</issue>
          <year>2009</year>
          <fpage>2035</fpage>
          <lpage>2040</lpage>
          <comment>Feb 10</comment>
          <pub-id pub-id-type="pmid">19188601</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0250">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>The existence of generalized synchronization of chaotic systems in complex networks</article-title>
          <source>Chaos</source>
          <volume>20</volume>
          <issue>1</issue>
          <year>2010</year>
          <fpage>013112</fpage>
          <comment>Mar</comment>
          <pub-id pub-id-type="pmid">20370267</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0255">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kass</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Raftery</surname>
              <given-names>A.E.</given-names>
            </name>
          </person-group>
          <article-title>Bayes factors</article-title>
          <source>J. Am. Stat. Assoc.</source>
          <volume>90</volume>
          <year>1995</year>
          <fpage>773</fpage>
          <lpage>795</lpage>
        </element-citation>
      </ref>
      <ref id="bb0260">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kilner</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Mattout</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Henson</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Hemodynamic correlates of EEG: a heuristic</article-title>
          <source>Neuroimage</source>
          <volume>28</volume>
          <issue>1</issue>
          <year>2005</year>
          <fpage>280</fpage>
          <lpage>286</lpage>
          <comment>Oct 15</comment>
          <pub-id pub-id-type="pmid">16023377</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0265">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kitzbichler</surname>
              <given-names>M.G.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Christensen</surname>
              <given-names>S.R.</given-names>
            </name>
            <name>
              <surname>Bullmore</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Broadband criticality of human brain network synchronization</article-title>
          <source>PLoS Comput. Biol.</source>
          <volume>5</volume>
          <issue>3</issue>
          <year>2009</year>
          <fpage>e1000314</fpage>
          <comment>Mar</comment>
          <pub-id pub-id-type="pmid">19300473</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0270">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kopell</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ermentrout</surname>
              <given-names>G.B.</given-names>
            </name>
          </person-group>
          <article-title>Symmetry and phase-locking in chains of weakly coupled oscillators</article-title>
          <source>Comm. Pure Appl. Math.</source>
          <volume>39</volume>
          <year>1986</year>
          <fpage>623</fpage>
          <lpage>660</lpage>
        </element-citation>
      </ref>
      <ref id="bb0275">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>KÃ¶tter</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
          </person-group>
          <article-title>Network participation indices: characterizing component roles for information processing in neural networks</article-title>
          <source>Neural Netw.</source>
          <volume>16</volume>
          <year>2003</year>
          <fpage>1261</fpage>
          <lpage>1275</lpage>
          <pub-id pub-id-type="pmid">14622883</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0280">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Laufs</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Duncan</surname>
              <given-names>J.S.</given-names>
            </name>
          </person-group>
          <article-title>Electroencephalography/functional MRI in human epilepsy: what it currently can and cannot do</article-title>
          <source>Curr. Opin. Neurol.</source>
          <volume>20</volume>
          <issue>4</issue>
          <year>2007</year>
          <fpage>417</fpage>
          <lpage>423</lpage>
          <comment>Aug</comment>
          <pub-id pub-id-type="pmid">17620876</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0285">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>T.S.</given-names>
            </name>
            <name>
              <surname>Mumford</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>
          <source>J. Opt. Soc. Am. A Opt. Image Sci. Vis.</source>
          <volume>20</volume>
          <issue>7</issue>
          <year>2003</year>
          <fpage>1434</fpage>
          <lpage>1448</lpage>
          <comment>Jul</comment>
          <pub-id pub-id-type="pmid">12868647</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0290">
        <mixed-citation publication-type="other">Li B, Daunizeau J, Stephan KE, Penny W, Friston KJ (2010). Stochastic DCM and generalised filtering. Under revision.</mixed-citation>
      </ref>
      <ref id="bb0485">
        <mixed-citation publication-type="other">Li, B., Daunizeau, J., Stephan, K.E., Penny, W., Hu, D., Friston, K., in press. Generalised filtering and stochastic DCM for fMRI. NeuroImage. <ext-link ext-link-type="doi" xlink:href="10.1016/j.neuroimage.2011.01.085">doi:10.1016/j.neuroimage.2011.01.085</ext-link>.</mixed-citation>
      </ref>
      <ref id="bb0295">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lizier</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Heinzle</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Horstmann</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Prokopenko</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Multivariate information-theoretic measures reveal directed information structure and task relevant changes in fMRI connectivity</article-title>
          <source>J. Comput. Neurosci.</source>
          <year>2010</year>
          <comment>Aug 27. [Epub ahead of print]</comment>
        </element-citation>
      </ref>
      <ref id="bb0300">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>MacKay</surname>
              <given-names>D.J.C.</given-names>
            </name>
          </person-group>
          <article-title>Probable networks and plausible predictions â€” a review of practical Bayesian methods for supervised neural networks</article-title>
          <source>Netw. Comput. Neural Syst.</source>
          <volume>6</volume>
          <year>1995</year>
          <fpage>469</fpage>
          <lpage>505</lpage>
        </element-citation>
      </ref>
      <ref id="bb0305">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Marreiros</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic causal modelling for fMRI: a two-state model</article-title>
          <source>Neuroimage</source>
          <volume>39</volume>
          <issue>1</issue>
          <year>2008</year>
          <fpage>269</fpage>
          <lpage>278</lpage>
          <comment>Jan 1</comment>
          <pub-id pub-id-type="pmid">17936017</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0310">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Marrelec</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Krainik</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Duffau</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Pelegrini-Issac</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lehericy</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Doyon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Benali</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Partial correlation for functional brain interactivity investigation in functional MRI</article-title>
          <source>Neuroimage</source>
          <volume>32</volume>
          <year>2006</year>
          <fpage>228</fpage>
          <lpage>237</lpage>
          <pub-id pub-id-type="pmid">16777436</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0315">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Marrelec</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Doyon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Horwitz</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Large-scale neural model validation of partial correlation analysis for effective connectivity investigation in functional MRI</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>30</volume>
          <issue>3</issue>
          <year>2009</year>
          <fpage>941</fpage>
          <lpage>950</lpage>
          <pub-id pub-id-type="pmid">18344176</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0320">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Meek</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <chapter-title>Causal inference and causal explanation with background knowledge</chapter-title>
          <source>Proceedings of the 11th Conference on Uncertainty in artificial Intelligence</source>
          <year>1995</year>
          <fpage>403</fpage>
          <lpage>410</lpage>
        </element-citation>
      </ref>
      <ref id="bb0325">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Melnik</surname>
              <given-names>R.V.N.</given-names>
            </name>
            <name>
              <surname>Roberts</surname>
              <given-names>A.H.</given-names>
            </name>
          </person-group>
          <article-title>Computational models for multi-scale coupled dynamic problems</article-title>
          <source>Future Generation Comput Syst.</source>
          <volume>20</volume>
          <issue>3</issue>
          <year>2004</year>
          <fpage>453</fpage>
          <lpage>464</lpage>
        </element-citation>
      </ref>
      <ref id="bb0330">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mumford</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>On the computational architecture of the neocortex. II. The role of cortico-cortical loops</article-title>
          <source>Biol. Cybern.</source>
          <volume>66</volume>
          <year>1992</year>
          <fpage>241</fpage>
          <lpage>251</lpage>
          <pub-id pub-id-type="pmid">1540675</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0335">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nalatore</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rangarajan</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Mitigating the effects of measurement noise on Granger causality</article-title>
          <source>Phys. Rev. E Stat. Nonlin. Soft Matter Phys.</source>
          <volume>75</volume>
          <issue>3 Pt 1</issue>
          <year>2007</year>
          <fpage>031123</fpage>
          <comment>Mar</comment>
          <pub-id pub-id-type="pmid">17500684</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0340">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Pearl</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <chapter-title>Causality: Models, Reasoning and Inference</chapter-title>
          <edition>2nd edition</edition>
          <year>2009</year>
          <publisher-name>Cambridge University Press</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0480">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Litvak</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Fuentemilla</surname>
              <given-names>L..</given-names>
            </name>
            <name>
              <surname>Duzel</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic causal models for phase coupling</article-title>
          <source>J. Neurosci. Methods</source>
          <volume>183</volume>
          <issue>1</issue>
          <year>2009</year>
          <fpage>19</fpage>
          <lpage>30</lpage>
          <comment>Sep 30</comment>
          <pub-id pub-id-type="pmid">19576931</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0345">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Mechelli</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Comparing dynamic causal models</article-title>
          <source>Neuroimage</source>
          <volume>22</volume>
          <year>2004</year>
          <fpage>1157</fpage>
          <lpage>1172</lpage>
          <pub-id pub-id-type="pmid">15219588</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0350">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Ghahramani</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Bilinear dynamical systems</article-title>
          <source>Phil. Trans. R. Soc. B</source>
          <volume>360</volume>
          <issue>1457</issue>
          <year>2005</year>
          <fpage>983</fpage>
          <lpage>993</lpage>
          <pub-id pub-id-type="pmid">16087442</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0355">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Rosa</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Schofield</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Leff</surname>
              <given-names>A.P.</given-names>
            </name>
          </person-group>
          <article-title>Comparing families of dynamic causal models</article-title>
          <source>PLoS Comput. Biol.</source>
          <volume>6</volume>
          <issue>3</issue>
          <year>2010</year>
          <fpage>e1000709</fpage>
          <comment>Mar 12</comment>
          <pub-id pub-id-type="pmid">20300649</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0360">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rajapakse</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Learning effective brain connectivity with dynamic Bayesian networks</article-title>
          <source>Neuroimage</source>
          <volume>37</volume>
          <issue>3</issue>
          <year>2007</year>
          <fpage>749</fpage>
          <lpage>760</lpage>
          <comment>Sep 1</comment>
          <pub-id pub-id-type="pmid">17644415</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0365">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ramsey</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Hanson</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Hanson</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Halchenko</surname>
              <given-names>Y.O.</given-names>
            </name>
            <name>
              <surname>Poldrack</surname>
              <given-names>R.A.</given-names>
            </name>
            <name>
              <surname>Glymour</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Six problems for causal inference from fMRI</article-title>
          <source>Neuroimage</source>
          <volume>49</volume>
          <year>2010</year>
          <fpage>1545</fpage>
          <lpage>1558</lpage>
          <pub-id pub-id-type="pmid">19747552</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0370">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rao</surname>
              <given-names>R.P.</given-names>
            </name>
            <name>
              <surname>Ballard</surname>
              <given-names>D.H.</given-names>
            </name>
          </person-group>
          <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>
          <source>Nat. Neurosci.</source>
          <volume>2</volume>
          <year>1999</year>
          <fpage>79</fpage>
          <lpage>87</lpage>
          <pub-id pub-id-type="pmid">10195184</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0375">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reid</surname>
              <given-names>A.T.</given-names>
            </name>
            <name>
              <surname>Krumnack</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wanke</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>KÃ¶tter</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Optimization of cortical hierarchies with continuous scales and ranges</article-title>
          <source>Neuroimage</source>
          <volume>47</volume>
          <issue>2</issue>
          <year>2009</year>
          <fpage>611</fpage>
          <lpage>617</lpage>
          <comment>Aug 15</comment>
          <pub-id pub-id-type="pmid">19398021</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0380">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Richardson</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Spirtes</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <chapter-title>Automated discovery of linear feedback models</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Glymour</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Cooper</surname>
              <given-names>G.F.</given-names>
            </name>
          </person-group>
          <source>Computation, Causation, and Discovery</source>
          <year>1999</year>
          <publisher-name>MIT Press</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0385">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Riera</surname>
              <given-names>J.J.</given-names>
            </name>
            <name>
              <surname>Watanabe</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kazuki</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Naoki</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Aubert</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ozaki</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Kawashima</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>A state-space model of the hemodynamic approach: nonlinear filtering of BOLD signals</article-title>
          <source>Neuroimage</source>
          <volume>21</volume>
          <issue>2</issue>
          <year>2004</year>
          <fpage>547</fpage>
          <lpage>567</lpage>
          <comment>Feb</comment>
          <pub-id pub-id-type="pmid">14980557</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0390">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Roebroeck</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Formisano</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Goebel</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>The identification of interacting networks in the brain using fMRI: model selection, causality and deconvolution</article-title>
          <source>Neuroimage</source>
          <year>2009</year>
          <comment>Sep 25. [Epub ahead of print]</comment>
        </element-citation>
      </ref>
      <ref id="bb0395">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rosa</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Kilner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Blankenburg</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Josephs</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Estimating the transfer function from neuronal activity to BOLD using simultaneous EEG-fMRI</article-title>
          <source>Neuroimage</source>
          <volume>49</volume>
          <issue>2</issue>
          <year>2010</year>
          <fpage>1496</fpage>
          <lpage>1509</lpage>
          <comment>Jan 15</comment>
          <pub-id pub-id-type="pmid">19778619</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0400">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schwarz</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Estimating the dimension of a model</article-title>
          <source>Ann. Stat.</source>
          <volume>6</volume>
          <year>1978</year>
          <fpage>461</fpage>
          <lpage>464</lpage>
        </element-citation>
      </ref>
      <ref id="bb0405">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shin</surname>
              <given-names>C.W.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Self-organized criticality and scale-free properties in emergent functional neural networks</article-title>
          <source>Phys. Rev. E Stat. Nonlin. Soft Matter Phys.</source>
          <volume>74</volume>
          <issue>4 Pt 2</issue>
          <year>2006</year>
          <fpage>045101</fpage>
          <comment>Oct</comment>
          <pub-id pub-id-type="pmid">17155118</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0410">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sillito</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>H.E.</given-names>
            </name>
          </person-group>
          <article-title>Corticothalamic interactions in the transfer of visual information</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <volume>357</volume>
          <year>2002</year>
          <fpage>1739</fpage>
          <lpage>1752</lpage>
          <pub-id pub-id-type="pmid">12626008</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0415">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Miller</surname>
              <given-names>K.L.</given-names>
            </name>
            <name>
              <surname>Salimi-Khorshidi</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Webster</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Beckmann</surname>
              <given-names>C.F.</given-names>
            </name>
            <name>
              <surname>Nichols</surname>
              <given-names>T.E.</given-names>
            </name>
            <name>
              <surname>Ramsey</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Woolrich</surname>
              <given-names>M.W.</given-names>
            </name>
          </person-group>
          <article-title>Network modelling methods for FMRI</article-title>
          <source>Neuroimage</source>
          <year>2010</year>
          <comment>Sep 1. [Epub ahead of print]</comment>
        </element-citation>
      </ref>
      <ref id="bb0420">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Spirtes</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Glymour</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Scheines</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <source>Causation, prediction and search</source>
          <edition>2nd edition</edition>
          <year>2000</year>
          <publisher-name>MIT Press</publisher-name>
          <comment>Cambridge, MA</comment>
        </element-citation>
      </ref>
      <ref id="bb0425">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stam</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>de Bruin</surname>
              <given-names>E.A.</given-names>
            </name>
          </person-group>
          <article-title>Scale-free dynamics of global functional connectivity in the human brain</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>22</volume>
          <issue>2</issue>
          <year>2004</year>
          <fpage>97</fpage>
          <lpage>109</lpage>
          <comment>Jun</comment>
          <pub-id pub-id-type="pmid">15108297</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0430">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Weiskopf</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Drysdale</surname>
              <given-names>P.M.</given-names>
            </name>
            <name>
              <surname>Robinson</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Comparing hemodynamic models with DCM</article-title>
          <source>Neuroimage</source>
          <volume>38</volume>
          <year>2007</year>
          <fpage>387</fpage>
          <lpage>401</lpage>
          <pub-id pub-id-type="pmid">17884583</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0435">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Kasper</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Harrison</surname>
              <given-names>L.M.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>den Ouden</surname>
              <given-names>H.E.M.</given-names>
            </name>
            <name>
              <surname>Breakspear</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Nonlinear dynamic causal models for fMRI</article-title>
          <source>Neuroimage</source>
          <volume>42</volume>
          <year>2008</year>
          <fpage>649</fpage>
          <lpage>662</lpage>
          <pub-id pub-id-type="pmid">18565765</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0440">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Moran</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>den Ouden</surname>
              <given-names>H.E.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Ten simple rules for dynamic causal modeling</article-title>
          <source>Neuroimage</source>
          <volume>49</volume>
          <issue>4</issue>
          <year>2010</year>
          <fpage>3099</fpage>
          <lpage>3109</lpage>
          <comment>Feb 15</comment>
          <pub-id pub-id-type="pmid">19914382</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0445">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Suckling</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wink</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Bernard</surname>
              <given-names>F.A.</given-names>
            </name>
            <name>
              <surname>Barnes</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bullmore</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Endogenous multifractal brain dynamics are modulated by age, cholinergic blockade and cognitive performance</article-title>
          <source>J. Neurosci. Meth.</source>
          <volume>174</volume>
          <issue>2</issue>
          <year>2008</year>
          <fpage>292</fpage>
          <lpage>300</lpage>
          <comment>Sep 30</comment>
        </element-citation>
      </ref>
      <ref id="bb0450">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Summerfield</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Egner</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Mangels</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hirsch</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Mistaking a house for a face: neural correlates of misperception in healthy humans</article-title>
          <source>Cereb. Cortex</source>
          <volume>16</volume>
          <issue>4</issue>
          <year>2006</year>
          <fpage>500</fpage>
          <lpage>508</lpage>
          <comment>Apr</comment>
          <pub-id pub-id-type="pmid">16014866</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0455">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ungerleider</surname>
              <given-names>L.G.</given-names>
            </name>
            <name>
              <surname>Haxby</surname>
              <given-names>J.V.</given-names>
            </name>
          </person-group>
          <article-title>â€˜Whatâ€™ and â€˜whereâ€™ in the human brain</article-title>
          <source>Curr. Opin. Neurobiol.</source>
          <volume>4</volume>
          <issue>2</issue>
          <year>1994</year>
          <fpage>157</fpage>
          <lpage>165</lpage>
          <comment>Apr</comment>
          <pub-id pub-id-type="pmid">8038571</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0460">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>ValdÃ©s-Sosa</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>SÃ¡nchez-Bornot</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Lage-Castellanos</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vega-HernÃ¡ndez</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bosch-Bayard</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Melie-GarcÃ­a</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Canales-RodrÃ­guez</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Estimating brain functional connectivity with sparse multivariate autoregression</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <volume>360</volume>
          <issue>1457</issue>
          <year>2005</year>
          <fpage>969</fpage>
          <lpage>981</lpage>
          <comment>May 29</comment>
          <pub-id pub-id-type="pmid">16087441</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0465">
        <mixed-citation publication-type="other">ValdÃ©s-Sosa PA, Roebroeck, A, Daunizeau J, and Friston K. (2010). Effective connectivity: influence, causality and biophysical modelling. <italic>NeuroImage</italic>; under revision.</mixed-citation>
      </ref>
      <ref id="bb0470">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Dijk</surname>
              <given-names>K.R.</given-names>
            </name>
            <name>
              <surname>Hedden</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Venkataraman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>K.C.</given-names>
            </name>
            <name>
              <surname>Lazar</surname>
              <given-names>S.W.</given-names>
            </name>
            <name>
              <surname>Buckner</surname>
              <given-names>R.L.</given-names>
            </name>
          </person-group>
          <article-title>Intrinsic functional connectivity as a tool for human connectomics: theory, properties, and optimization</article-title>
          <source>J. Neurophysiol.</source>
          <volume>103</volume>
          <year>2010</year>
          <fpage>297</fpage>
          <lpage>321</lpage>
          <pub-id pub-id-type="pmid">19889849</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0475">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zeki</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shipp</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>The functional logic of cortical connections</article-title>
          <source>Nature</source>
          <volume>335</volume>
          <issue>6188</issue>
          <year>1988</year>
          <fpage>311</fpage>
          <lpage>317</lpage>
          <comment>Sep 22</comment>
          <pub-id pub-id-type="pmid">3047584</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <ack>
      <title>Acknowledgments</title>
      <p>This work was funded by the Wellcome Trust and supported by the China Scholarship Council (CSC), the NEUROCHOICE project by SystemsX.ch (JD, KES) and the University Research Priority on â€œFoundations of Human Social Behaviourâ€ at the University of Zurich (KES). We are indebted to Marcia Bennett for help in preparing this manuscript and to Will Penny for pointing out the connection of model selection to the Savageâ€“Dickey density ratio. We would also like to thank our four reviewers for invaluable help in presenting this work rigorously and clearly.</p>
    </ack>
  </back>
  <floats-group>
    <fig id="f0005">
      <label>Fig.Â 1</label>
      <caption>
        <p>The slaving principle and centre manifolds: This schematic illustrates the basic idea behind the slaving principle. In this example, there are two states, whose flows bring them to an attracting invariant set (the centre manifold); <italic>h</italic>(<italic>Î¶</italic><sub>1</sub>). Once the states have been attracted to this manifold they remain on (or near) it. This means the flow of states can be decomposed into a tangential component (on the manifold) and a transverse component (that draws states to the manifold). This decomposition can be described in terms of a change of coordinates, which implicitly separate fast (stable) transverse dynamics <inline-formula><mml:math id="M32" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>Î¾</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> from slow (unstable) tangential flow <inline-formula><mml:math id="M33" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Î¶</mml:mi><mml:mo>Ë™</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>Î¾</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> on the centre manifold. We exploit this decomposition to motivate the separation of dynamics into a slow, low-dimensional flow on an attracting manifold and a fast (analytic) fluctuating part that describes perturbations away from (and back to) the manifold. Please see the main text for a full description of the equations.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="f0010">
      <label>Fig.Â 2</label>
      <caption>
        <p>Synthetic data: This figure shows the synthetic data generated by a network or graph with three nodes. The upper left panel shows the simulated activity of each node over 256 (3.22 s) time bins. Signal is shown as solid lines and observation noise as dotted lines. The signal is a nonlinear function of the hidden hemodynamic and neuronal states shown on the upper right. In this model, there are five hidden states per node, which evolve according to the model's equations of motion. The dynamics seen here are caused by random neuronal fluctuations shown in the lower left panel. These were created by convolving a random Gaussian variable with a Gaussian convolution kernel of two bins standard deviation. The ensuing neuronal responses are communicated among nodes by extrinsic connections. In this example, we connected the blue node to the green node and the green node to the red node, as described in the main text. These influences or effective connectivity is denoted by the bidirectional solid black arrows. The light grey arrow denotes a possible but absent edge (anti-edge).</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="f0015">
      <label>Fig.Â 3</label>
      <caption>
        <p>Conditional estimates using a deterministic model: This figure shows the conditional estimates of the coupling parameters among the three nodes of the previous figure. The top panel shows the conditional means (grey bars) and 90% confidence intervals (red bars), superimposed upon the true values (black bars). It can be seen that although the estimates are in the right direction, they are very imprecise (they have a high conditional uncertainty). These estimates were obtained using a deterministic scheme, where unknown (hidden) neuronal causes were modelled as a mixture of temporal basis functions (a discrete cosine set). The true fluctuation or hidden input is shown on the lower left, while the estimated fluctuation is shown on the lower right. This estimate is a reconstitution of the hidden cause, using the conditional estimates of the basis function coefficients. One can see that the amplitude of the input has been overestimated. This reflects the fact that the coupling coefficients were under estimated (upper panel). The colour scheme pertains to the same nodes as in the previous figure.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="f0020">
      <label>Fig.Â 4</label>
      <caption>
        <p>Conditional estimates using a stochastic model: This figure shows similar results to those presented in <xref rid="f0015" ref-type="fig">Fig.Â 3</xref>. However, in this case the conditional estimates were based upon a stochastic model using Generalised Filtering. Here (upper left), we see that the estimates are closer to their true values and are much more precise. Furthermore, the conditional (<italic>maximum a posteriori</italic>; MAP) estimates of the neuronal fluctuations are very close to those elicited by the neuronal input used to simulate the data (compare the left and right lower panels). Because this model includes unknown (hidden) neuronal and physiological states, it also returns a conditional estimate of the hidden states causing responses. These are shown in the upper right panel. The conditional expectations are shown as coloured solid lines and the 90% confidence intervals (tubes) are shown as grey regions. Note that these hidden states are effectively log-states, such that a value of zero corresponds to 100% of the steady-state value. For small deviations from zero, the values of these hidden states correspond roughly to proportional changes. In this example, we see changes of up to about 20% (in blood flow).</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="f0025">
      <label>Fig.Â 5</label>
      <caption>
        <p>Model spaces and adjacency matrices: This figure illustrates the model spaces induced by considering different adjacency matrices or combinations of edges among the nodes of a graph. The upper panel shows the number of different models that one can entertain as a function of the number of nodes. Here, we placed the additional constraint on the models that each connection has to be bidirectional. The lower panel shows all the alternative models that could be considered, given four nodes. One example is highlighted in the insert, where the solid bidirectional arrows denote edges and the grey arrows denote anti-edges. This particular example was used to generate simulated data for the results described in the next figure.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="f0030">
      <label>Fig.Â 6</label>
      <caption>
        <p>Model inversion and selection: This figure reports the inversion and model selection, following an analysis of simulated data using the graph in the insert of the previous figure. The parameter estimates are shown on the upper left, using the same format as <xref rid="f0015 f0020" ref-type="fig">Figs.Â 3 and 4</xref>. It is immediately obvious that the true edges have been detected with reasonably high precision. The conditional density on these coupling parameters was then used to compute the log evidence of (64) reduced models as described in the main text, using the Savageâ€“Dickey ratio. The resulting log-evidence profile over models is shown in the right panels. The upper panel shows the log-evidence as approximated with its free-energy upper bound, while the lower panel shows the corresponding posterior probability over models (assuming flat priors over models). In this example, the correct model has been selected with almost 100% posterior model probability. The log-evidences are also shown as a function of graph size on the lower left. The red dot corresponds to the true model (see previous figure) and has the highest log-evidence. All log-evidences shown in this and subsequent figures are relative to their full model.</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="f0035">
      <label>Fig.Â 7</label>
      <caption>
        <p>Comparative evaluation of log evidence approximations: This figure presents a comparative evaluation of the <italic>post hoc</italic> log-evidence based upon the conditional density of the full model and the approximation based upon explicit inversions of reduced models. The free-energy of reduced models is plotted against the reduced free-energy in the upper panel and shows a reasonable agreement. The true model is shown as a red dot. The dashed line corresponds to a 100% agreement between the two approximations. The lower panel shows the same data but here as a function of graph size (number of bidirectional edges). The reduced free-energy approximation is shown in black, while the free-energy of reduced models is shown in cyan. Reassuringly, the true model has the highest log-evidence under both proxies, for the correct graph size</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
    <fig id="f0040">
      <label>Fig.Â 8</label>
      <caption>
        <p>Conditional expectations of the coupling parameters: The upper panels show the conditional expectations of the coupling parameters plotted against their true values for the full model (left) and under the (optimum) reduced model (right). One can see the characteristic shrinkage of a subset of parameters to their prior mean; in this case the parameters associated with anti-edges (black dots). The veracity of this shrinkage depends upon the accuracy of model selection. The lower panel shows the same estimates; however, here, we have tried to highlight that the shrinkage to true values is also evident for the connections that were present (red dots). This panel plots the change in conditional estimate when moving from the full model to the selected (reduced) model against the coupling strength error under the full model. For the connections that were absent, this is simply a straight line, because in all cases, the correct model was chosen. Crucially, we see a similar effect for the connections which were present, with a mild negative correlation between the change in estimate and its mismatch under the full model. Inhibitory self-connections are not included in these results (and did not change much, because they were subject to relatively informative priors).</p>
      </caption>
      <graphic xlink:href="gr8"/>
    </fig>
    <fig id="f0045">
      <label>Fig.Â 9</label>
      <caption>
        <p>Empirical data: This figure illustrates the data used for the empirical illustration of model selection. Regional summaries were harvested from six regions of interest using the attention to motion paradigm described in the main text. The central location of each region is shown on the left, superimposed on a translucent (canonical) cortical surface in MNI space. The resulting principle eigenvariate (summarising observed responses) are shown in the right panels. In this example, we can see evoked responses in visual areas (every 60Â s) with a progressive loss of stimulus-bound activity and a hint of attentional modulation and other fluctuations in higher regions. The predictions of these dynamics, based on inferred hidden states are shown in the next figure.</p>
      </caption>
      <graphic xlink:href="gr9"/>
    </fig>
    <fig id="f0050">
      <label>Fig.Â 10</label>
      <caption>
        <p>Conditional estimates of hidden states: A summary of the conditional expectations (means) of the hidden states generating observed regional data is shown on the upper right. The solid lines are time-dependent means and the grey regions are 90% confidence intervals (i.e., confidence tubes). These states comprise, for each region, neuronal activity, vasodilatory signal, normalised flow, volume and deoxyhemoglobin content. The last three are log-states. These hidden states provide the predicted responses (conditional expectation) in the upper left for each region and associated prediction errors (red dotted lines), in relation to the observed data. The same data are plotted in the lower panels for about the first four minutes of data acquisition. These results show that the inferred neuronal activity in the visual region (highlighted in blue) follows visual stimulation (grey filled areas â€” high for attention and low for no attention). The resulting hemodynamic changes are shown as conditional means on the lower right (blue highlights blood flow in the visual region). In this figure log-states have been plotted as states (with a normalised steady-state value of one).</p>
      </caption>
      <graphic xlink:href="gr10"/>
    </fig>
    <fig id="f0055">
      <label>Fig.Â 11</label>
      <caption>
        <p>Model selection using empirical data: This figure summarises the results of model selection using the empirical fMRI data. The upper left panel shows the log-evidence profile over the models considered (reflecting different combinations of edges among the six nodes analysed). The implicit model posterior (assuming flat priors over models), is shown on the upper right and suggests that we can be over 80% certain that a particular architecture generated these data. The parameter estimates of the connections under the full (left) and selected model (right) are shown in the lower panels. Again, we see that certain connections have been switched off as the parameter estimates are reduced to their prior value of zero. It is these anti-edges that define the architecture we are seeking. This architecture is shown graphically in the next figure.</p>
      </caption>
      <graphic xlink:href="gr11"/>
    </fig>
    <fig id="f0060">
      <label>Fig.Â 12</label>
      <caption>
        <p>The selected graph in anatomical space and functional space: This figure shows the graph selected (on the basis of the posterior probabilities in the previous figure) in anatomical space and functional (spectral embedding) space. The upper panel shows the same regions depicted in <xref rid="f0045" ref-type="fig">Fig.Â 9</xref>, but now connected using the conditional means of the coupling parameters, under the model selected. The colour of the arrow reports the source of the strongest bidirectional connection, while its width represents its absolute (positive or negative) strength. This provides a description of the architecture or graph in anatomical space. A more functionally intuitive depiction of this graph is provided in the lower panel. Here, we have used spectral embedding to place the nodes in a functional space, where the distance between them reflects the strength of bidirectional coupling. Spectral embedding uses the eigenvectors vectors (principle components) of the weighted graph Laplacian to define a small number of dimensions that best capture the proximity or conditional dependence between nodes. Here, we have used the first three eigenvectors to define this functional space. The weighted adjacency matrix was, in this case, simply the maximum (absolute) conditional estimate of the coupling parameters described in the previous figure. The middle panel shows the asymmetry strengths based on the conditional estimates of the selected model. This provides a further way of characterising the functional architecture in hierarchical terms, based on (bidirectional) coupling.</p>
      </caption>
      <graphic xlink:href="gr12"/>
    </fig>
    <fig id="f0065">
      <label>Fig.Â 13</label>
      <caption>
        <p>Model selection using null data: This figure has exactly the same format as <xref rid="f0045" ref-type="fig">Fig.Â 9</xref> but reports <italic>post hoc</italic> model selection results following inversion of phase-shuffled data. Phase shuffling preserves the amplitude and spectral properties of within-node dynamics but destroys any conditional dependencies among nodes. This should result in an edgeless graph, which is nearly the case but not quite.</p>
      </caption>
      <graphic xlink:href="gr13"/>
    </fig>
    <table-wrap id="t0005" position="float">
      <label>TableÂ 1</label>
      <caption>
        <p>Regions selected for DCM analysis on the basis of an (Omnibus) SPM of the F-statistic testing for evoked responses. Regions are defined as contiguous voxels in the SPM surviving a threshold of pÂ &lt;Â .001 (uncorrected).The anatomical designations should not be taken too seriously because the extent of several regions covered more than one cytoarchitectonic area, according to the atlas of Talairach and Tournoux.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Name</th>
            <th align="left">Rough designation</th>
            <th align="left">Location (mm)</th>
            <th align="left">Number of (3Â mm<sup>3</sup>) voxels</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">vis</td>
            <td align="left">Striate and extrastriate cortex</td>
            <td align="left">âˆ’Â 12 âˆ’Â 81 âˆ’Â 6</td>
            <td align="char">300</td>
          </tr>
          <tr>
            <td align="left">sts</td>
            <td align="left">Superior temporal sulcus</td>
            <td align="left">âˆ’Â 54 âˆ’Â 30 âˆ’Â 3</td>
            <td align="char">269</td>
          </tr>
          <tr>
            <td align="left">pfc</td>
            <td align="left">Prefrontal cortex</td>
            <td align="left">âˆ’Â 57 21 33</td>
            <td align="char">48</td>
          </tr>
          <tr>
            <td align="left">ppc</td>
            <td align="left">Posterior parietal cortex</td>
            <td align="left">âˆ’Â 21 âˆ’Â 57 66</td>
            <td align="char">168</td>
          </tr>
          <tr>
            <td align="left">ag</td>
            <td align="left">Angular gyrus</td>
            <td align="left">âˆ’Â 66 âˆ’Â 48 21</td>
            <td align="char">51</td>
          </tr>
          <tr>
            <td align="left">fef</td>
            <td align="left">Frontal eye fields</td>
            <td align="left">âˆ’Â 33 âˆ’Â 6 63</td>
            <td align="char">81</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </floats-group>
</article>