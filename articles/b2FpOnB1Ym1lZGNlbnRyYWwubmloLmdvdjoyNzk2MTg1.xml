<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neural Netw</journal-id>
      <journal-title>Neural Networks</journal-title>
      <issn pub-type="ppub">0893-6080</issn>
      <issn pub-type="epub">1879-2782</issn>
      <publisher>
        <publisher-name>Pergamon Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2796185</article-id>
      <article-id pub-id-type="pmid">19635656</article-id>
      <article-id pub-id-type="publisher-id">NN2638</article-id>
      <article-id pub-id-type="doi">10.1016/j.neunet.2009.07.023</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>2009 Special Issue</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Cortical circuits for perceptual inference</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Friston</surname>
            <given-names>Karl</given-names>
          </name>
          <email>k.friston@fil.ion.ucl.ac.uk</email>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Kiebel</surname>
            <given-names>Stefan</given-names>
          </name>
        </contrib>
      </contrib-group>
      <aff>
        <addr-line>The Wellcome Trust Centre of Neuroimaging, University College London, Queen Square, London WC1N 3BG, United Kingdom</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding address: The Wellcome Trust Centre for Neuroimaging, Institute of Neurology, Queen Square, London, WC1N 3BG, United Kingdom. Tel.: +44 207 833 7454; fax: +44 207 813 1445. <email>k.friston@fil.ion.ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="ppub">
        <month>10</month>
        <year>2009</year>
      </pub-date>
      <volume>22</volume>
      <issue>8-10</issue>
      <fpage>1093</fpage>
      <lpage>1104</lpage>
      <history>
        <date date-type="received">
          <day>26</day>
          <month>1</month>
          <year>2009</year>
        </date>
        <date date-type="rev-recd">
          <day>14</day>
          <month>5</month>
          <year>2009</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>7</month>
          <year>2009</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2009 Elsevier Ltd.</copyright-statement>
        <copyright-year>2009</copyright-year>
        <copyright-holder>Elsevier Ltd</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Abstract</title>
        <p>This paper assumes that cortical circuits have evolved to enable inference about the causes of sensory input received by the brain. This provides a principled specification of <italic>what</italic> neural circuits have to achieve. Here, we attempt to address <italic>how</italic> the brain makes inferences by casting inference as an optimisation problem. We look at how the ensuing recognition dynamics could be supported by directed connections and message-passing among neuronal populations, given our knowledge of intrinsic and extrinsic neuronal connections. We assume that the brain models the world as a dynamic system, which imposes causal structure on the sensorium. Perception is equated with the optimisation or inversion of this internal model, to explain sensory input. Given a model of how sensory data are generated, we use a generic variational approach to model inversion to furnish equations that prescribe recognition; i.e., the dynamics of neuronal activity that represents the causes of sensory input. Here, we focus on a model whose hierarchical and dynamical structure enables simulated brains to recognise and predict sequences of sensory states. We first review these models and their inversion under a variational free-energy formulation. We then show that the brain has the necessary infrastructure to implement this inversion and present stimulations using synthetic birds that generate and recognise birdsongs.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Generative models</kwd>
        <kwd>Predictive coding</kwd>
        <kwd>Hierarchical</kwd>
        <kwd>Dynamic</kwd>
        <kwd>Nonlinear</kwd>
        <kwd>Circuits</kwd>
        <kwd>Variational</kwd>
        <kwd>Birdsong</kwd>
        <kwd>Free-energy</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <label>1</label>
      <title>Introduction</title>
      <p>This paper looks at the functional architecture of cortical circuits from the point of view of perception; namely, the fitting or inversion of internal models of sensory data by the brain. Critically, the nature of this inversion lends itself to a relatively simple neural network implementation that shares many formal similarities with real cortical hierarchies in the brain. The basic idea that the brain uses hierarchical inference has been described in a series of papers (<xref rid="b17 b18 b39 b45" ref-type="bibr">Friston, 2005; Friston, Kilner, &amp; Harrison, 2006; Mumford, 1992; Rao &amp; Ballard, 1998</xref>). These papers suggest that the brain uses <italic>empirical</italic> Bayes for inference about its sensory input, given the hierarchical organisation of cortical systems. Here, we focus on how neural networks could be configured to invert these models and deconvolve sensory causes from sensory input.</p>
      <p>This paper comprises three sections. In the first, we introduce a free-energy formulation of model inversion or perception, which is then applied to a specific class of models that we assume the brain uses — hierarchical dynamic models. An important aspect of these models is their formulation in generalised coordinates of motion. This lends them a hierarchical form in both structure and dynamics, which can be exploited during inversion. In the second section, we show how inversion can be formulated as a simple gradient descent using neuronal networks and relate these to cortical circuits in the brain. In the final section, we consider how evoked brain responses might be understood in terms of perceptual inference and categorisation, using the schemes of the preceding section.</p>
    </sec>
    <sec id="sec2">
      <label>2</label>
      <title>The free-energy formulation</title>
      <p>This section considers the problem of inverting generative models of sensory data and provides a summary of the material in <xref rid="b19" ref-type="bibr">Friston (2008)</xref>. This problem is addressed using <italic>ensemble learning</italic> or <italic>Variational Bayes</italic>. These are generic approaches to model inversion that provide an approximation to the conditional density <mml:math id="M1" altimg="si3.gif" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math> on some causes ϑ of generalised sensory input, <mml:math id="M2" altimg="si5.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup></mml:math>. Generalised input (e.g., the intensity of photoreceptor stimulation) includes the input, its velocity, acceleration, jerk, <italic>etc</italic>. Causes are quantities in the environment that generate sensory input (e.g., the orientation of an object in the visual field). The approximation of the conditional density (i.e., the probability of a particular set of causes given sensory input) is achieved by optimising a recognition density <mml:math id="M3" altimg="si6.gif" display="inline" overflow="scroll"><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> with respect to a bound on the surprise or negative log-evidence <mml:math id="M4" altimg="si7.gif" display="inline" overflow="scroll"><mml:mo>−</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math> of the sensory input, as we will see next (<xref rid="b14 b17 b18 b25 b36 b41" ref-type="bibr">Feynman, 1972; Friston, 2005; Friston et al., 2006; Hinton &amp; von Cramp, 1993; MacKay, 1995; Neal &amp; Hinton, 1998</xref>). This bound is called free-energy<disp-formula id="fd1"><label>(1)</label><mml:math id="M5" altimg="si8.gif" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>−</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><mml:math id="M6" altimg="si9.gif" display="block" overflow="scroll"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mo>ln</mml:mo><mml:mfrac><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mtext>.</mml:mtext></mml:math></disp-formula> The free-energy comprises a cross-entropy or divergence term <mml:math id="M7" altimg="si10.gif" display="inline" overflow="scroll"><mml:mi>C</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math> and surprise. By Gibb’s inequality, the divergence is greater than zero, with equality when <mml:math id="M8" altimg="si11.gif" display="inline" overflow="scroll"><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math>; i.e., when the recognition density is the posterior or conditional density on the causes of sensory input. The recognition density can be optimised to minimise this bound and implicitly minimise the divergence between the recognition density and the conditional density we seek (<xref rid="b17 b25 b36 b41" ref-type="bibr">Friston, 2005; Hinton &amp; von Cramp, 1993; MacKay, 1995; Neal &amp; Hinton, 1998</xref>). In summary, the recognition density, induces a free-energy bound, which converts a difficult integration problem (inherent in computing the exact conditional density) into an easier optimisation problem.</p>
      <p>The bound can be evaluated easily because it is a function of <mml:math id="M9" altimg="si12.gif" display="inline" overflow="scroll"><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> and some generative model <mml:math id="M10" altimg="si13.gif" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> entailed by the brain <disp-formula id="fd2"><label>(2)</label><mml:math id="M11" altimg="si14.gif" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mo>ln</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mo>ln</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula><mml:math id="M12" altimg="si15.gif" display="block" overflow="scroll"><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:math></disp-formula> Here, we have expressed the free-energy in terms of the negentropy of <mml:math id="M13" altimg="si16.gif" display="inline" overflow="scroll"><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> and an expected Gibb’s energy — <mml:math id="M14" altimg="si17.gif" display="inline" overflow="scroll"><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math>. This energy is usually specified in terms of a likelihood and prior; <mml:math id="M15" altimg="si18.gif" display="inline" overflow="scroll"><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math>, which define a generative model. This is important because it shows that we need a generative model in order to evaluate free-energy. The likelihood model just quantifies the probability of any sensations, given their cause; while the prior model encodes prior beliefs about the probability of those causes being present. It is fairly easy to show that minimising free-energy corresponds to finding a recognition density that predicts sensory input accurately, while suppressing its complexity.</p>
      <p>If we assume that the recognition density <mml:math id="M16" altimg="si19.gif" display="inline" overflow="scroll"><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math> is Gaussian (the Laplace assumption), then we can express free-energy in terms of its sufficient statistics (i.e., its mean and covariance: <mml:math id="M17" altimg="si20.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:math>) <disp-formula id="fd3"><label>(3)</label><mml:math id="M18" altimg="si21.gif" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mi>∇</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>ln</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>ln</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:mtext>.</mml:mtext></mml:math></disp-formula> Here n is the number of unknown causes. We can now minimise free-energy w.r.t. the conditional covariances by finding the value that renders its gradient zero <disp-formula id="fd4"><label>(4)</label><mml:math id="M19" altimg="si23.gif" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi>∇</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>⇒</mml:mo></mml:math></disp-formula><disp-formula><mml:math id="M20" altimg="si24.gif" display="block" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>∇</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> where a subscript means differentiation; i.e., <mml:math id="M21" altimg="si25.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>∂</mml:mi><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:mi>∂</mml:mi><mml:mi>Σ</mml:mi></mml:math> is the free-energy gradient w.r.t. the conditional covariance, Here, the conditional precision <mml:math id="M22" altimg="si26.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math> is the inverse covariance. Critically, the conditional precision is just a function of the mean and does not have to be encoded explicitly. This means we can simplify the expression for free-energy by eliminating the curvatures <mml:math id="M23" altimg="si27.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>∇</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>U</mml:mi></mml:math> of Gibb’s energy <disp-formula id="fd5"><label>(5)</label><mml:math id="M24" altimg="si28.gif" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>ln</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>ln</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mtext>.</mml:mtext></mml:math></disp-formula> Now, the only unknown quantities are the conditional means of the causes, which only have to minimise Gibb’s energy because this is the only term that depends on them. In this paper, we will focus on time-varying causes or states of the environment: <mml:math id="M25" altimg="si29.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊂</mml:mo><mml:mi>ϑ</mml:mi></mml:math>. The values we seek are the solutions to the following differential equations. <disp-formula id="fd6"><label>(6)</label><mml:math id="M26" altimg="si30.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"/><mml:mtd columnalign="center"><mml:mo>⇔</mml:mo></mml:mtd><mml:mtd columnalign="left"/></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>μ</mml:mi><mml:msup><mml:mrow/><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow/><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>μ</mml:mi><mml:msup><mml:mrow/><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow/><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>μ</mml:mi><mml:msup><mml:mrow/><mml:mrow><mml:mo>‴</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mo>⋮</mml:mo><mml:mtext>.</mml:mtext></mml:mtd><mml:mtd columnalign="center"/><mml:mtd columnalign="left"/></mml:mtr></mml:mtable></mml:math></disp-formula> This solution (which is stationary in a frame of reference that moves with its generalised motion), minimises free-energy <disp-formula id="fd7"><label>(7)</label><mml:math id="M27" altimg="si31.gif" display="block" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>⇒</mml:mo></mml:math></disp-formula><disp-formula><mml:math id="M28" altimg="si32.gif" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>⇒</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext>.</mml:mtext></mml:math></disp-formula> This construction ensures that when Gibb’s energy is minimised and <mml:math id="M29" altimg="si33.gif" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>, the mean of the motion is the motion of the mean; i.e., <mml:math id="M30" altimg="si34.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:math>. Here D is a derivative matrix operator with identity matrices along the first leading diagonal.</p>
      <p>Eq. <xref rid="fd7" ref-type="disp-formula">(7)</xref> prescribes recognition dynamics that track time-varying causes or states of the world and can be thought of as a gradient descent in a moving frame of reference. The recognition dynamics for time-invariant causes (i.e., parameters <mml:math id="M31" altimg="si36.gif" display="inline" overflow="scroll"><mml:mi>θ</mml:mi><mml:mo>⊂</mml:mo><mml:mi>ϑ</mml:mi></mml:math>, like rate constants) have a different form, because we know <italic>a priori</italic> their generalised motion is zero. In this paper, we will assume the parameters have already been learnt and focus on recognising hidden states of the environment. In summary, we have derived recognition dynamics for expected environmental states, which cause sensations. The solution to these equations minimise Gibb’s energy and (under the Laplace assumption) free-energy, which is an upper bound on their surprise. Finding these solutions corresponds to perceptual inference. The precise form of Eq. <xref rid="fd7" ref-type="disp-formula">(7)</xref> depends on the generative model that defines Gibb’s energy. Next, we examine forms associated with hierarchical dynamic models.</p>
      <sec id="sec2.1">
        <label>2.1</label>
        <title>Hierarchical dynamic models</title>
        <p>This section introduces a general class of generative models that the brain may use for perception. We will start with simple dynamic models and then deal with hierarchical cases later. Consider a state-space model that describes the evolution of states in the world and how they map to sensory input <disp-formula id="fd8"><label>(8)</label><mml:math id="M32" altimg="si37.gif" display="block" overflow="scroll"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>z</mml:mi></mml:math></disp-formula><disp-formula><mml:math id="M33" altimg="si38.gif" display="block" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mtext>.</mml:mtext></mml:math></disp-formula> Here, the functions f and g are parameterised by <mml:math id="M34" altimg="si41.gif" display="inline" overflow="scroll"><mml:mi>θ</mml:mi><mml:mo>⊂</mml:mo><mml:mi>ϑ</mml:mi></mml:math> (which are omitted from the following expressions for clarity). These functions correspond to equations of motion and an observer function, respectively. The states <mml:math id="M35" altimg="si42.gif" display="inline" overflow="scroll"><mml:mi>v</mml:mi><mml:mo>⊂</mml:mo><mml:mi>u</mml:mi></mml:math> are variously referred to as sources or causal states. The hidden states <mml:math id="M36" altimg="si43.gif" display="inline" overflow="scroll"><mml:mi>x</mml:mi><mml:mo>⊂</mml:mo><mml:mi>u</mml:mi></mml:math> mediate the influence of causal states on sensory data and endow the system with memory. We assume the random fluctuations z are analytic, such that the covariance of <mml:math id="M37" altimg="si45.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup></mml:math> is well defined; similarly for state noise, <mml:math id="M38" altimg="si46.gif" display="inline" overflow="scroll"><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math>, which represents random fluctuations on the motion of the hidden states. Under local linearity assumptions, the generalised motion of the data or response <mml:math id="M39" altimg="si47.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup></mml:math> is given by <disp-formula id="fd9"><label>(9)</label><mml:math id="M40" altimg="si48.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>y</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>z</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"/><mml:mtd columnalign="left"/></mml:mtr></mml:mtable><mml:mspace width="2em" class="qquad"/><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>‴</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mo>⋮</mml:mo><mml:mtext>.</mml:mtext></mml:mtd><mml:mtd columnalign="center"/><mml:mtd columnalign="left"/></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> We can write this generalised state-space model more compactly as <disp-formula id="fd10"><label>(10)</label><mml:math id="M41" altimg="si49.gif" display="block" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:math></disp-formula><disp-formula><mml:math id="M42" altimg="si50.gif" display="block" overflow="scroll"><mml:mi>D</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:math></disp-formula> where the predicted response <mml:math id="M43" altimg="si51.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup></mml:math> and motion <mml:math id="M44" altimg="si52.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup></mml:math> are <disp-formula id="fd11"><label>(11)</label><mml:math id="M45" altimg="si53.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>g</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"/><mml:mtd columnalign="left"/></mml:mtr></mml:mtable><mml:mspace width="2em" class="qquad"/><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>f</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mo>⋮</mml:mo><mml:mtext>.</mml:mtext></mml:mtd><mml:mtd columnalign="center"/><mml:mtd columnalign="left"/></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> Gaussian assumptions about the fluctuations <mml:math id="M46" altimg="si54.gif" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>:</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math> provide the form of the likelihood, <mml:math id="M47" altimg="si55.gif" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math>. Similarly, Gaussian assumptions about state noise <mml:math id="M48" altimg="si56.gif" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>:</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math> specify empirical priors, <mml:math id="M49" altimg="si57.gif" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math> in terms of predicted motion <disp-formula id="fd12"><label>(12)</label><mml:math id="M50" altimg="si58.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>:</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>:</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> The covariances <mml:math id="M51" altimg="si59.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:math> and <mml:math id="M52" altimg="si60.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:math> or precisions <mml:math id="M53" altimg="si61.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> and <mml:math id="M54" altimg="si62.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> are functions of precision parameters, <mml:math id="M55" altimg="si63.gif" display="inline" overflow="scroll"><mml:mi>λ</mml:mi><mml:mo>⊂</mml:mo><mml:mi>ϑ</mml:mi></mml:math>, which control the amplitude and smoothness of random fluctuations. Generally, these covariances factorise; <mml:math id="M56" altimg="si64.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:msup></mml:math> into a covariance proper and a matrix of correlations <mml:math id="M57" altimg="si65.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:msup></mml:math> among generalised motion that encodes an autocorrelation function.</p>
        <sec id="sec2.1.1">
          <label>2.1.1</label>
          <title>Hierarchical forms</title>
          <p>Hierarchical dynamic models with m levels have the following form, which generalises the <mml:math id="M58" altimg="si67.gif" display="inline" overflow="scroll"><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math> model above <disp-formula id="fd13"><label>(13)</label><mml:math id="M59" altimg="si68.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>y</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"/><mml:mtd columnalign="left"/></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"/><mml:mtd columnalign="left"/></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> Again, <mml:math id="M60" altimg="si69.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math> and <mml:math id="M61" altimg="si70.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math> are continuous nonlinear functions of the states. The innovations <mml:math id="M62" altimg="si71.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math> and <mml:math id="M63" altimg="si72.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math> are conditionally independent fluctuations that enter each level of the hierarchy. These play the role of observation error or noise at the first level and induce random fluctuations in the states at higher levels. The causal states <mml:math id="M64" altimg="si73.gif" display="inline" overflow="scroll"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup></mml:math> link levels, whereas the hidden states <mml:math id="M65" altimg="si74.gif" display="inline" overflow="scroll"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup></mml:math> link dynamics over time. In hierarchical form, the output of one level acts as an input to the next. Inputs from higher levels can enter nonlinearly into the state equations and can be regarded as changing its control parameters to produce quite complicated generalised convolutions with deep (i.e., hierarchical) structure.</p>
          <p>In summary, hierarchical dynamic models are about as complicated as one could imagine; they comprise causal and hidden states, whose dynamics can be coupled with arbitrary (analytic) nonlinear functions. Furthermore, these states can have random fluctuations with unknown amplitude and arbitrary (analytic) autocorrelation functions. A key aspect of these models is their hierarchical form, which induces empirical priors on the causal states. See <xref rid="b29" ref-type="bibr">Kass and Steffey (1989)</xref> for a discussion of approximate Bayesian inference in conditionally independent hierarchical models of static data.</p>
        </sec>
        <sec id="sec2.1.2">
          <label>2.1.2</label>
          <title>Energy functions</title>
          <p>We can now write down Gibb’s energy for these generative models, which has a simple quadratic form (ignoring constants) <disp-formula id="fd14"><label>(14)</label><mml:math id="M66" altimg="si75.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>U</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>ln</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>ln</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> The auxiliary variables <mml:math id="M67" altimg="si76.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> comprise prediction errors for the generalised response and motion of hidden states, where <mml:math id="M68" altimg="si77.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> and <mml:math id="M69" altimg="si78.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> are the respective predictions, whose precision is encoded by <mml:math id="M70" altimg="si79.gif" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math>. These prediction errors provide a compact way to express Gibb’s energy and, as we will see below, lead to very simple recognition schemes. For hierarchical models, the prediction error on the response is supplemented with prediction errors on the causal states <disp-formula id="fd15"><label>(15)</label><mml:math id="M71" altimg="si80.gif" display="block" overflow="scroll"><mml:msup><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:math></disp-formula> Note that the data enter the prediction error at the lowest level. At intermediate levels, the prediction errors, <mml:math id="M72" altimg="si81.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math> mediate empirical priors on the causal states.</p>
        </sec>
      </sec>
      <sec id="sec2.2">
        <label>2.2</label>
        <title>Summary</title>
        <p>In this section, we have seen how the inversion of dynamic models can be formulated as an optimisation of free-energy. By assuming a Gaussian (Laplace) approximation to the conditional density, one can reduce optimisation to finding the conditional means of the unknown causes of sensory data. This can be formulated as a gradient ascent in a frame of reference that moves along the path encoded in generalised coordinates (Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref>). The only thing needed to implement this recognition scheme is Gibb’s energy, which is specified by a generative model. We have looked at hierarchical dynamic models, whose form provides empirical priors or constraints on inference at both a structural and dynamic level (Eq. <xref rid="fd14" ref-type="disp-formula">(14)</xref>). The <italic>structural</italic> priors arise from coupling different levels of the hierarchy with causal states and the <italic>dynamic</italic> priors emerge by coupling different levels of generalised motion of the hidden states. We can now look at the recognition dynamics entailed by these models, in the context of neuronal processes in the brain.</p>
      </sec>
    </sec>
    <sec id="sec3">
      <label>3</label>
      <title>Hierarchical models in the brain</title>
      <p>A key architectural principle of the brain is its hierarchical organisation (<xref rid="b13 b37 b55" ref-type="bibr">Felleman &amp; Van Essen, 1991; Maunsell &amp; van Essen, 1983; Zeki &amp; Shipp, 1988</xref>).  This has been established most thoroughly in the visual system, where lower (primary) areas receive sensory input and higher areas adopt a multimodal or associational role. The neurobiological notion of a hierarchy rests upon the distinction between forward and backward connections (<xref rid="b1 b13 b40 b47 b49" ref-type="bibr">Angelucci et al., 2002; Felleman &amp; Van Essen, 1991; Murphy &amp; Sillito, 1987; Rockland &amp; Pandya, 1979; Sherman &amp; Guillery, 1998</xref>). This distinction is based upon the specificity of cortical layers that are the predominant sources and origins of extrinsic connections. Forward connections arise largely in superficial pyramidal cells, in supra-granular layers and terminate on spiny stellate cells of layer four in higher cortical areas (<xref rid="b12 b13" ref-type="bibr">DeFelipe, Alonso-Nanclares, &amp; Arellano, 2002; Felleman &amp; Van Essen, 1991</xref>). Conversely, backward connections arise largely from deep pyramidal cells in infra-granular layers and target cells in the infra- and supra-granular layers of lower cortical areas. Intrinsic connections mediate lateral interactions between neurons that are a few millimetres away. There is a key functional asymmetry between forward and backward connections that renders backward connections more modulatory or nonlinear in their effects on neuronal responses (<xref rid="b49" ref-type="bibr">Sherman &amp; Guillery, 1998</xref>; see also <xref rid="b27" ref-type="bibr">Hupe et al., 1998</xref>). This is consistent with the deployment of voltage-sensitive NMDA receptors in supra-granular layers that are targeted by backward connections (<xref rid="b48" ref-type="bibr">Rosier, Arckens, Orban, &amp; Vandesande, 1993</xref>). Typically, the synaptic dynamics of backward connections have slower time constants. This has led to the notion that forward connections are driving and illicit an obligatory response in higher levels, whereas backward connections have both driving and modulatory effects and operate over larger spatial and temporal scales. This hierarchical aspect of the brain’s functional anatomy speaks to hierarchical models of sensory input. We now consider how this functional architecture can be understood under the inversion of hierarchical models by the brain.</p>
      <sec id="sec3.1">
        <label>3.1</label>
        <title>Perceptual inference</title>
        <p>If we assume that the activity of neurons encode the conditional mean of external states causing sensory data, then Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref> specifies the neuronal dynamics entailed by recognising states of the world from sensory data. Using Gibb’s energy in Eq. <xref rid="fd14" ref-type="disp-formula">(14)</xref> we have <disp-formula id="fd16"><label>(16)</label><mml:math id="M73" altimg="si82.gif" display="block" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:msubsup><mml:mi>ξ</mml:mi></mml:math></disp-formula><disp-formula><mml:math id="M74" altimg="si83.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>ξ</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>−</mml:mo><mml:mi>Λ</mml:mi><mml:mi>ξ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="left"/></mml:mtr><mml:mtr><mml:mtd columnalign="center"/><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> Eq. <xref rid="fd16" ref-type="disp-formula">(16)</xref> describes how neuronal states self-organise, when exposed to sensory input. Its form is quite revealing and suggests two distinct populations of neurons; causal or hidden <italic>state-units</italic> whose activity encodes <mml:math id="M75" altimg="si84.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math> and <italic>error-units</italic> encoding precision-weighted prediction error <mml:math id="M76" altimg="si85.gif" display="inline" overflow="scroll"><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Π</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:math>, with one error-unit for each state. Furthermore, the activities of error-units are a function of the states and the dynamics of state-units are a function of prediction error. This means the two populations pass messages to each other and to themselves. The messages passed within the states, <mml:math id="M77" altimg="si86.gif" display="inline" overflow="scroll"><mml:mi>D</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:math> mediate empirical priors on their motion, while <mml:math id="M78" altimg="si87.gif" display="inline" overflow="scroll"><mml:mo>−</mml:mo><mml:mi>Λ</mml:mi><mml:mi>ξ</mml:mi></mml:math> mediates precision-dependent modulation of prediction errors. The matrix <mml:math id="M79" altimg="si88.gif" display="inline" overflow="scroll"><mml:mi>Λ</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:math> can be thought of encoding self-inhibition, which is modulated by precision (where precision might be encoded by neuromodulatory neurotransmitters like dopamine and acetylcholine).</p>
      </sec>
      <sec id="sec3.2">
        <label>3.2</label>
        <title>Hierarchical message-passing</title>
        <p>If we unpack these equations, we can see the hierarchical nature of the implicit message-passing <disp-formula id="fd17"><label>(17)</label><mml:math id="M80" altimg="si89.gif" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> This shows that error-units receive messages from the states in the same level and the level above, whereas states are driven by error-units in the same level and the level below (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). Critically, inference requires only the prediction error from the lower level <mml:math id="M81" altimg="si90.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math> and the level in question, <mml:math id="M82" altimg="si91.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math>. These provide bottom-up and lateral messages that drive conditional expectations <mml:math id="M83" altimg="si92.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math> towards a better prediction, to explain away the prediction error in the level below. These top-down and lateral predictions correspond to <mml:math id="M84" altimg="si93.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math> and <mml:math id="M85" altimg="si94.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math>. This is the essence of recurrent message-passing between hierarchical levels to optimise free-energy or suppress prediction error; i.e., recognition dynamics. In summary, all connections between error- and state-units are reciprocal but the only connections that link levels are forward connections conveying prediction error to state-units and reciprocal backward connections that mediate predictions. This sort of scheme is referred to as predictive coding (<xref rid="b45" ref-type="bibr">Rao &amp; Ballard, 1998</xref>).</p>
        <p>We can identify error-units with superficial pyramidal cells, because the only messages that pass up the hierarchy are prediction errors and superficial pyramidal cells originate forward connections in the brain. This is useful because it is these cells that are primarily responsible for electroencephalographic (EEG) signals that can be measured non-invasively. Similarly, the only messages that are passed down the hierarchy are the predictions from state-units that are necessary to form prediction errors in lower levels. The sources of extrinsic backward connections are deep pyramidal cells; suggesting that these encode the expected causes of sensory states (see <xref rid="b39" ref-type="bibr">Mumford, 1992</xref> and <xref rid="fig1" ref-type="fig">Fig. 1</xref>). Critically, the motion of each state-unit is a linear mixture of bottom-up prediction error (see Eq. <xref rid="fd17" ref-type="disp-formula">(17)</xref>). This is exactly what is observed physiologically; bottom-up driving inputs elicit obligatory responses that do not depend on other bottom-up inputs. The prediction error itself is formed by predictions conveyed by backward and lateral connections. These influences embody the nonlinearities implicit in <mml:math id="M86" altimg="si95.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math> and <mml:math id="M87" altimg="si96.gif" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>˜</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math>. Again, this is entirely consistent with the nonlinear or modulatory characteristics of backward connections.</p>
        <p>It has been shown recently that hierarchical architectures (cf, <xref rid="fig1" ref-type="fig">Fig. 1</xref>) can be reformulated as a specific type of biased competition, where state-units receive messages from lower-level error-units and direct inputs from higher-level state-units (replacing lateral inputs from error-units in the original predictive coding scheme based on Kalman filtering; <xref rid="b45" ref-type="bibr">Rao &amp; Ballard, 1998</xref>). It has been argued that this architecture provides a more realistic model of backward connections in cortex (<xref rid="b50 b51" ref-type="bibr">Spratling, 2008a, 2008b</xref>) and usefully connects predictive coding, Kalman filtering and biased competition.</p>
        <p>A related Bayesian algorithm called belief-propagation (<xref rid="b10 b26 b35 b46" ref-type="bibr">Dean, 2006; Hinton, Osindero, &amp; Teh, 2006; Lee &amp; Mumford, 2003; Rao, 2006</xref>) also rests on message-passing. In these schemes, the messages are not prediction errors but, like prediction errors, are defined self-consistently, in terms of likelihoods and empirical priors. Critically, the belief-propagation algorithm can be derived by minimising free-energy (<xref rid="b54" ref-type="bibr">Yedidia, Freeman, &amp; Weiss, 2005</xref>); for example, it can be shown that the Kalman filter is a special case of belief-propagation. This speaks to formal similarities between predictive coding, Bayesian filtering and belief-propagation, which could be implemented by recursive message-passing in the brain and understood in terms of free-energy optimisation.</p>
      </sec>
      <sec id="sec3.3">
        <label>3.3</label>
        <title>Summary</title>
        <p>In summary, we have seen how the inversion of a generic hierarchical and dynamical model of sensory inputs can be transcribed onto neuronal quantities that optimise a free-energy bound on surprise. This optimisation corresponds, under some simplifying assumptions, to suppression of prediction error at all levels in a cortical hierarchy. This suppression rests upon a balance between bottom-up (prediction error) and top-down (empirical prior) influences. In the final section, we use this scheme to simulate neuronal responses. Specifically, we look at the electrophysiological correlates of prediction error and ask whether we can understand some common phenomena in event-related potential (ERP) research in terms of the free-energy formulation and message-passing in the brain.</p>
      </sec>
    </sec>
    <sec id="sec4">
      <label>4</label>
      <title>Birdsong and attractors</title>
      <p>In this section, we examine a system that uses hierarchical dynamics as a generative model of sensory input. The aim of this section is to provide some face-validity for the functional deconstruction of extrinsic and intrinsic circuits in the previous section. To do this, we try to show how empirical measures of neuronal processes can be reproduced using simulations based on the theoretical analysis above. The example we use is birdsong and the empirical measures we focus on are local field potentials (LFP) or evoked (ERP) responses that can be recorded non-invasively. The material in section is based on the simulations described in <xref rid="b20" ref-type="bibr">Friston and Kiebel (2009)</xref>.</p>
      <p>We first describe our model of birdsong and demonstrate the nature and form of this model through simulated lesion experiments. We then use simplified versions of this model to show how attractors can be used to categorise sequences of stimuli quickly and efficiently. Throughout this section, we will exploit the fact that superficial pyramidal cells are the major contributors to observed LFP and ERP signals. This means we can ascribe these signals to prediction error; because the superficial pyramidal cells are the source of bottom-up messages in the brain (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>).</p>
      <sec id="sec4.1">
        <label>4.1</label>
        <title>Attractors in the brain</title>
        <p>The basic idea here is that the environment unfolds as an ordered sequence of spatiotemporal dynamics (<xref rid="b21 b31" ref-type="bibr">George &amp; Hawkins, 2005; Kiebel, Daunizeau, &amp; Friston, 2008</xref>), whose equations of motion entail attractor manifolds that contain sensory trajectories. Critically, the shape of the manifold generating sensory data is itself changed by other dynamical systems that could have their own attractors. If we consider the brain has a generative model of these coupled dynamical systems, then we would expect to see attractors in neuronal dynamics that are trying to predict sensory input. In a hierarchical setting, the states of a high-level attractor enter the equations of motion of a low-level attractor in a nonlinear way, to change the shape of its manifold. This form of generative model has some key characteristics:</p>
        <p>First, any level in the model can generate and therefore encode structured sequences of events, as the states flow over different parts of the manifold. These sequences can be simple, such as the quasi-periodic attractors of central pattern generators (<xref rid="b38" ref-type="bibr">McCrea &amp; Rybak, 2008</xref>) or can exhibit complicated sequences of the sort associated with chaotic and itinerant dynamics (e.g., <xref rid="b5 b7 b16 b22 b28 b32 b44" ref-type="bibr">Breakspear &amp; Stam, 2005; Canolty et al., 2006; Friston, 1997; Haken, Kelso, Fuchs, &amp; Pandya, 1990; Jirsa, Fuchs, &amp; Kelso, 1998; Kopell, Ermentrout, Whittington, &amp; Traub, 2000; Rabinovich, Huerta, &amp; Laurent, 2008</xref>). The notion of attractors as the basis of generative models extends the notion of encoding trajectories in terms of generalised motion, to families of trajectories that lie on the attractor manifold. Hierarchically deployed attractors enable the brain to generate and therefore predict or represent different categories of sequences. This is because any low-level attractor encodes a family of trajectories that correspond to a structured sequence. The neuronal activity representing the trajectory at any one time determines <italic>where</italic> the current dynamics are within the sequence, while the shape of the attractor manifold determines <italic>which</italic> sequence is currently being expressed.</p>
        <p>Secondly, if the states in a higher attractor change the manifold of a subordinate attractor, then the states of the higher attractor come to encode the category of the sequence represented by the lower attractor. This means it is possible to generate and represent sequences of sequences and, by induction sequences of sequences of sequences <italic>etc</italic>. This rests upon the states of neuronal attractors at any cortical level providing control parameters for attractors below. This necessarily entails a nonlinear interaction between the top-down effects of the higher attractor and the states of the lower attractor. Again, this is entirely consistent with the nonlinear effects of top-down connections in the real brain.</p>
        <p>Finally, this particular model has implications for the temporal structure of perception. Put simply, the dynamics of high-level representations unfold more slowly than the dynamics of lower-level representations. This is because the state of a higher attractor prescribes a manifold that guides the flow of lower states, which could change quite rapidly. We will see an example of this below when considering the perceptual categorisation of different sequences of chirps subtending birdsongs. This suggests that neuronal representations in the brain will change more slowly at higher levels (<xref rid="b31" ref-type="bibr">Kiebel et al., 2008</xref>; see also <xref rid="b4 b23" ref-type="bibr">Botvinick, 2007; Hasson, Yang, Vallines, Heeger, &amp; Rubin, 2008</xref>). One can turn this argument on its head and use the fact that we are able to recognise sequences of sequences (e.g., <xref rid="b8" ref-type="bibr">Chait, Poeppel, de Cheveigné, &amp; Simon, 2007</xref>) as an existence proof for this sort of generative model. In the examples below, we will try to show how autonomous dynamics furnish generative models of sensory input, which behave much like real brains, when measured electrophysiologically.</p>
      </sec>
      <sec id="sec4.2">
        <label>4.2</label>
        <title>A synthetic avian brain</title>
        <p>The toy example used here deals with the generation and recognition of birdsongs (<xref rid="b34" ref-type="bibr">Laje &amp; Mindlin, 2002</xref>). We imagine that birdsongs are produced by two time-varying causal states that control the frequency and amplitude of vibrations of the syrinx of a songbird (see <xref rid="fig2" ref-type="fig">Fig. 2</xref>). There has been an extensive modelling effort using attractor models at the biomechanical level to understand the generation of birdsong (e.g., <xref rid="b33" ref-type="bibr">Laje, Gardner, &amp; Mindlin, 2002</xref>). Here we use the attractors at a higher level to provide time-varying control over the resulting sonograms. We drive the syrinx with two states of a Lorenz attractor, one controlling the frequency (between two to five kHz) and the other (after rectification) controlling the amplitude or volume. The parameters of the Lorenz attractor were chosen to generate a short sequence of chirps every second or so. To endow the generative model with a hierarchical structure, we placed a second Lorenz attractor, whose dynamics were an order of magnitude slower, over the first. The states of the slower attractor entered as control parameters (the Raleigh and Prandtl number) to control the dynamics exhibited by the first. These dynamics could range from a fixed-point attractor, where the states of the first are all zero; through to quasi-periodic and chaotic behaviour, when the value of the Raleigh number exceeds an appropriate threshold (about twenty four) and induces a bifurcation. Because higher states evolve more slowly, they switch the lower attractor on and off, generating distinct songs, where each song comprises a series of distinct chirps (see <xref rid="fig3" ref-type="fig">Fig. 3</xref>).</p>
      </sec>
      <sec id="sec4.3">
        <label>4.3</label>
        <title>Song recognition</title>
        <p>This model generates spontaneous sequences of songs using autonomous dynamics. We generated a single song, corresponding roughly to a cycle of the higher attractor and then inverted the ensuing sonogram (summarised as peak amplitude and volume) using the message-passing scheme described in the previous section. The results are shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref> and demonstrate that, after several hundred milliseconds, the veridical hidden states and supraordinate causal states can be recovered. Interestingly, the third chirp is not perceived, in that the first-level prediction error was not sufficient to overcome the dynamical and structural priors of the model. However, once the subsequent chirp had been predicted correctly the following sequence of chirps was recognised with a high degree of conditional confidence. Note that when the second and third chirps in the sequence are not recognised, first-level prediction error is high and the conditional confidence about the causal states at the second level is low (reflected in the wide 90% confidence intervals). Heuristically, this means that the synthetic bird listening to the song did not know which song was being emitted and was unable to predict subsequent chirps.</p>
        <sec id="sec4.3.1">
          <label>4.3.1</label>
          <title>Structural and dynamic priors</title>
          <p>This example provides a nice opportunity to illustrate the relative roles of structural and dynamic priors. Structural priors are provided by the top-down inputs that reshape the manifold of the low-level attractor. However, this attractor itself contains an abundance of dynamical priors that unfold in generalised coordinates. Both provide important constraints on the evolution of sensory states, which facilitate recognition. We can selectively destroy these priors by lesioning the top-down connections to remove structural priors or by cutting the intrinsic connections that mediate dynamic priors. The latter involves cutting the self-connections in <xref rid="fig1" ref-type="fig">Fig. 1</xref>, among the causal and state-units. The results of these two simulated lesion experiments are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. The top panel shows the percept as in the previous panel, in terms of the predicted sonogram and prediction error at the first and second level. The subsequent two panels show exactly the same things but without structural (middle) and dynamic (lower) priors. In both cases, the synthetic bird fails to recognise the sequence with a corresponding inflation of prediction error, particularly at the sensory level. Interestingly, the removal of structural priors has a less marked effect on recognition than removing the dynamical priors. Without dynamical priors there is a failure to segment the sensory stream and, although there is a preservation of frequency tracking, the dynamics <italic>per se</italic> have completely lost their tempo. Although it is interesting to compare and contrast the relative roles of structural and dynamics priors; the important message here is that both are necessary for veridical perception and that destruction of either leads to suboptimal inference. Both of these empirical priors prescribe dynamics which enable the synthetic bird to predict what will be heard next. This leads to the question ‘what would happen if the song terminated prematurely?’</p>
        </sec>
      </sec>
      <sec id="sec4.4">
        <label>4.4</label>
        <title>Omission-related responses</title>
        <p>We repeated the above simulation but terminated the song after the fifth chirp. The corresponding sonograms and percepts are shown with their prediction errors in <xref rid="fig5" ref-type="fig">Fig. 5</xref>. The left panels show the stimulus and percept as in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, while the right panels show the stimulus and responses to omission of the last syllables. These results illustrate two important phenomena. First, there is a vigorous expression of prediction error after the song terminates prematurely. This reflects the dynamical nature of the recognition process because, at this point, there is no sensory input to predict. In other words, the prediction error is generated entirely by the predictions afforded by the dynamic model of sensory input. It can be seen that this prediction error (with a percept but no stimulus) is almost as large as the prediction error associated with the third and fourth stimuli that are not perceived (stimulus but no percept). Second, it can be seen that there is a transient percept, when the omitted chirp should have occurred. Its frequency is slightly too low but its timing is preserved, in relation to the expected stimulus train. This is an interesting stimulation from the point of view of ERP studies of omission-related responses. These simulations and related empirical studies (e.g., <xref rid="b43 b53" ref-type="bibr">Nordby, Hammerborg, Roth, &amp; Hugdahl, 1994; Yabe, Tervaniemi, Reinikainen, &amp; Näätänen, 1997</xref>) provide clear evidence for the predictive capacity of the brain. In this example, prediction rests upon the internal construction of an attractor manifold that defines a family of trajectories, each corresponding to the realisation of a particular song. In the last simulation we look more closely at perceptual categorisation of these songs.</p>
      </sec>
      <sec id="sec4.5">
        <label>4.5</label>
        <title>Perceptual categorisation</title>
        <p>In the previous simulations, we saw that a song corresponds to a sequence of chirps that are preordained by the shape of an attractor manifold that is controlled by top-down inputs. This means that for every point in the state-space of the higher attractor there is a corresponding manifold or category of song. In other words, recognising or categorising a particular song corresponds to finding a fixed location in the higher state-space. This provides a nice metaphor for perceptual categorisation; because the neuronal states of the higher attractor represent, implicitly, a category of song. Inverting the generative model means that, probabilistically, we can map from a sequence of sensory events to a point in some perceptual space; where this mapping corresponds to perceptual recognition or categorisation. This can be demonstrated in our synthetic songbird by ignoring the dynamics of the second-level attractor and exposing the bird to a song and letting the states at the second level optimise their location in perceptual space. To illustrate this, we generated three songs by fixing the Raleigh and Prandtl variables to three distinct values. We then placed uninformative priors on the second-level causal states (that were previously driven by the hidden states of the second-level attractor) and inverted the model in the usual way. <xref rid="fig6" ref-type="fig">Fig. 6</xref> shows the results of this simulation for a single song. This song comprises a series of relatively low-frequency chirps emitted every 250 ms or so. The causal states of this song (song C in the next figure) are recovered after the second chirp, with relatively tight confidence intervals (the blue and green lines in the lower left panel). We then repeated this exercise for three songs. The results are shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>. The songs are portrayed in sonogram format in the top panels and the inferred perceptual causal states in the bottom panels. The left panel shows the evolution of the causal states for all three songs as a function of peristimulus time and the right panel shows the corresponding conditional density in the causal or perceptual space of these two states after convergence. It can be seen that for all three songs, the 90% confidence interval encompasses the true values (red dots). Furthermore, there is very little overlap between the conditional densities (grey regions), which means that the precision of the perceptual categorisation is almost 100%. This is a simple but nice example of perceptual categorisation, where sequences of sensory events with extended temporal support can be mapped to locations in perceptual space, through Bayesian deconvolution of the sort entailed by the free-energy formulation.</p>
      </sec>
    </sec>
    <sec id="sec5">
      <label>5</label>
      <title>Conclusion</title>
      <p>This paper has suggested that the architecture of cortical circuits speaks to hierarchical generative models in the brain. The estimation or inversion of these models corresponds to a generalised deconvolution of sensory inputs to disclose their causes. This deconvolution could be implemented in a neuronally plausible fashion, where neuronal dynamics self-organise when exposed to inputs to suppress free-energy. The focus of this paper has been on the nature of the hierarchical models and, in particular, how one can understand message-passing among neuronal populations in terms of perception. We have tried to demonstrate their plausibility, in relation to empirical observations, by interpreting the prediction error, associated with model inversion, with observed electrophysiological responses.</p>
      <p>The ideas reviewed in this paper have a long history, starting with the notion of neuronal energy (<xref rid="b24" ref-type="bibr">Helmholtz, 1860/1962</xref>); covering ideas like efficient coding and analysis by synthesis (<xref rid="b3 b42" ref-type="bibr">Barlow, 1961; Neisser, 1967</xref>) to more recent formulations in terms of Bayesian inversion and Predictive coding (e.g., <xref rid="b2 b9 b30 b39 b45" ref-type="bibr">Ballard, Hinton, &amp; Sejnowski, 1983; Dayan, Hinton, &amp; Neal, 1995; Kawato, Hayakawa, &amp; Inui, 1993; Mumford, 1992; Rao &amp; Ballard, 1998</xref>). This work has also tried to provide support for the notion that the brain uses dynamics to represent and predict causes in the sensorium (<xref rid="b6 b11 b15 b52" ref-type="bibr">Byrne, Becker, &amp; Burgess, 2007; Deco &amp; Rolls, 2003; Freeman, 1987; Tsodyks, 1999</xref>).</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgements</title>
      <p>The Wellcome Trust funded this work. We would like to thank our colleagues for invaluable discussion about these ideas and Marcia Bennett for helping to prepare this manuscript.</p>
      <p>
        <bold>Software note</bold>
      </p>
      <p>All the schemes described in this paper are available in Matlab code as academic freeware (<ext-link xlink:href="http://www.fil.ion.ucl.ac.uk/spm" ext-link-type="uri">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). The simulation figures in this paper can be reproduced from a graphical user interface called from the DEM toolbox.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="b1">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Angelucci</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Levitt</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Walton</surname>
              <given-names>E.J.</given-names>
            </name>
            <name>
              <surname>Hupe</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Bullier</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lund</surname>
              <given-names>J.S.</given-names>
            </name>
          </person-group>
          <article-title>Circuits for local and global signal integration in primary visual cortex</article-title>
          <source>Journal of Neuroscience</source>
          <year>2002</year>
          <volume>22</volume>
          <fpage>8633</fpage>
          <lpage>8646</lpage>
          <pub-id pub-id-type="pmid">12351737</pub-id>
        </citation>
      </ref>
      <ref id="b2">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ballard</surname>
              <given-names>D.H.</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G.E.</given-names>
            </name>
            <name>
              <surname>Sejnowski</surname>
              <given-names>T.J.</given-names>
            </name>
          </person-group>
          <article-title>Parallel visual computation</article-title>
          <source>Nature</source>
          <year>1983</year>
          <volume>306</volume>
          <fpage>21</fpage>
          <lpage>26</lpage>
          <pub-id pub-id-type="pmid">6633656</pub-id>
        </citation>
      </ref>
      <ref id="b3">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Barlow</surname>
              <given-names>H.B.</given-names>
            </name>
          </person-group>
          <article-title>Possible principles underlying the transformation of sensory messages</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Rosenblith</surname>
              <given-names>W.A.</given-names>
            </name>
          </person-group>
          <source>Sensory communication</source>
          <year>1961</year>
          <publisher-name>MIT Press</publisher-name>
          <publisher-loc>Cambridge, MA</publisher-loc>
        </citation>
      </ref>
      <ref id="b4">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Botvinick</surname>
              <given-names>M.M.</given-names>
            </name>
          </person-group>
          <article-title>Multilevel structure in behaviour and in the brain, a model of Fuster’s hierarchy</article-title>
          <source>Philosophical Transactions of the Royal Society of London. B: Biological Sciences</source>
          <year>2007</year>
          <volume>362</volume>
          <issue>1485</issue>
          <fpage>1615</fpage>
          <lpage>1626</lpage>
        </citation>
      </ref>
      <ref id="b5">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Breakspear</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Stam</surname>
              <given-names>C.J.</given-names>
            </name>
          </person-group>
          <article-title>Dynamics of a neural system with a multiscale architecture</article-title>
          <source>Philosophical Transactions of the Royal Society of London. B: Biological Sciences</source>
          <year>2005</year>
          <volume>360</volume>
          <fpage>1051</fpage>
          <lpage>1107</lpage>
        </citation>
      </ref>
      <ref id="b6">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Byrne</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Becker</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Burgess</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Remembering the past and imagining the future, a neural model of spatial memory and imagery</article-title>
          <source>Psychological Review</source>
          <year>2007</year>
          <volume>114</volume>
          <issue>2</issue>
          <fpage>340</fpage>
          <lpage>375</lpage>
          <pub-id pub-id-type="pmid">17500630</pub-id>
        </citation>
      </ref>
      <ref id="b7">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Canolty</surname>
              <given-names>R.T.</given-names>
            </name>
            <name>
              <surname>Edwards</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Dalal</surname>
              <given-names>S.S.</given-names>
            </name>
            <name>
              <surname>Soltani</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Nagarajan</surname>
              <given-names>S.S.</given-names>
            </name>
            <name>
              <surname>Kirsch</surname>
              <given-names>H.E.</given-names>
            </name>
          </person-group>
          <article-title>High gamma power is phase-locked to theta oscillations in human neocortex</article-title>
          <source>Science</source>
          <year>2006</year>
          <volume>313</volume>
          <fpage>1626</fpage>
          <lpage>1628</lpage>
          <pub-id pub-id-type="pmid">16973878</pub-id>
        </citation>
      </ref>
      <ref id="b8">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chait</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Poeppel</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>de Cheveigné</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Simon</surname>
              <given-names>J.Z.</given-names>
            </name>
          </person-group>
          <article-title>Processing asymmetry of transitions between order and disorder in human auditory cortex</article-title>
          <source>Journal of Neuroscience</source>
          <year>2007</year>
          <volume>27</volume>
          <issue>19</issue>
          <fpage>5207</fpage>
          <lpage>5214</lpage>
          <pub-id pub-id-type="pmid">17494707</pub-id>
        </citation>
      </ref>
      <ref id="b9">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dayan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G.E.</given-names>
            </name>
            <name>
              <surname>Neal</surname>
              <given-names>R.M.</given-names>
            </name>
          </person-group>
          <article-title>The Helmholtz machine</article-title>
          <source>Neural Computation</source>
          <year>1995</year>
          <volume>7</volume>
          <fpage>889</fpage>
          <lpage>904</lpage>
          <pub-id pub-id-type="pmid">7584891</pub-id>
        </citation>
      </ref>
      <ref id="b10">
        <citation citation-type="other">Dean, T. (2006). Scalable inference in hierarchical generative models. In <italic>Proceedings of the ninth international symposium on artificial intelligence and mathematics</italic></citation>
      </ref>
      <ref id="b11">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deco</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Rolls</surname>
              <given-names>E.T.</given-names>
            </name>
          </person-group>
          <article-title>Attention and working memory, a dynamical model of neuronal activity in the prefrontal cortex</article-title>
          <source>European Journal of Neuroscience</source>
          <year>2003</year>
          <volume>18</volume>
          <issue>8</issue>
          <fpage>2374</fpage>
          <lpage>2390</lpage>
          <pub-id pub-id-type="pmid">14622200</pub-id>
        </citation>
      </ref>
      <ref id="b12">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>DeFelipe</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Alonso-Nanclares</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Arellano</surname>
              <given-names>J.I.</given-names>
            </name>
          </person-group>
          <article-title>Microstructure of the neocortex, comparative aspects</article-title>
          <source>Journal of Neurocytology</source>
          <year>2002</year>
          <volume>31</volume>
          <fpage>299</fpage>
          <lpage>316</lpage>
          <pub-id pub-id-type="pmid">12815249</pub-id>
        </citation>
      </ref>
      <ref id="b13">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Felleman</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Van Essen</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>
          <source>Cerebral Cortex</source>
          <year>1991</year>
          <volume>1</volume>
          <fpage>1</fpage>
          <lpage>47</lpage>
          <pub-id pub-id-type="pmid">1822724</pub-id>
        </citation>
      </ref>
      <ref id="b14">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Feynman</surname>
              <given-names>R.P.</given-names>
            </name>
          </person-group>
          <article-title>Statistical mechanics</article-title>
          <year>1972</year>
          <publisher-name>Benjamin</publisher-name>
          <publisher-loc>Reading MA, USA</publisher-loc>
        </citation>
      </ref>
      <ref id="b15">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Freeman</surname>
              <given-names>W.J.</given-names>
            </name>
          </person-group>
          <article-title>Simulation of chaotic EEG patterns with a dynamic model of the olfactory system</article-title>
          <source>Biological Cybernetics</source>
          <year>1987</year>
          <volume>56</volume>
          <issue>2–3</issue>
          <fpage>139</fpage>
          <lpage>150</lpage>
          <pub-id pub-id-type="pmid">3593783</pub-id>
        </citation>
      </ref>
      <ref id="b16">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Transients, metastability, and neuronal dynamics</article-title>
          <source>NeuroImage</source>
          <year>1997</year>
          <volume>5</volume>
          <issue>2</issue>
          <fpage>164</fpage>
          <lpage>171</lpage>
          <pub-id pub-id-type="pmid">9345546</pub-id>
        </citation>
      </ref>
      <ref id="b17">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>A theory of cortical responses</article-title>
          <source>Philosophical Transactions of the Royal Society of London. B: Biological Sciences</source>
          <year>2005</year>
          <volume>360</volume>
          <fpage>815</fpage>
          <lpage>836</lpage>
        </citation>
      </ref>
      <ref id="b18">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kilner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Harrison</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>A free-energy principle for the brain</article-title>
          <source>Journal de Physiologie (Paris)</source>
          <year>2006</year>
          <volume>100</volume>
          <issue>1–3</issue>
          <fpage>70</fpage>
          <lpage>87</lpage>
        </citation>
      </ref>
      <ref id="b19">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical models in the brain</article-title>
          <source>PLoS Computational Biology</source>
          <year>2008</year>
          <volume>4</volume>
          <issue>11</issue>
          <fpage>e1000211</fpage>
          <comment>PMID, 18989391</comment>
          <pub-id pub-id-type="pmid">18989391</pub-id>
        </citation>
      </ref>
      <ref id="b20">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Predictive-coding under the free-energy principle</article-title>
          <source>Philosophical Transactions of the Royal Society. Series B</source>
          <year>2009</year>
          <volume>264</volume>
          <fpage>1211</fpage>
          <lpage>1221</lpage>
        </citation>
      </ref>
      <ref id="b21">
        <citation citation-type="other">George, D., &amp; Hawkins, J. (2005). A hierarchical Bayesian model of invariant pattern recognition in the visual cortex. In <italic>IEEE international joint conference on neural networks</italic> (pp. 1812–1817)</citation>
      </ref>
      <ref id="b22">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haken</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Kelso</surname>
              <given-names>J.A.S.</given-names>
            </name>
            <name>
              <surname>Fuchs</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pandya</surname>
              <given-names>A.S.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic pattern-recognition of coordinated biological motion</article-title>
          <source>Neural Networks</source>
          <year>1990</year>
          <volume>3</volume>
          <fpage>395</fpage>
          <lpage>401</lpage>
        </citation>
      </ref>
      <ref id="b23">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hasson</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Vallines</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Heeger</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Rubin</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>A hierarchy of temporal receptive windows in human cortex</article-title>
          <source>Journal of Neuroscience</source>
          <year>2008</year>
          <volume>28</volume>
          <fpage>2539</fpage>
          <lpage>2550</lpage>
          <pub-id pub-id-type="pmid">18322098</pub-id>
        </citation>
      </ref>
      <ref id="b24">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Helmholtz</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Southall</surname>
              <given-names>J.P.C.</given-names>
            </name>
          </person-group>
          <source>Handbuch der physiologischen optik</source>
          <volume>Vol. 3</volume>
          <year>1860/1962</year>
          <publisher-name>Dover</publisher-name>
          <publisher-loc>New York</publisher-loc>
          <comment>(Engl. transl.)</comment>
        </citation>
      </ref>
      <ref id="b25">
        <citation citation-type="other">Hinton, G.E., &amp; von Cramp, D. (1993). Keeping neural networks simple by minimising the description length of weights. In <italic>Proceedings of COLT-93</italic> (pp. 5–13)</citation>
      </ref>
      <ref id="b26">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hinton</surname>
              <given-names>G.E.</given-names>
            </name>
            <name>
              <surname>Osindero</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Teh</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>A fast learning algorithm for deep belief nets</article-title>
          <source>Neural Computation</source>
          <year>2006</year>
          <volume>18</volume>
          <fpage>1527</fpage>
          <lpage>1554</lpage>
          <pub-id pub-id-type="pmid">16764513</pub-id>
        </citation>
      </ref>
      <ref id="b27">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hupe</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>James</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Payne</surname>
              <given-names>B.R.</given-names>
            </name>
            <name>
              <surname>Lomber</surname>
              <given-names>S.G.</given-names>
            </name>
            <name>
              <surname>Girard</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bullier</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Cortical feedback improves discrimination between figure and background by V1, V2 and V3 neurons</article-title>
          <source>Nature</source>
          <year>1998</year>
          <volume>394</volume>
          <fpage>784</fpage>
          <lpage>787</lpage>
          <pub-id pub-id-type="pmid">9723617</pub-id>
        </citation>
      </ref>
      <ref id="b28">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jirsa</surname>
              <given-names>V.K.</given-names>
            </name>
            <name>
              <surname>Fuchs</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kelso</surname>
              <given-names>J.A.</given-names>
            </name>
          </person-group>
          <article-title>Connecting cortical and behavioral dynamics, bimanual coordination</article-title>
          <source>Neural Computation</source>
          <year>1998</year>
          <volume>10</volume>
          <fpage>2019</fpage>
          <lpage>2045</lpage>
          <pub-id pub-id-type="pmid">9804670</pub-id>
        </citation>
      </ref>
      <ref id="b29">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kass</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Steffey</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models)</article-title>
          <source>Journal of the American Statistical Association</source>
          <year>1989</year>
          <volume>407</volume>
          <fpage>717</fpage>
          <lpage>726</lpage>
        </citation>
      </ref>
      <ref id="b30">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kawato</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hayakawa</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Inui</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>A forward-inverse optics model of reciprocal connections between visual cortical areas</article-title>
          <source>Network</source>
          <year>1993</year>
          <volume>4</volume>
          <fpage>415</fpage>
          <lpage>422</lpage>
        </citation>
      </ref>
      <ref id="b31">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>A hierarchy of time-scales and the brain</article-title>
          <source>PLoS Computational Biology</source>
          <year>2008</year>
          <volume>4</volume>
          <issue>11</issue>
          <fpage>e1000209</fpage>
          <pub-id pub-id-type="pmid">19008936</pub-id>
        </citation>
      </ref>
      <ref id="b32">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kopell</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ermentrout</surname>
              <given-names>G.B.</given-names>
            </name>
            <name>
              <surname>Whittington</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Traub</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>Gamma rhythms and beta rhythms have different synchronization properties</article-title>
          <source>Proceedings of the National Academy of Sciences of the Unites States of America</source>
          <year>2000</year>
          <volume>97</volume>
          <fpage>1867</fpage>
          <lpage>1872</lpage>
        </citation>
      </ref>
      <ref id="b33">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Laje</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gardner</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Mindlin</surname>
              <given-names>G.B.</given-names>
            </name>
          </person-group>
          <article-title>Neuromuscular control of vocalizations in birdsong, a model</article-title>
          <source>Physical Review E. Statistical, Nonlinear and Soft Matter Physics</source>
          <year>2002</year>
          <volume>65</volume>
          <fpage>051921</fpage>
        </citation>
      </ref>
      <ref id="b34">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Laje</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Mindlin</surname>
              <given-names>G.B.</given-names>
            </name>
          </person-group>
          <article-title>Diversity within a birdsong</article-title>
          <source>Physics Review Letters</source>
          <year>2002</year>
          <volume>89</volume>
          <fpage>288102</fpage>
        </citation>
      </ref>
      <ref id="b35">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>T.S.</given-names>
            </name>
            <name>
              <surname>Mumford</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>
          <source>Journal of the Optical Society of America A</source>
          <year>2003</year>
          <volume>20</volume>
          <fpage>1434</fpage>
          <lpage>1448</lpage>
        </citation>
      </ref>
      <ref id="b36">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>MacKay</surname>
              <given-names>D.J.C.</given-names>
            </name>
          </person-group>
          <article-title>Free-energy minimisation algorithm for decoding and cryptoanalysis</article-title>
          <source>Electronics Letters</source>
          <year>1995</year>
          <volume>31</volume>
          <fpage>445</fpage>
          <lpage>447</lpage>
        </citation>
      </ref>
      <ref id="b37">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maunsell</surname>
              <given-names>J.H.</given-names>
            </name>
            <name>
              <surname>van Essen</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>The connections of the middle temporal visual area (MT) and their relationship to a cortical hierarchy in the macaque monkey</article-title>
          <source>Journal of Neuroscience</source>
          <year>1983</year>
          <volume>3</volume>
          <fpage>2563</fpage>
          <lpage>2586</lpage>
          <pub-id pub-id-type="pmid">6655500</pub-id>
        </citation>
      </ref>
      <ref id="b38">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McCrea</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Rybak</surname>
              <given-names>I.A.</given-names>
            </name>
          </person-group>
          <article-title>Organization of mammalian locomotor rhythm and pattern generation</article-title>
          <source>Brain Research Review</source>
          <year>2008</year>
          <volume>57</volume>
          <issue>1</issue>
          <fpage>134</fpage>
          <lpage>146</lpage>
        </citation>
      </ref>
      <ref id="b39">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mumford</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>On the computational architecture of the neocortex. II. The role of cortico-cortical loops</article-title>
          <source>Biological Cybernetics</source>
          <year>1992</year>
          <volume>66</volume>
          <fpage>241</fpage>
          <lpage>251</lpage>
          <pub-id pub-id-type="pmid">1540675</pub-id>
        </citation>
      </ref>
      <ref id="b40">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Murphy</surname>
              <given-names>P.C.</given-names>
            </name>
            <name>
              <surname>Sillito</surname>
              <given-names>A.M.</given-names>
            </name>
          </person-group>
          <article-title>Corticofugal feedback influences the generation of length tuning in the visual pathway</article-title>
          <source>Nature</source>
          <year>1987</year>
          <volume>329</volume>
          <fpage>727</fpage>
          <lpage>729</lpage>
          <pub-id pub-id-type="pmid">3670375</pub-id>
        </citation>
      </ref>
      <ref id="b41">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Neal</surname>
              <given-names>R.M.</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G.E.</given-names>
            </name>
          </person-group>
          <article-title>A view of the EM algorithm that justifies incremental sparse and other variants</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Jordan</surname>
              <given-names>M.I.</given-names>
            </name>
          </person-group>
          <source>Learning in graphical models</source>
          <year>1998</year>
          <publisher-name>Kulver Academic Press</publisher-name>
        </citation>
      </ref>
      <ref id="b42">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Neisser</surname>
              <given-names>U.</given-names>
            </name>
          </person-group>
          <article-title>Cognitive psychology</article-title>
          <year>1967</year>
          <publisher-name>Appleton-Century-Crofts</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </citation>
      </ref>
      <ref id="b43">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nordby</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hammerborg</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Roth</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Hugdahl</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>ERPs for infrequent omissions and inclusions of stimulus elements</article-title>
          <source>Psychophysiology</source>
          <year>1994</year>
          <volume>31</volume>
          <issue>6</issue>
          <fpage>544</fpage>
          <lpage>552</lpage>
          <pub-id pub-id-type="pmid">7846215</pub-id>
        </citation>
      </ref>
      <ref id="b44">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rabinovich</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Huerta</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Laurent</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Neuroscience. Transient dynamics for neural processing</article-title>
          <source>Science</source>
          <year>2008</year>
          <volume>321</volume>
          <issue>5885</issue>
          <fpage>48</fpage>
          <lpage>50</lpage>
          <pub-id pub-id-type="pmid">18599763</pub-id>
        </citation>
      </ref>
      <ref id="b45">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rao</surname>
              <given-names>R.P.</given-names>
            </name>
            <name>
              <surname>Ballard</surname>
              <given-names>D.H.</given-names>
            </name>
          </person-group>
          <article-title>Predictive-coding in the visual cortex. A functional interpretation of some extra-classical receptive field effects</article-title>
          <source>Nature Neuroscience</source>
          <year>1998</year>
          <volume>2</volume>
          <fpage>79</fpage>
          <lpage>87</lpage>
        </citation>
      </ref>
      <ref id="b46">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Rao</surname>
              <given-names>R.P.N.</given-names>
            </name>
          </person-group>
          <article-title>Neural models of bayesian belief propagation</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Doya</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <source>Bayesian brain: Probabilistic approaches to neural coding</source>
          <year>2006</year>
          <publisher-name>MIT Press</publisher-name>
          <fpage>239</fpage>
          <lpage>268</lpage>
        </citation>
      </ref>
      <ref id="b47">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rockland</surname>
              <given-names>K.S.</given-names>
            </name>
            <name>
              <surname>Pandya</surname>
              <given-names>D.N.</given-names>
            </name>
          </person-group>
          <article-title>Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey</article-title>
          <source>Brain Research</source>
          <year>1979</year>
          <volume>179</volume>
          <fpage>3</fpage>
          <lpage>20</lpage>
          <pub-id pub-id-type="pmid">116716</pub-id>
        </citation>
      </ref>
      <ref id="b48">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rosier</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Arckens</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Orban</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Vandesande</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Laminar distribution of NMDA receptors in cat and monkey visual cortex visualized by [3H]-MK-801 binding</article-title>
          <source>Journal of Comparative Neurology</source>
          <year>1993</year>
          <volume>335</volume>
          <fpage>369</fpage>
          <lpage>380</lpage>
          <pub-id pub-id-type="pmid">7901247</pub-id>
        </citation>
      </ref>
      <ref id="b49">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sherman</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Guillery</surname>
              <given-names>R.W.</given-names>
            </name>
          </person-group>
          <article-title>On the actions that one nerve cell can have on another, distinguishing “drivers” from “modulators”</article-title>
          <source>Proceedings of the National Academy of Sciences of the Unites States of America</source>
          <year>1998</year>
          <volume>95</volume>
          <fpage>7121</fpage>
          <lpage>7126</lpage>
        </citation>
      </ref>
      <ref id="b50">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Spratling</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Reconciling Predictive-coding and biased competition models of cortical function</article-title>
          <source>Frontiers in Computational Neuroscience</source>
          <year>2008</year>
          <volume>2</volume>
          <fpage>4</fpage>
          <pub-id pub-id-type="pmid">18978957</pub-id>
        </citation>
      </ref>
      <ref id="b51">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Spratling</surname>
              <given-names>M.W.</given-names>
            </name>
          </person-group>
          <article-title>Predictive-coding as a model of biased competition in visual attention</article-title>
          <source>Vision Research</source>
          <year>2008</year>
          <volume>48</volume>
          <fpage>1391</fpage>
          <lpage>1408</lpage>
          <pub-id pub-id-type="pmid">18442841</pub-id>
        </citation>
      </ref>
      <ref id="b52">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tsodyks</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Attractor neural network models of spatial maps in hippocampus</article-title>
          <source>Hippocampus</source>
          <year>1999</year>
          <volume>9</volume>
          <issue>4</issue>
          <fpage>481</fpage>
          <lpage>489</lpage>
          <pub-id pub-id-type="pmid">10495029</pub-id>
        </citation>
      </ref>
      <ref id="b53">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yabe</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Tervaniemi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Reinikainen</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Näätänen</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Temporal window of integration revealed by MMN to sound omission</article-title>
          <source>NeuroReport</source>
          <year>1997</year>
          <volume>8</volume>
          <issue>8</issue>
          <fpage>1971</fpage>
          <lpage>1974</lpage>
          <pub-id pub-id-type="pmid">9223087</pub-id>
        </citation>
      </ref>
      <ref id="b54">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yedidia</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Freeman</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Weiss</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Constructing free-energy approximations and generalized belief propagation algorithms</article-title>
          <source>IEEE Transactions on Information Theory</source>
          <year>2005</year>
          <volume>51</volume>
          <issue>7</issue>
          <fpage>2282</fpage>
          <lpage>2312</lpage>
        </citation>
      </ref>
      <ref id="b55">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zeki</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shipp</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>The functional logic of cortical connections</article-title>
          <source>Nature</source>
          <year>1988</year>
          <volume>335</volume>
          <fpage>311</fpage>
          <lpage>331</lpage>
          <pub-id pub-id-type="pmid">3047584</pub-id>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>Schematic detailing the neuronal architectures that encode a recognition density on the states of a hierarchical model. This schematic shows the speculative cells of origin of forward driving connections that convey prediction error from a lower area to a higher area and the backward connections that are used to construct predictions. These predictions try to explain away input from lower areas by suppressing prediction error. In this scheme, the sources of forward connections are superficial pyramidal cell populations and the sources of backward connections are deep pyramidal cell populations. The differential equations relate to the optimisation scheme detailed in the main text. The state-units and their efferents are in black and the error-units in red, with causal states on the right and hidden states on the left. For simplicity, we have assumed the output of each level is a function of, and only of, the hidden states. This induces a hierarchy over levels and, within each level, a hierarchical relationship between states, where causal states predict hidden states. This schematic shows how the neuronal populations may be deployed hierarchically within three cortical areas (or macro-columns). Within each area the cells are shown in relation to the laminar structure of the cortex that includes supra-granular (<bold>SG</bold>) granular (<bold>L4</bold>) and infra-granular (<bold>IG</bold>) layers. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Fig. 2</label>
      <caption>
        <p>Schematic showing the construction of the generative model for birdsongs. This comprises two Lorenz attractors where the higher attractor delivers two control parameters (grey circles) to a lower-level attractor, which, in turn, delivers two control parameters to a synthetic syrinx to produce amplitude and frequency modulated stimuli. This stimulus is represented as a sonogram in the right panel. The equations represent the hierarchical dynamic model in the form of Eq. <xref rid="fd13" ref-type="disp-formula">(13)</xref>.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Fig. 3</label>
      <caption>
        <p>Results of an inversion or deconvolution of the sonogram shown in the previous figure. (a) Upper panels show the time courses of hidden and causal states. Upper left: These are the true and predicted states driving the syrinx and are simple mappings from two of the three hidden states of the first-level attractor. The coloured lines respond to the conditional mean and the dotted lines to the true values. The discrepancy is the prediction error and is shown as a broken red line. Upper right: The true and estimated hidden states of the first-level attractor. Note that the third hidden state has to be inferred from the sensory data. Confidence intervals on the conditional expectations are shown in grey and demonstrate a high degree of confidence, because a low level of sensory noise was used in these simulations. The panels below show the corresponding causal and hidden states at the second level. Again the conditional expectations are shown as coloured lines and the true values as broken lines. Note the inflated conditional confidence interval halfway through the song when the third and fourth chirps are misperceived. (b) The stimulus and percept in sonogram format, detailing the expression of different frequencies generated over peristimulus time. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="fig4">
      <label>Fig. 4</label>
      <caption>
        <p>Results of simulated lesion studies using the birdsong model of the previous figure. The left panels show the percept in terms of the predicted sonograms and the right panels show the corresponding prediction error (at the both levels); these are the differences between the incoming sensory information and the prediction and the discrepancy between the conditional expectation of the second level cause and that predicted by the second-level hidden states. Top row: the recognition dynamics in the intact bird. Middle row: the percept and corresponding prediction errors when the connections between the hidden states at the second level and their corresponding causes are removed. This effectively removes structural priors on the evolution of the attractor manifold prescribing the sensory dynamics at the first level. Lower panels: the effects of retaining the structural priors but removing the dynamical priors by cutting the connections that mediate inversion in generalised coordinates. These results suggest that both structural and dynamical priors are necessary for veridical perception.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="fig5">
      <label>Fig. 5</label>
      <caption>
        <p>Omission-related responses: Here, we have omitted the last few chirps from the stimulus. The left hand panels show the original sequence and responses evoked. The right hand panels show the equivalent dynamics on omission of the last chirps. The top panels show the stimulus and the middle panels the corresponding percept in sonogram format. The interesting thing to note here is the occurrence of an anomalous percept after termination of the song on the lower right (i). This corresponds roughly to the chirp that would have been perceived in the absence of omission. The lower panels show the corresponding (precision-weighted) prediction error under the two stimuli at both levels. A comparison of the two reveals a burst of prediction error when a stimulus is missed (ii) and at the point that the stimulus terminates (iii) despite the fact that there is no stimulus present at this time. The red lines correspond to prediction error at the first level and the pink lines correspond to prediction error at the second level. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="fig6">
      <label>Fig. 6</label>
      <caption>
        <p>Schematic demonstration of perceptual categorisation. This figure follows the same format as <xref rid="fig3" ref-type="fig">Fig. 3</xref>. However, here there are no hidden states at the second level and the causal states were subject to stationary and uninformative priors. This song was generated by a first-level attractor with fixed control variables of <mml:math id="M88" altimg="si1.gif" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:math> and <mml:math id="M89" altimg="si2.gif" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:math> respectively. It can be seen that, on inversion of this model, these two control variables, corresponding to causal states at the second level are recovered with relatively high conditional precision. However, it takes about 50 iterations (about 600 ms) before they stabilise. In other words, the sensory sequence has been mapped correctly to a point in perceptual space after the occurrence of the second chirp. This song corresponds to song C in the next figure.</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="fig7">
      <label>Fig. 7</label>
      <caption>
        <p>The results of inversion for three songs, each produced with three distinct pairs of values for the second-level causal states (the Raleigh and <italic>Prandtl</italic> variables of the first-level attractor). Upper panel: the three songs shown in sonogram format corresponding to a series of relatively high-frequency chirps that fall progressively in both frequency and number as the Raleigh number is decreased. Lower left: Inferred second-level causal states (blue lines — <italic>Raleigh</italic> and green lines — <italic>Prandtl</italic>) shown as a function of peristimulus time for the three songs. It can be seen that the causal states are identified with high conditional precision after about 600 ms. Lower right: this shows the conditional density on the causal states shortly before the end of peristimulus time (dotted line on the left). The blue dots correspond to conditional means or expectations and the grey areas correspond to the conditional confidence regions. Note that these encompass the true values (red dots) used to generate the songs. These results indicate that there has been a successful categorisation, in the sense that there is no ambiguity (from the point of view of the synthetic bird) about which song was heard.</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
  </floats-wrap>
</article>